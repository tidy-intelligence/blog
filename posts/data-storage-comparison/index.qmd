---
title: "Tidy Data: Tabular Data Storage Comparison"
description: "A comparison of popular open-source data storage technologies using R and Python."
metadata:
  pagetitle: "Tidy Data: Tabular Data Storage Comparison"
author: "Christoph Scheuch"
date: "2024-01-19" 
image: thumbnail.png
image-alt: A server room with a more subdued and professional atmosphere. It features four different database servers, each representing a distinct tabular data storage technology. The first server has a sleek and efficient design, symbolizing a modern, high-performance database. The second server is sturdy and straightforward, indicative of a traditional, reliable database system. The third server is compact and practical, representing an efficient, resource-conserving database. The fourth is advanced yet unobtrusive, suggesting a sophisticated, AI-powered database. The room has soft lighting, with a focus on functionality and neatness. The overall ambiance is more business-like and less flashy, conveying a sense of serious technology at work. Created with DALL-E 3.
draft: true
---

# Create example data

::: panel-tabset
### R

```{r}
#| message: false
library(tidyverse)

data <- tibble(
  character_column = c("A", "B", "C", "D"), 
  date_column = as.Date(c("2023-01-01", "2023-02-01", "2023-03-01", "2023-04-01")),
  datetime_column = ymd_hms(c("2023-01-01 10:00:00", "2023-02-01 11:00:00", "2023-03-01 12:00:00", "2023-04-01 13:00:00")),
  numeric_column = c(1.5, 2.5, 3.5, 4.5),
  integer_column = as.integer(c(1, 2, 3, 4)),
  logical_column = c(TRUE, FALSE, FALSE, TRUE)
)

extract_column_classes <- function(df) {
  sapply(sapply(df, class), function(x) paste(x, collapse = ", "))
}

tibble("data" = extract_column_classes(data))
```

### Python

```{python}
import pandas as pd

data = pd.DataFrame({
  "character_column": ["A", "B", "C", "D"],
  "date_column": pd.to_datetime(["2023-01-01", "2023-02-01", "2023-03-01", "2023-04-01"]),
  "datetime_column": pd.to_datetime(["2023-01-01 10:00:00", "2023-02-01 11:00:00", "2023-03-01 12:00:00", "2023-04-01 13:00:00"]),
  "numeric_column": [1.5, 2.5, 3.5, 4.5],
  "integer_column": [1, 2, 3, 4],
  "logical_column": [True, False, False, True]
})

data.info()
```

:::

The following table shows the mapping between the different column types in the example above

| R         | Python   | 
|-----------|----------|
| character | object   |
| date      | datetime |
| datetime  | datetime |
| numeric   | float    |
| integer   | numeric  |
| logical   | bool     |

# CSV

Simple, widely supported, and easy to read and write in R and Python

Not efficient for large datasets and doesn't support complex data structures or metadata well.

::: panel-tabset
### R

```{r}
#| message: false
library(readr)

write_csv(data, file = "data_r.csv")
data_csv_r <- read_csv("data_r.csv")

glimpse(data_csv_r)
```

### Python

```{python}
data.to_csv("data_python.csv", index = False)
data_csv = pd.read_csv("data_python.csv")

data_csv.info()
```

:::

# SQLite

Lightweight, file-based SQL database. Easy to use and supported by R and Python without the need for a separate server.

Not suitable for very large or high-concurrency applications.

::: panel-tabset
### R

We can see that the column types are not automatically preserved as dates, datetime and logical columns are converted to integers. 

```{r}
library(RSQLite)

con_sqlite_r <- dbConnect(SQLite(), "data_r.sqlite")

copy_to(con_sqlite_r, data, "data", overwrite = TRUE)
data_sqlite_r <- tbl(con_sqlite_r, "data") |> 
  collect()

glimpse(data_sqlite_r)
```

If we now read from the SQLite databases created in the other languages, we can see that things get even more complicated: Python stores dates as characters, so you need to convert them to dates or datetime.

```{r}
con_sqlite_python <- dbConnect(SQLite(), "data_python.sqlite")
data_sqlite_python <- tbl(con_sqlite_python, "data") |> 
  collect()

tibble(
  "data_sqlite_r" = extract_column_classes(data_sqlite_r),
  "data_sqlite_python" = extract_column_classes(data_sqlite_python)
)
```

### Python

```{python}
import sqlite3

con_sqlite = sqlite3.connect(database = "data_python.sqlite")

res = data.to_sql("data", con_sqlite, if_exists = "replace", index = False)

data_sqlite = pd.read_sql_query("SELECT * FROM data", con_sqlite)

data_sqlite.info()
```

:::

# DuckDB

https://r4ds.hadley.nz/databases

DuckDB is an emerging database management system that's gaining attention for its efficiency and ease of use, particularly in the data science community. It's designed to be an OLAP (Online Analytical Processing) database and is especially well-suited for analytical queries on large datasets.

As a relatively new system, it might not have the same level of community support, tools, and integrations as more established databases.

::: panel-tabset
### R

If we write the data to DuckDB and then read it back in, we get the same column types again - which is nice!

```{r}
#| warning: false
library(duckdb)

con_duckdb_r <- dbConnect(duckdb(), "data_r.duckdb")

copy_to(con_duckdb_r, data, "data", overwrite = TRUE)
data_duckdb_r <- tbl(con_duckdb_r, "data") |> 
  collect()

glimpse(data_duckdb_r)
```

```{r}
# con_duckdb_python <- dbConnect(duckdb(), "data_python.duckdb")
# data_duckdb_python <- tbl(con_duckdb_python, "data") |> 
#   collect()

# tibble(
#   "data_r.sqlite" = paste(sapply(data_duckdb_r, class), collapse = ", "),
#   "data_python.sqlite" = paste(sapply(data_duckdb_python, class), collapse = ", ")
# )
```

### Python

```{python}
#| warning: false
import duckdb

con_duckdb = duckdb.connect("data_python.duckdb")

data.to_sql("data", con_duckdb, if_exists = "replace", index = False)

data_duckdb = pd.read_sql_query("SELECT * FROM data", con_duckdb)

con_duckdb.close()

data_duckdb.info()
```

:::


# Parquet

https://r4ds.hadley.nz/arrow

Columnar storage format, which is great for analytics and large datasets. Offers efficient data compression and encoding schemes.

Requires additional libraries and understanding of its format.

::: panel-tabset
### R

Similar to DuckDB, all column types are preserved exactly as to before we wrote the data frame to a parquet file. 

```{r}
#| message: false
library(arrow)

write_parquet(data, "data_r.parquet")
data_parquet_r <- read_parquet("data_r.parquet")

glimpse(data_parquet_r)
```

```{r}
data_parquet_python <- read_parquet("data_python.parquet")

tibble(
  "data_parquet_r" = extract_column_classes(data_parquet_r),
  "data_parquet_python" = extract_column_classes(data_parquet_python),
)
```

### Python

```{python}
import pyarrow.parquet as pq

data.to_parquet("data_python.parquet")
data_parquet = pd.read_parquet("data_python.parquet")

data_parquet.info()
```

:::

# Conclusion

...
