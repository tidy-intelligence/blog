{
  "hash": "3e8d5660b8e61aed933df2f06e05cbee",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"RAG Prototyping in R\"\ndescription: \"Building a Retrieval-Augmented Generation (RAG) prototype with `ellmer` and `duckdb`\"\nmetadata:\n  pagetitle: \"RAG Prototyping in R\"\nauthor: \"Christoph Scheuch\"\ndate: \"2025-02-22\" \nimage: thumbnail.png\nimage-alt: Created with DALL-E.\ncategories: \n  - R\n  - Generative AI\nformat:\n  html: \n    fig-width: 12\n    fig-height: 8\n    fig-dpi: 300\n---\n\n\n\nOne of the reasons why R is amazing is its capability to rapidly prototype algorithms and apps. In this blog post, I demonstrate how `ellmer` and DuckDB can be used to create an LLM that uses domain-specific information. The problem of endowing an LLM with more grounded responses is usually denoted as retrieval-augmented generation (RAG). RAG systems help overcome one of the main limitations of LLMs - their inability to access or reference specific, up-to-date information outside their training data. By combining the power of LLMs with dedicated knowledge bases, RAG enables more accurate and contextual responses.\n\n`ellmer` is an R package that provides a convenient interface to a wide variety of LLM providers with out-of-the-box support for things such as tool calling and structured data extraction. DuckDB is an embedded analytics database, which means it runs directly in your application. It excels at processing large datasets, and can efficiently query various file formats. Conveniently, it also supports an extension that allows us to implement vector search. The combination of both packages allows for fast prototyping of LLM-based apps. If you're building a quick proof-of-concept for a larger system, this stack provides the flexibility and performance needed for it.\n\n## Response without Augmentation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr2)\nlibrary(ellmer)\nlibrary(duckdb)\nlibrary(tibble)\n```\n:::\n\n\n\nThroughout this post, I'm using OpenAI's API. The integration is handled seamlessly through `ellmer`, which provides a clean interface for interacting with various LLM providers. As an example, we start by asking Chat what it knows about me:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat <- chat_openai(model = \"gpt-4o-mini\")\nchat$chat(\"Who is Christoph Scheuch?\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAs of my last update in October 2023, Christoph Scheuch is not a widely \nrecognized public figure, so there may not be significant information available\nabout him. It is possible that he could be a professional in a specific field, \na researcher, or an individual with a lesser-known public presence. If you have\nmore context or details about who you are referring to, I could help you \nbetter. Please clarify if you mean someone in a specific industry or context.\n```\n\n\n:::\n:::\n\n\n\n<br>\n\nToo bad, I'm not a widely recognized public figure, so Chat cannot say anything about me. This example perfectly illustrates a common limitation of LLMs - while they excel at general knowledge and patterns learned during training, they lack access to specific, current, or private information. This is where RAG becomes invaluable. By augmenting the model's capabilities with our own knowledge base, we can help it provide accurate, contextual responses about topics it wasn't originally trained on.\n\n## Store Embeddings\n\nEmbeddings are numerical representations of text that capture semantic meaning in a way that computers can process. They convert words and sentences into high-dimensional vectors where similar meanings result in similar vectors. This mathematical representation is crucial for implementing efficient information retrieval in RAG systems.\n\nI'm using the `text-embedding-3-small` model that is optimized for latency and costs. This model returns a vector of length 1536, striking a balance between computational efficiency and semantic representation quality. While larger models might offer marginally better performance, they often come with increased latency and cost. Feel free to experiment with other models based on your specific needs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_embedding <- function(\n  text,\n  model = \"text-embedding-3-small\",\n  api_key = Sys.getenv(\"OPENAI_API_KEY\")\n) {\n\n  base_url <- \"https://api.openai.com/v1\"\n  req <- request(base_url)\n  req <- req_url_path_append(req, \"/embeddings\")\n  req <- req_auth_bearer_token(req, api_key)\n  req <- req_body_json(req, list(\n    input = as.list(text),\n    model = model\n  ))\n\n  resp <- req_perform(req)\n  json <- resp_body_json(resp)\n\n  embedding <- as.vector(unlist(json$data[[1]]$embedding))\n\n  embedding\n}\n```\n:::\n\n\n\nHere is an example that demonstrates how a simple question gets transformed into its vector representation. For brevity, I just print the first 5 numbers:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nembedding <- get_embedding(\"Who is Christoph Scheuch?\")\nembedding[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.015694630 -0.026622834 -0.008347419  0.025807848 -0.036254470\n```\n\n\n:::\n:::\n\n\n\nThese numbers obviously don't mean anything to us humans. But through vector similarity, they are tremendously useful to find relevant content. When we later search for information, we'll compare these vector representations to find semantically similar content, rather than relying on simple keyword matching. This allows our RAG system to understand and retrieve contextually relevant information even when the exact words don't match.\n\nWhile there are many specialized vector databases available, DuckDB offers a lightweight yet powerful solution for implementing vector similarity search through its extensions system. The [`vss` extension](https://duckdb.org/docs/extensions/vss.html) enables efficient similarity searches directly within DuckDB, making it an excellent choice for prototyping and smaller-scale applications.\n\nYou can create a local database and install and load the extension as follows: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon <- dbConnect(duckdb(), \"database.duckdb\")\ndbSendQuery(con, \"INSTALL vss;\")\ndbSendQuery(con, \"LOAD vss;\")\n```\n:::\n\n\n\nNext, we'll create the foundation of our knowledge base - a table called docs that will store both the original text and its vector representation. The table schema reflects hence the two components of our RAG system.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbSendQuery(con, \"DROP TABLE IF EXISTS docs;\")\ndbSendQuery(con, \"CREATE TABLE docs (text TEXT, embedding FLOAT[1536]);\")\n```\n:::\n\n\n\nThe array size of 1536 matches the dimensionality of our chosen embedding model, ensuring compatibility between the embeddings we generate and our storage solution.\n\nNext, we'll create a set of documents with varying degrees of relevance to our original question. Let's start with a comprehensive document about me:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoc_1 <- c(\n  \"Who is Christoph Scheuch? Christoph Scheuch is an independent BI & Data Science consultant specializing in financial topics. He provides services and trainings related to data infrastructure, data analysis, machine learning, and general data science topics. Previously, he was the Head of AI, Director of Product, and Head of BI & Data Science at the social trading platform wikifolio.com. He is also the co-creator and maintainer of the open-source project Tidy Finance. In his free time, he occasionally designs shirts and mugs, available under the Tidy Swag brand.\"\n  )\n```\n:::\n\n\n\nNow we need a way to store these documents and their embeddings in our DuckDB database. Due to current limitations in the `duckdb` R package (see the issue [here](https://github.com/duckdb/duckdb-r/issues/102)), we need to construct our SQL query manually. While this approach isn't ideal for production systems, it serves our prototyping purposes well:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstore_embedding <- function(text) {\n  embedding <- get_embedding(text)\n  embedding_sql <- paste0(\"array_value(\", paste(embedding, collapse = \", \"), \")\")\n  query <- sprintf(\"INSERT INTO docs VALUES ('%s', %s);\", text, embedding_sql)\n  result <- dbExecute(con, query)\n}\nstore_embedding(doc_1)\n```\n:::\n\n\n\nTo better understand the retrieval mechanism, let's add documents with varying levels of relevance. First, a shorter, more focused document that contains only key information:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoc_2 <- c(\n  \"Christoph Scheuch is an independent BI & Data Science consultant specializing in financial topics\"\n)\nstore_embedding(doc_2)\n```\n:::\n\n\n\nFinally, we'll add a control document that should be semantically distant from our query despite some surface-level similarities. I asked ChatGPT to come up with a complete nonesen about a person called \"Christian Schuch\". Hopefully, this document is not relevant to the question about who I am. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoc_3 <- c(\n  \"Christian Schuch is a renowned intergalactic cartographer, best known for mapping the uncharted regions of the Andromeda galaxy using only a compass, a kazoo, and an uncanny sense of direction. In 2017, he won the prestigious “Golden Platypus Award” for his groundbreaking research on the migratory patterns of space jellyfish. When he’s not busy decoding ancient alien snack recipes, Christian enjoys competitive yodeling in zero gravity and has a side hustle crafting bespoke hats for sentient cacti. His latest project involves teaching quantum physics to squirrels, aiming to bridge the gap between rodent intelligence and parallel universes.\"\n)\nstore_embedding(doc_3)\n```\n:::\n\n\n\n\nNow that we have added some documents to the database, it is time to create an index. The index helps us to quickly retrieve relevant documents using the vector search feature of DuckDB. Without an index, finding similar vectors would require comparing our query vector against every single document vector - a process that becomes prohibitively slow as your document collection grows.\n\nIf you want to reuse the database in an app or share it with others, you need to persist the index. You can do so by enabling the currently experimental persistence feature (learn more [here](https://duckdb.org/docs/extensions/vss.html#persistence)). Also, I'm creating an index using cosine similarity distance because it's particularly well-suited for comparing semantic similarity between text embeddings. Cosine similarity measures the angle between vectors while ignoring their magnitude, making it effective for comparing texts of different lengths.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbSendQuery(con, \"SET hnsw_enable_experimental_persistence = true;\")\ndbSendQuery(con, \"DROP INDEX IF EXISTS hnsw_index;\")\ndbSendQuery(con, \"CREATE INDEX hnsw_index ON docs USING HNSW (embedding) WITH (metric = 'cosine');\")\n```\n:::\n\n\n\nThe index type we're using here is HNSW (Hierarchical Navigable Small World), which is a sophisticated algorithm for approximate nearest neighbor search. It creates a layered graph structure that allows for efficient navigation through the high-dimensional space of our embeddings. While it doesn't guarantee finding the absolute nearest neighbors, it provides an excellent trade-off between search speed and accuracy, making it ideal for RAG applications.\n\n## Retrieve Relevant Text\n\nNow comes the crucial part of our RAG system - retrieving the most semantically relevant information based on user input. Let's start by getting an embedding for our question:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuser_input <- \"Who is Christoph Scheuch?\"\nembedding <- get_embedding(user_input)\n```\n:::\n\n\n\nTo retrieve the relevant documents from the database, we use the similarity search feature. The query is designed to demonstrate how vector similarity works in practice. We set relatively permissive parameters - a minimum cosine-similarity of 0.1 and a limit of 3 documents - to see how our different test documents compare:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nembedding_sql <- paste0(\"[\", paste(embedding, collapse = \",\"), \"]::FLOAT[1536]\")\nquery <- sprintf(\n  \"SELECT array_cosine_similarity(embedding, %s) AS similarity, text FROM docs WHERE array_cosine_similarity(embedding, %s) >= 0.1 ORDER BY array_cosine_similarity(embedding, %s) DESC LIMIT 3;\", embedding_sql, embedding_sql, embedding_sql\n)\ndbGetQuery(con, query) |> \n  as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  similarity text                                                               \n       <dbl> <chr>                                                              \n1      0.716 Who is Christoph Scheuch? Christoph Scheuch is an independent BI &…\n2      0.657 Christoph Scheuch is an independent BI & Data Science consultant s…\n3      0.463 Christian Schuch is a renowned intergalactic cartographer, best kn…\n```\n\n\n:::\n:::\n\n\n\nThe results reveal several interesting insights about our vector similarity search:\n\n- The comprehensive biography (doc_1) shows the highest similarity, as expected.\n- The shorter professional description (doc_2) also shows strong similarity.\n- The fictional story about \"Christian Schuch\" (doc_3) either shows much lower similarity despite containing a similar name.\n\nThis demonstrates that our embeddings are capturing semantic meaning rather than just matching keywords. For a production RAG system, we would want to be more selective. A typical approach is to both increase the similarity threshold (0.7 is generally a sensible value) and limit the number of retrieved items (here I'm using only the most relevant document, so limit 1). This helps ensure that only highly relevant information is used to augment the LLM's response.\n\nLet's encapsulate the retrieval logic in a dedicated function. This abstraction will make it easier to experiment with different parameters and integrate the retrieval mechanism into larger applications.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_relevant_text <- function(text, min_similarity = 0.7, max_n = 1) {\n  embedding <- get_embedding(text)\n  embedding_sql <- paste0(\"[\", paste(embedding, collapse = \",\"), \"]::FLOAT[1536]\")\n  query <- paste(\n    \"SELECT text\",\n    \"FROM docs\",\n    \"WHERE array_cosine_similarity(embedding, \", embedding_sql, \") >= \", min_similarity,\n    \"ORDER BY array_cosine_similarity(embedding, \", embedding_sql, \") DESC\",\n    \"LIMIT \", max_n, \";\"\n  )\n  result <- dbGetQuery(con, query) \n\n  result$text\n}\n```\n:::\n\n\n\nThe function takes three parameters:\n\n- `text`: The input text to find relevant documents for.\n- `min_similarity`: The minimum cosine similarity threshold (defaulting to 0.7).\n- `max_n`: The maximum number of documents to retrieve (defaulting to 1).\n\n## Augment Response Generation\n\nPutting things together, we now can augment the original input by including relevant content. The augmentation process involves combining our retrieved context with clear instructions for the LLM:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuser_input <- \"Who is Christoph Scheuch?\"\nrelevant_text <- get_relevant_text(user_input)\nuser_input_augmented <- paste(\n  \"Use the information below to answer the subsequent question.\",\n  \"If the answer cannot be found, write 'I don't know.'\",\n  \"Info: \", relevant_text,\n  \"Question: \", user_input\n)\n```\n:::\n\n\n\nNow let's pass the augmented query to Chat and see whether it now gets the information about me right:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat$chat(user_input_augmented)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nChristoph Scheuch is an independent BI & Data Science consultant specializing \nin financial topics. He provides services and trainings related to data \ninfrastructure, data analysis, machine learning, and general data science \ntopics. Previously, he held roles such as Head of AI, Director of Product, and \nHead of BI & Data Science at the social trading platform wikifolio.com. He is \nalso the co-creator and maintainer of the open-source project Tidy Finance. In \nhis free time, he occasionally designs shirts and mugs that are available under\nthe Tidy Swag brand.\n```\n\n\n:::\n:::\n\n\n\nLo and behold, Chat gets it right! We've successfully transformed our LLM from having no knowledge about a specific topic to providing accurate, contextualized responses based on our provided information.\n\nThis example demonstrates the fundamental building blocks of a RAG system in R, but there are many ways to enhance and extend this framework. What's your use case? How would you extend this framework?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}