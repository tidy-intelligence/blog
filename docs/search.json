[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy Intelligence",
    "section": "",
    "text": "Clustering Binary Data\n\n\nAn application of different clustering approaches to simulated survey responses\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data: A Recipe for Efficient Data Analysis\n\n\nOn the importance of tidy data for efficient analysis using the analogy of a well-organized kitchen\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html",
    "href": "posts/clustering-binary-data/index.html",
    "title": "Clustering Binary Data",
    "section": "",
    "text": "In this post I tackle the challenge to extract a small number of typical respondent profiles from a large scale survey with multiple yes-no questions. This type of setting corresponds to a classification problem without knowing the true labels of the observations â€“ also known as unsupervised learning. Since I regularly face tasks in this area, I decided to start an irregular series of blogs that touch upon practical aspects of unsupervised learning in R using tidy principles.\nTechnically speaking, I have a set of \\(N\\) observations \\((x_1, x_2, ... , x_N)\\) of a random \\(p\\)-vector \\(X\\) with joint density \\(\\text{Pr}(X)\\). The goal of classification is to directly infer the properties of this probability density without the help of the correct answers (or degree-of-error) for each observation. In this note I focus on cluster analysis that attempts to find convex regions of the \\(X\\)-space that contain modes of \\(\\text{Pr}(X)\\). This approach aims to tell whether \\(\\text{Pr}(X)\\) can be represented by a mixture of simpler densities representing distinct classes of observations.\nIntuitively, I want to find clusters of the survey responses such that respondents within each cluster are more closely related to one another than respondents assigned to different clusters. There are many possible ways to achieve that, but I focus on the most popular and most approachable ones: \\(K\\)-means, \\(K\\)-modes, as well as agglomerative and divisive hierarchical clustering. AS we see below, the 4 models yield quite different results for clustering binary data.\nI mainly use the tidyverse family of packages throughout this post.\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#creating-sample-data",
    "href": "posts/clustering-binary-data/index.html#creating-sample-data",
    "title": "Clustering Binary Data",
    "section": "Creating sample data",
    "text": "Creating sample data\nLet us start by creating some sample data where we basically exactly know which kind of answer profiles are out there. Later, we evaluate the cluster models according to how well they are doing in uncovering the clusters and assigning respondents to clusters. I assume that there are 4 yes/no questions labeled q1, q2, q3 and q4. In addition, there are 3 different answer profiles where cluster 1 answers positively to the first question only, cluster 2 answers positively to question 2 and 3 and cluster 3 answers all questions positively. I also define the the number of respondents for each cluster.\n\ncenters &lt;- tibble(\n  cluster = factor(1:3), \n  respondents = c(250, 500, 200),\n  q1 = c(1, 0, 1),\n  q2 = c(0, 1, 1),             \n  q3 = c(0, 1, 1),\n  q4 = c(0, 0, 1)\n)\n\nAlternatively, we could think of the yes/no questions as medical records that indicate whether the subject has a certain pre-condition or not.\nSince it should be a bit tricky for the clustering models to find the actual response profiles, let us add some noise in the form of respondents that deviate from their assigned cluster profile. We find out below how the cluster algorithms are able to deal with this noise.\n\nset.seed(123)\nlabelled_respondents &lt;- centers |&gt; \n  mutate(\n    across(\n      starts_with(\"q\"),\n      ~map2(respondents, .x, function(x, y) {\n        rbinom(x, 1, max((y - 0.1), 0.1))\n      }),\n      .names = \"{col}\"\n    )\n  ) |&gt; \n  select(-respondents) |&gt; \n  unnest(cols = c(q1, q2, q3, q4))\n\nThe figure below visualizes the distribution of simulated question responses by cluster.\n\nlabelled_respondents |&gt;\n  pivot_longer(cols = -cluster, names_to = \"question\", values_to = \"response\") |&gt;\n  mutate(response = response == 1) |&gt;\n  ggplot(aes(x = response, y = question, color = cluster)) +\n  geom_jitter() +\n  theme_bw() +\n  labs(x = \"Response\", y = \"Question\", color = \"Cluster\",\n       title = \"Visualization of simulated question responses by cluster\")"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#k-means-clustering",
    "href": "posts/clustering-binary-data/index.html#k-means-clustering",
    "title": "Clustering Binary Data",
    "section": "\\(K\\)-means clustering",
    "text": "\\(K\\)-means clustering\nThe \\(K\\)-means algorithm is one of the most popular clustering methods (see also this tidymodels example). It is intended for situations in which all variables are of the quantitative type since it partitions all respondents into \\(k\\) groups such that the sum of squares from respondents to the assigned cluster centers are minimized. For binary data, the Euclidean distance reduces to counting the number of variables on which two cases disagree.\nThis leads to a problem (which is also described here) because of an arbitrary cluster assignment after cluster initialization. The first chosen clusters are still binary data and hence observations have integer distances from each of the centers. The corresponding ties are hard to overcome in any meaningful way. Afterwards, the algorithm computes means in clusters and revisits assignments. Nonetheless, \\(K\\)-means might produce informative results in a fast and easy to interpret way. I hence include it in this post for comparison.\nTo run the \\(K\\)-means algorithm, we first drop the cluster column.\n\nrespondents &lt;- labelled_respondents |&gt;\n  select(-cluster)\n\nIt is very straight-forward to run the built-in stats::kmeans clustering algorithm. I choose the parameter of maximum iterations to be 100 to increase the likeliness of getting the best fitting clusters. Since the data is fairly small and the algorithm is also quite fast, I see no harm in using a high number of iterations.\n\niter_max &lt;- 100\nkmeans_example &lt;- stats::kmeans(respondents, centers = 3, iter.max = iter_max)\n\nThe output of the algorithm is a list with different types of information including the assigned clusters for each respondent.\nAs we want to compare cluster assignment across different models and we repeatedly assign different clusters to respondents, I write up a helper function that adds assignments to the respondent data from above. The function shows that \\(K\\)-means and \\(K\\)-modes contain a field with cluster information. The two hierarchical cluster models, however, need to be cut a the desired number of clusters (more on that later).\n\nassign_clusters &lt;- function(model, k = NULL) {\n  if (class(model)[1] %in% c(\"kmeans\", \"kmodes\")) {\n    cluster_assignment &lt;- model$cluster\n  }\n  if (class(model)[1] %in% c(\"agnes\", \"diana\")) {\n    if (is.null(k)) {\n      stop(\"k required for hierarchical models!\")\n    }\n    cluster_assignment &lt;- stats::cutree(model, k = k)\n  }\n  \n  clusters &lt;- respondents |&gt;\n    mutate(cluster = cluster_assignment)\n  \n  return(clusters)\n}\n\nIn addition, I introduce a helper function that summarizes information by cluster. In particular, the function computes average survey responses (which correspond to proportion of yes answers in the current setting) and sorts the clusters according to the total number of positive answers. The latter helps us later to compare clusters across different models.\n\nsummarize_clusters &lt;- function(model, k = NULL) {\n  # Assign respondents to clusters\n  clusters &lt;- assign_clusters(model = model, k = k)\n  \n  # Compute summary statistics by cluster\n  summary_statistics &lt;- clusters |&gt;\n    group_by(cluster) |&gt;\n    summarize(across(matches(\"q\"), \\(x) mean(x, na.rm = TRUE)),\n              assigned_respondents = n()) |&gt;\n    select(-cluster) |&gt;\n    # Rank clusters by total share of positive answers\n    mutate(total = rowSums(across(matches(\"q\")))) |&gt;\n    arrange(-total) |&gt;\n    mutate(k = row_number(),\n           model = class(model)[1])\n  \n  return(summary_statistics)\n}\n\nWe could easily introduce other summary statistics into the function, but the current specification is sufficient for the purpose of this note.\n\nkmeans_example &lt;- summarize_clusters(kmeans_example)\n\nSince we do not know the true number of clusters in real-world settings, we want to compare the performance of clustering models for different numbers of clusters. Since we know that the true number of clusters is 3 in the current setting, let us stick to a maximum of 7 clusters. In practice, you might of course choose an arbitrary maximum number of clusters.\n\nk_min &lt;- 1\nk_max &lt;- 7\n\nkmeans_results &lt;- tibble(k = k_min:k_max) |&gt;\n  mutate(\n    kclust = map(k, ~kmeans(respondents, centers = .x, iter.max = iter_max)),\n  )\n\nA common heuristic to determine the optimal number of clusters is the elbow method where we plot the within-cluster sum of squared errors of an algorithm for increasing number of clusters. The optimal number of clusters corresponds to the point where adding another cluster does lead to much of an improvement anymore. In economic terms, we look for the point where the diminishing returns to an additional cluster are not worth the additional cost (assuming that we want the minimum number of clusters with optimal predictive power).\nThe function below computes the within-cluster sum of squares for any cluster assignments.\n\ncompute_withinss &lt;- function(model, k = NULL) {\n  # Assign respondents to clusters\n  clusters &lt;- assign_clusters(model = model, k = k)\n  \n  # Compute averages per cluster center\n  centers &lt;- clusters |&gt;\n    group_by(cluster) |&gt;\n    summarize_all(mean) |&gt;\n    pivot_longer(cols = -cluster, names_to = \"question\", values_to = \"cluster_mean\")\n  \n  # Compute sum of squared differences from cluster centers\n  withinss &lt;- clusters |&gt;\n    pivot_longer(cols = -cluster, names_to = \"question\", values_to = \"response\") |&gt;\n    left_join(centers, by = c(\"cluster\", \"question\")) |&gt;\n    summarize(k = max(cluster),\n              withinss = sum((response - cluster_mean)^2)) |&gt;\n    mutate(model = class(model)[1])\n  \n  return(withinss)\n}\n\nWe can simply map the function across our list of \\(K\\)-means models. For better comparability, we normalize the within-cluster sum of squares for any number of cluster by the benchmark case of only having a single cluster. Moreover, we consider log-differences to because we care more about the percentage decrease in sum of squares rather than the absolute number.\n\nkmeans_logwithindiss &lt;- kmeans_results$kclust |&gt;\n  map(compute_withinss) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#k-modes-clustering",
    "href": "posts/clustering-binary-data/index.html#k-modes-clustering",
    "title": "Clustering Binary Data",
    "section": "\\(K\\)-modes clustering",
    "text": "\\(K\\)-modes clustering\nSince \\(K\\)-means is actually not ideal for binary (or hierarchical data in general), Huang (1997) came up with the \\(K\\)-modes algorithm. This clustering approach aims to partition respondents into \\(K\\) groups such that the distance from respondents to the assigned cluster modes is minimized. A mode is a vector of elements that minimize the dissimilarities between the vector and each object of the data. Rather than using the Euclidean distance, \\(K\\)-modes uses simple matching distance between respondents to quantify dissimilarity which translates into counting the number of mismatches in all question responses in the current setting.\nFortunately, the klaR package provides an implementation of the \\(K\\)-modes algorithm that we can apply just like the \\(K\\)-means above.\n\nkmodes_example &lt;- klaR::kmodes(respondents, iter.max = iter_max, modes = 3) |&gt;\n  summarize_clusters()\n\nSimilarly, we just map the model across different numbers of target cluster modes and compute the within-cluster sum of squares.\n\nkmodes_results &lt;- tibble(k = k_min:k_max) |&gt;\n  mutate(\n    kclust = map(k, ~klaR::kmodes(respondents, modes = ., iter.max = iter_max))\n  )\n\nkmodes_logwithindiss &lt;- kmodes_results$kclust |&gt;\n  map(compute_withinss) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))\n\nNote that I computed the within-cluster sum of squared errors rather than using the within-cluster simple-matching distance provided by the function itself. The latter counts the number of differences from assigned respondents to their cluster modes."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "",
    "text": "Imagine trying to cook a meal in a disorganized kitchen where ingredients are mixed up and nothing is labeled. It would be chaotic and time-consuming to look for the right ingredients and there might be some trial error involved, possibly ruining your planned meal.\nTidy data are like well-organized shelves in your kitchen. Each shelf provides a collection of containers that semantically belong together, e.g., spices or dairies. Each container on the shelf holds one type of ingredient, and the labels on the containers clearly describe what is inside, e.g., pepper or milk. In the same way, tidy data organizes information into a clear and consistent format, where each type of observational unit forms a table, each variable is in a column, and each observation is in a row (Wickham 2014).\nTidying data is about structuring datasets to facilitate analysis, visualization, report generation, or modelling. By following the principle that each variable forms a column, each observation forms a row, and each type of observational unit forms a table, data analysis becomes more intuitive, akin to cooking in a well-organized kitchen where everything has its place and you spend less time on searching for ingredients."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#example-for-tidy-data",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#example-for-tidy-data",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "Example for tidy data",
    "text": "Example for tidy data\nTo illustrate the concept of tidy data in our tidy kitchen, suppose we have a table called ingredient that contains information about all the ingredients that we currently have in our kitchen. It might look as follows:\n\n\n\nname\nquantity\nunit\ncategory\n\n\n\n\nflour\n500\ngrams\nbaking\n\n\nsugar\n200\ngrams\nbaking\n\n\nbutter\n100\ngrams\ndairy\n\n\neggs\n4\nunits\ndairy\n\n\nmilk\n1\nliters\ndairy\n\n\nsalt\n10\ngrams\nseasoning\n\n\nolive oil\n0.2\nliters\noil\n\n\ntomatoes\n300\ngrams\nvegetable\n\n\nchicken\n400\ngrams\nmeat\n\n\nrice\n250\ngrams\ngrain\n\n\n\nEach row refers to a specific ingredient and each column has a dedicated type and meaning. For instance, the column quantity contains information about how much of the ingredient called name we currently have and which unit we use to measure it.\nSimilarly, we could have a table just for dairy that might look as follows:\n\n\n\nname\nquantity\nunit\n\n\n\n\nmilk\n1\nliters\n\n\nbutter\n200\ngrams\n\n\nyogurt\n150\ngrams\n\n\ncheese\n100\ngrams\n\n\ncream\n0.5\nliters\n\n\ncottage cheese\n250\ngrams\n\n\nsour cream\n150\ngrams\n\n\nghee\n100\ngrams\n\n\nwhipping cream\n0.3\nliters\n\n\nice cream\n500\ngrams\n\n\n\nNotice that there is no category column in this table? It would actually be redundant to have this column because all rows in the `dairy`` table have the same category."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-colum-headers-are-values-not-variable-names",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-colum-headers-are-values-not-variable-names",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When colum headers are values, not variable names",
    "text": "When colum headers are values, not variable names\nNow let us move to data structures that are untidy. Consider the following variant of our dairy table:\n\n\n\ntype\nliters\ngrams\n\n\n\n\nmilk\n1\n\n\n\nbutter\n\n200\n\n\nyogurt\n\n150\n\n\ncheese\n\n100\n\n\ncream\n0.5\n\n\n\ncottage cheese\n\n250\n\n\nsour cream\n\n150\n\n\nghee\n\n100\n\n\nwhipping cream\n0.3\n\n\n\nice cream\n\n500\n\n\n\nWhat is the issue here? Each row still refers to a specific dairy product. However, instead of dedicated quantity and unit columns, we have a liters and grams column. Since the units differ across dairy products, the table even contains missing values in the form of emtpy cells. So if you want to find out how much of ice cream you still have, you need to also check out the column name. In practice, we would create dedicated quantity and unit columns. we might even decide to have the same unit for all ingredients (e.g., measure everything in grams) and just keep a quantity column."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-multiple-variables-are-stored-in-one-column",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-multiple-variables-are-stored-in-one-column",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When multiple variables are stored in one column",
    "text": "When multiple variables are stored in one column\nLet us consider the following untidy version of our ingredient table.\n\n\n\ntype\nquantity_and_unit\n\n\n\n\nflour\n500 grams\n\n\nsugar\n200 grams\n\n\nbutter\n100 grams\n\n\neggs\n4 units\n\n\nmilk\n1 liter\n\n\nsalt\n10 grams\n\n\nolive oil\n0.2 liters\n\n\ntomatoes\n300 grams\n\n\nchicken\n400 grams\n\n\nrice\n250 grams\n\n\n\nThis one is really annoying, since the quantity_and_unit column combines both the quantity and the unit of measurement into one string for each ingredient. Why is this an issue? This format actually makes it harder to perform numerical operations on the quantities or to filter or aggregate the data based on the unit of measurement. So in practice, we would actually start our data analysis by splitting out the quantity_and_unit column into quantity and unit."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-variables-are-stored-in-both-rows-and-columns",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-variables-are-stored-in-both-rows-and-columns",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When variables are stored in both rows and columns",
    "text": "When variables are stored in both rows and columns\nLet us extend our kitchen analogy by additionally considering recipes. For simplicity, a recipe just denotes how much of each ingredient is required. The following table contains two variants of a recipe for pancakes:\n\n\n\ningredient\nrecipe1_quantity\nrecipe2_quantity\n\n\n\n\nflour\n500 grams\n300 grams\n\n\nsugar\n200 grams\n150 grams\n\n\nbutter\n100 grams\n50 grams\n\n\neggs\n4 units\n3 units\n\n\nmilk\n1 liters\n0.5 liters\n\n\n\nThe quantity for each ingredient for two different recipes is stored in separate columns. This structure makes it harder to perform operations like filtering or summarizing the data by recipe or ingredient.\nTo convert this data to a tidy format, you would typically want to gather the quantities into a single column, and include additional columns to specify the recipe and unit of measurement for each quantity. We can then filer"
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-there-are-multiple-types-of-data-in-the-same-column",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-there-are-multiple-types-of-data-in-the-same-column",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When there are multiple types of data in the same column",
    "text": "When there are multiple types of data in the same column\nA recipe typically contains information on the required utensils and how much time a step requires. Consider the following table with different types of data:\n\n\n\ntype\nquantity\ncategory\n\n\n\n\nflour\n500 grams\ningredient\n\n\nbutter\n100 grams\ningredient\n\n\nwhisk\n1 unit\nutensil\n\n\nsugar\n200 grams\ningredient\n\n\nbaking time\n30 minutes\ntime\n\n\n\nThe table is trying to describe a recipe but combines different types of data within the same columns. There are ingredients with their quantities, a utensil, and cooking time, all mixed together.\nA tidy approach would typically separate these different types of data into separate tables or at least into distinct sets of columns, making it clear what each part of the data represents and facilitating further analysis and visualization."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-some-data-is-missing",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-some-data-is-missing",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When some data is missing",
    "text": "When some data is missing\nAs a last example for untidy data, let us consider the original ingredient table again, but with a few empty cells.\nKey points:\n\nHuge difference between NA and 0 (or any other value)\nAre you sure that you donâ€™t have the ingredient or do you just donâ€™t know?\nMissing are dropped in filters\n\n\n\n\nname\nquantity\nunit\n\n\n\n\nflour\n\ngrams\n\n\nsugar\n200\ngrams\n\n\nbutter\n100\ngrams\n\n\neggs\n4\nunits\n\n\nmilk\n10\n\n\n\nsalt\n10\ngrams\n\n\nolive oil\n0.2\nliters\n\n\ntomatoes\n300\ngrams\n\n\nchicken\n400\ngrams\n\n\n\n250\ngrams\n\n\n\nWhat is the issue here? There are actually a couple of them:\n\nThe flour row does have any information about quantity, so we just donâ€™t know how much we have.\nThe milk row does not contain a unit, so we might have 10 liters, 10 milliliters, or 10 cups of milk.\nThe last row does not have any name, so we have 250 grams of something that we just canâ€™t identify.\n\nWhy is this important? It makes a huge difference how me treat the missing information. For instance, we might make an educated guess for milk if we always record that information in litres, then the missing unit is very likely litres. For flour, we could play it safe and just say that the available quantity is zero. For the ingredient without a name, we might have to throw it away or ask somebody else to tell us what it is.\nOverall, these examples highlight the most important issues that you might have to consider when preparing data for your analysis."
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#hierarchical-clustering",
    "href": "posts/clustering-binary-data/index.html#hierarchical-clustering",
    "title": "Clustering Binary Data",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nAs an alternative to computing optimal assignments for a given number of clusters, we might sometimes prefer to arrange the clusters into a natural hierarchy. This involves successively grouping the clusters themselves such that at each level of the hierarchy, clusters within the same group are more similar to each other than those in different groups. There are two fundamentally different approaches to hierarchical clustering that are fortunately implemented in the great cluster package.\nBoth hierarchical clustering approaches require a dissimilarity or distance matrix. Since we have binary data, we choose the asymmetric binary distance matrix based on the Jaccard distance. Intuitively, the Jaccard distance measures how far the overlap of responses between two groups is from perfect overlap.\n\ndissimilarity_matrix &lt;- stats::dist(respondents, method = \"binary\")\n\nAgglomerative clustering start at the bottom and at each level recursively merge a selected pair of clusters into a single cluster. This produces a clustering at the next higher level with one less cluster. The pair chosen for merging consist of the two clusters with the smallest within-cluster dissimilarity. On an intuitive level, agglomerative clustering is hence better in discovering small clusters.\nThe cluster package provides the agnes algorithm (AGglomerative NESting) that can easily applied to the dissimilarity matrix.\n\nagnes_results &lt;- cluster::agnes(dissimilarity_matrix, diss = TRUE, keep.diss = TRUE, method = \"complete\")\n\nThe function returns a clustering tree that we could plot (which I actually rarely found really helpful) or cut into different partitions using the stats::cutree function. This is why the helper functions from above need a number of target clusters as an input for hierarchical clustering models. However, the logic of the summary statistics are just as above.\n\nagnes_example &lt;- summarize_clusters(agnes_results, k = 3)\n\nagnes_logwithindiss &lt;- k_min:k_max |&gt;\n  map(~compute_withinss(agnes_results, .)) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))\n\nDivisive methods start at the top and at each level recursively split one of the existing clusters at that level into two new clusters. The split is chosen such that two new groups with the largest between-group dissimilarity emerge. Intuitively speaking, divisive clustering is thus better in discovering large clusters.\nThe cluster package provides the diana algorithm (DIvise ANAlysis) for this clustering approach where the logic is basically the same as for the agnes model.\n\ndiana_results &lt;- cluster::diana(dissimilarity_matrix, diss = TRUE, keep.diss = TRUE) \ndiana_example &lt;- diana_results |&gt;\nsummarize_clusters(k = 3)\ndiana_logwithindiss &lt;-  k_min:k_max |&gt;\n  map(~compute_withinss(diana_results, .)) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#model-comparison",
    "href": "posts/clustering-binary-data/index.html#model-comparison",
    "title": "Clustering Binary Data",
    "section": "Model comparison",
    "text": "Model comparison\nLet us start the model comparison by looking at the within cluster sum of squares for different numbers of clusters. The figure shows that the (K)-modes algorithm improves the fastest towards the true number of 3 clusters. The elbow method would suggest in this case to stick with 3 clusters for this algorithm.\nFor the other models, the picture is less clear: the curve of (K)-means would rather suggest having 2 or 4 clusters. But then again this might be the result of initial conditions as the (K)-means algorithm assigns respondents to clusters in a rather arbitrary way at cluster initialization. The divise model would also suggest 3 clusters as there is no improvement to using 4 clusters, but the overall sum of squares does not improve much suggesting a lot of mis-classification in the first 2 clusters. The agglomerative model exhibits the worst performance since it shows no clear suggestion for an optimal cluster and the first 2 clusters seem to be only marginally better than having a single cluster.\n\nbind_rows(kmeans_logwithindiss, kmodes_logwithindiss,\n          agnes_logwithindiss, diana_logwithindiss) |&gt;\n  ggplot(aes(x = k, y = logwithindiss, color = model, linetype = model)) +\n  geom_line() +\n  scale_x_continuous(breaks = k_min:k_max) + \n  theme_minimal() +\n  labs(x = \"Number of Clusters\", y = bquote(log(W[k])-log(W[1])), color = \"Model\", linetype = \"Model\",\n       title = \"Within cluster sum of squares relative to benchmark case of one cluster\")\n\n\n\n\n\n\n\n\nNow, let us compare the proportion of positive responses within assigned clusters across models. Recall that I ranked clusters according to the total share of positive answers to ensure comparability. This approach is only possible in this type of setting where we can easily introduce such a ranking. The figure shows that all models are doing well in discovering the cluster with a single response. The cluster with 2 positive responses seems to be discovered by diana, (K)-means and (K)-modes, while agnes is clearly off. The cluster with only positive responses is again discovered by (K)-means and (K)-modes whereas the hierarchical models perform rather poorly. Overall, this picture again suggests that (K)-modes performs best for the current setting.\n\nbind_rows(\n  kmeans_example,\n  kmodes_example,\n  agnes_example,\n  diana_example) |&gt;\n  select(-c(total, assigned_respondents)) |&gt;\n  pivot_longer(cols = -c(k, model), names_to = \"question\", values_to = \"response\") |&gt;\n  mutate(cluster = str_c(\"Cluster \", k)) |&gt;\n  ggplot(aes(x = response, y = question, fill = model)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~cluster) +\n  theme_bw() +\n  scale_x_continuous(labels = scales::percent) + \n  geom_hline(yintercept = seq(1.5, length(unique(colnames(respondents)))-0.5, 1),\n             colour = 'black') +\n  labs(x = \"Proportion of responses\", y = \"Question\", fill = \"Model\",\n       title = \"Proportion of positive responses within assigned clusters\")\n\n\n\n\n\n\n\n\nFinally, let us check how well each model assigns respondents to the true cluster which is obviously not possible in real unsupervised applications. The figure below shows the true number of respondents by cluster as a dashed box and the assigned respondents as bars. Again, agnes and diana do a pretty bad job for the current setting, while (K)-means and (K)-modes show quite simiar assignments that are not too far off from the true ones.\n\nbind_rows(\n  kmeans_example,\n  kmodes_example,\n  agnes_example,\n  diana_example) |&gt;\n  mutate(cluster = str_c(\"Cluster \", k)) |&gt;\n  select(model, cluster, assigned_respondents) |&gt;\n  ggplot() +\n  geom_col(position = \"dodge\", aes(y = assigned_respondents, x = cluster, fill = model)) +\n  geom_col(data = labelled_respondents |&gt;\n             group_by(cluster = str_c(\"Cluster \", cluster)) |&gt;\n             summarize(assigned_respondents = n(),\n                       model = \"actual\"), aes(y = assigned_respondents, x = cluster), fill = \"white\", color = \"black\", alpha = 0, linetype = \"dashed\") +\n  theme_bw() +\n  labs(x = NULL, y = \"Number of assigned respondents\", fill = \"Model\",\n       title = \"Number of assigned respondents by cluster\",\n       subtitle = \"Dashed box indicates true number of respondents by cluster\")\n\n\n\n\n\n\n\n\nLet me end this post with a few words of caution: first, the ultimate outcome heavily depends on the seed chosen at the beginning of the post. The results might be quite different for other draws of respondents or initial conditions for clustering algorithms. Second, there are many more models out there that can be applied to the current setting. However, with this post I want to emphasize that it is important to consider different models at the same time and to compare them through a consistent set of measures. Ultimately, choosing the optimal number of clusters in practice requires a judgment call, but at least it can be informed as much as possible."
  }
]