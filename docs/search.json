[
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "",
    "text": "Imagine trying to cook a meal in a disorganized kitchen where ingredients are mixed up and nothing is labeled. It would be chaotic and time-consuming to look for the right ingredients and there might be some trial error involved, possibly ruining your planned meal.\nTidy data are like well-organized shelves in your kitchen. Each shelf provides a collection of containers that semantically belong together, e.g., spices or dairies. Each container on the shelf holds one type of ingredient, and the labels on the containers clearly describe what is inside, e.g., pepper or milk. In the same way, tidy data organizes information into a clear and consistent format, where each type of observational unit forms a table, each variable is in a column, and each observation is in a row (Wickham 2014).\nTidying data is about structuring datasets to facilitate analysis, visualization, report generation, or modelling. By following the principle that each variable forms a column, each observation forms a row, and each type of observational unit forms a table, data analysis becomes more intuitive, akin to cooking in a well-organized kitchen where everything has its place and you spend less time on searching for ingredients."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#example-for-tidy-data",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#example-for-tidy-data",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "Example for tidy data",
    "text": "Example for tidy data\nTo illustrate the concept of tidy data in our tidy kitchen, suppose we have a table called ingredient that contains information about all the ingredients that we currently have in our kitchen. It might look as follows:\n\n\n\nname\nquantity\nunit\ncategory\n\n\n\n\nflour\n500\ngrams\nbaking\n\n\nsugar\n200\ngrams\nbaking\n\n\nbutter\n100\ngrams\ndairy\n\n\neggs\n4\nunits\ndairy\n\n\nmilk\n1\nliters\ndairy\n\n\nsalt\n10\ngrams\nseasoning\n\n\nolive oil\n0.2\nliters\noil\n\n\ntomatoes\n300\ngrams\nvegetable\n\n\nchicken\n400\ngrams\nmeat\n\n\nrice\n250\ngrams\ngrain\n\n\n\nEach row refers to a specific ingredient and each column has a dedicated type and meaning. For instance, the column quantity contains information about how much of the ingredient called name we currently have and which unit we use to measure it.\nSimilarly, we could have a table just for dairy that might look as follows:\n\n\n\nname\nquantity\nunit\n\n\n\n\nmilk\n1\nliters\n\n\nbutter\n200\ngrams\n\n\nyogurt\n150\ngrams\n\n\ncheese\n100\ngrams\n\n\ncream\n0.5\nliters\n\n\ncottage cheese\n250\ngrams\n\n\nsour cream\n150\ngrams\n\n\nghee\n100\ngrams\n\n\nwhipping cream\n0.3\nliters\n\n\nice cream\n500\ngrams\n\n\n\nNotice that there is no category column in this table? It would actually be redundant to have this column because all rows in the `dairy`` table have the same category."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-colum-headers-are-values-not-variable-names",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-colum-headers-are-values-not-variable-names",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When colum headers are values, not variable names",
    "text": "When colum headers are values, not variable names\nNow let us move to data structures that are untidy. Consider the following variant of our dairy table:\n\n\n\ntype\nliters\ngrams\n\n\n\n\nmilk\n1\n\n\n\nbutter\n\n200\n\n\nyogurt\n\n150\n\n\ncheese\n\n100\n\n\ncream\n0.5\n\n\n\ncottage cheese\n\n250\n\n\nsour cream\n\n150\n\n\nghee\n\n100\n\n\nwhipping cream\n0.3\n\n\n\nice cream\n\n500\n\n\n\nWhat is the issue here? Each row still refers to a specific dairy product. However, instead of dedicated quantity and unit columns, we have a liters and grams column. Since the units differ across dairy products, the table even contains missing values in the form of emtpy cells. So if you want to find out how much of ice cream you still have, you need to also check out the column name. In practice, we would create dedicated quantity and unit columns. we might even decide to have the same unit for all ingredients (e.g., measure everything in grams) and just keep a quantity column."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-multiple-variables-are-stored-in-one-column",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-multiple-variables-are-stored-in-one-column",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When multiple variables are stored in one column",
    "text": "When multiple variables are stored in one column\nLet us consider the following untidy version of our ingredient table.\n\n\n\ntype\nquantity_and_unit\n\n\n\n\nflour\n500 grams\n\n\nsugar\n200 grams\n\n\nbutter\n100 grams\n\n\neggs\n4 units\n\n\nmilk\n1 liter\n\n\nsalt\n10 grams\n\n\nolive oil\n0.2 liters\n\n\ntomatoes\n300 grams\n\n\nchicken\n400 grams\n\n\nrice\n250 grams\n\n\n\nThis one is really annoying, since the quantity_and_unit column combines both the quantity and the unit of measurement into one string for each ingredient. Why is this an issue? This format actually makes it harder to perform numerical operations on the quantities or to filter or aggregate the data based on the unit of measurement. So in practice, we would actually start our data analysis by splitting out the quantity_and_unit column into quantity and unit."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-variables-are-stored-in-both-rows-and-columns",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-variables-are-stored-in-both-rows-and-columns",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When variables are stored in both rows and columns",
    "text": "When variables are stored in both rows and columns\nLet us extend our kitchen analogy by additionally considering recipes. For simplicity, a recipe just denotes how much of each ingredient is required. The following table contains two variants of a recipe for pancakes:\n\n\n\ningredient\nrecipe1_quantity\nrecipe2_quantity\n\n\n\n\nflour\n500 grams\n300 grams\n\n\nsugar\n200 grams\n150 grams\n\n\nbutter\n100 grams\n50 grams\n\n\neggs\n4 units\n3 units\n\n\nmilk\n1 liters\n0.5 liters\n\n\n\nThe quantity for each ingredient for two different recipes is stored in separate columns. This structure makes it harder to perform operations like filtering or summarizing the data by recipe or ingredient.\nTo convert this data to a tidy format, you would typically want to gather the quantities into a single column, and include additional columns to specify the recipe and unit of measurement for each quantity. We can then filer"
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-there-are-multiple-types-of-data-in-the-same-column",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-there-are-multiple-types-of-data-in-the-same-column",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When there are multiple types of data in the same column",
    "text": "When there are multiple types of data in the same column\nA recipe typically contains information on the required utensils and how much time a step requires. Consider the following table with different types of data:\n\n\n\ntype\nquantity\ncategory\n\n\n\n\nflour\n500 grams\ningredient\n\n\nbutter\n100 grams\ningredient\n\n\nwhisk\n1 unit\nutensil\n\n\nsugar\n200 grams\ningredient\n\n\nbaking time\n30 minutes\ntime\n\n\n\nThe table is trying to describe a recipe but combines different types of data within the same columns. There are ingredients with their quantities, a utensil, and cooking time, all mixed together.\nA tidy approach would typically separate these different types of data into separate tables or at least into distinct sets of columns, making it clear what each part of the data represents and facilitating further analysis and visualization."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-some-data-is-missing",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-some-data-is-missing",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When some data is missing",
    "text": "When some data is missing\nAs a last example for untidy data, let us consider the original ingredient table again, but with a few empty cells.\nKey points:\n\nHuge difference between NA and 0 (or any other value)\nAre you sure that you don’t have the ingredient or do you just don’t know?\nMissing are dropped in filters\n\n\n\n\nname\nquantity\nunit\n\n\n\n\nflour\n\ngrams\n\n\nsugar\n200\ngrams\n\n\nbutter\n100\ngrams\n\n\neggs\n4\nunits\n\n\nmilk\n10\n\n\n\nsalt\n10\ngrams\n\n\nolive oil\n0.2\nliters\n\n\ntomatoes\n300\ngrams\n\n\nchicken\n400\ngrams\n\n\n\n250\ngrams\n\n\n\nWhat is the issue here? There are actually a couple of them:\n\nThe flour row does have any information about quantity, so we just don’t know how much we have.\nThe milk row does not contain a unit, so we might have 10 liters, 10 milliliters, or 10 cups of milk.\nThe last row does not have any name, so we have 250 grams of something that we just can’t identify.\n\nWhy is this important? It makes a huge difference how me treat the missing information. For instance, we might make an educated guess for milk if we always record that information in litres, then the missing unit is very likely litres. For flour, we could play it safe and just say that the available quantity is zero. For the ingredient without a name, we might have to throw it away or ask somebody else to tell us what it is.\nOverall, these examples highlight the most important issues that you might have to consider when preparing data for your analysis."
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "",
    "text": "In this post, I provide a simple approach to scrape Environmental, Social and Governance (ESG) information from Yahoo Finance (e.g., Apple) using the programming language R. Yahoo Finance provides total ESG scores, environment, social and governance risk scores, as well as controversy levels, all compiled by Sustainalytics (which is by now owned by Morningstar). My code builds on the walk-through by Kyle Ruden, which I adapted to the current page structure of Yahoo Finance and my own coding style. In addition, I added a few steps that I, as web scraping newbie, had to look up while going through his guide.\nTo begin with, I want to urge you to read at least the legal and ethical considerations put forward by Kyle. Most importantly, I want to mention that, when performing web scraping tasks, it is both good practice and often required to set a custom user agent request header to identify yourself, as well as sending requests at a modest rate to ‘smell like a human’. I consider both of these key aspects in my code below.\nThroughout this post, I rely on the following packages:\nlibrary(tidyverse) # overall grammar\nlibrary(tidytext)  # only for reorder_within & scale_y_reordered functions\nlibrary(scales)    # only for percent function\nlibrary(httr2)     # for making http requests\nlibrary(rvest)     # for web scraping function\nlibrary(robotstxt) # only for paths_allowed function"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#get-symbols",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#get-symbols",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Get Symbols",
    "text": "Get Symbols\nFirst, we want to get some companies for which we want to scrap ESG information from Yahoo Finance. Let us get a table of symbols and industry information of the S&P 500 constituents from Wikipedia. The function read_html normalizes the page to a valid XML document. html_nodes then allows us to point exactly to the table we can find on the website using the name of the CSS node. html_table then parses the HTML table into a data frame. Note that, as one of the last steps, we need to replace all dots in the symbols with dashes to get the symbols used by Yahoo Finance.\n\nwikipedia_link &lt;- \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n\nsymbols &lt;- read_html(wikipedia_link) |&gt; \n  html_nodes(css = \"table[id='constituents']\") |&gt; \n  html_table() \n\nsymbols &lt;- symbols[[1]] |&gt; \n  select(symbol = Symbol, \n         company = Security, \n         sector = `GICS Sector`, \n         industry = `GICS Sub-Industry`) |&gt; \n  mutate(symbol = str_replace(symbol, \"[.]\", \"-\")) |&gt; \n  arrange(symbol)\n\nThe following chunk prints what we got from Wikipedia. We will use the sector information in the last section of this post where we take a quick look at the scraped data.\n\nsymbols\n\n# A tibble: 503 × 4\n   symbol company                 sector                 industry               \n   &lt;chr&gt;  &lt;chr&gt;                   &lt;chr&gt;                  &lt;chr&gt;                  \n 1 A      Agilent Technologies    Health Care            Health Care Equipment  \n 2 AAL    American Airlines Group Industrials            Passenger Airlines     \n 3 AAPL   Apple Inc.              Information Technology Technology Hardware, S…\n 4 ABBV   AbbVie                  Health Care            Pharmaceuticals        \n 5 ABNB   Airbnb                  Consumer Discretionary Hotels, Resorts & Crui…\n 6 ABT    Abbott                  Health Care            Health Care Equipment  \n 7 ACGL   Arch Capital Group      Financials             Reinsurance            \n 8 ACN    Accenture               Information Technology IT Consulting & Other …\n 9 ADBE   Adobe Inc.              Information Technology Application Software   \n10 ADI    Analog Devices          Information Technology Semiconductors         \n# ℹ 493 more rows"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#locate-esg-information",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#locate-esg-information",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Locate ESG Information",
    "text": "Locate ESG Information\nThe point where I struggled when I tried to replicate other guides was the search for the exact location of the information that I want to scrape (and the fact that the old locations seemed to have changed). After some trial and error, it turns out that it is really easy. Once you download a web page, you can in principle either use CSS nodes or XML paths to extract information using html_nodes() as above. However, the CSS nodes on Yahoo Finance have a weird structure that is apparently not straight-forward to use in this function. Fortunately, XML paths work perfectly! Google will explain to you what these terms mean, I only demonstrate how you find the relevant paths which we use in the scraping function below.\nLet us stick to Apple as our main example and go to the sustainability tab on Yahoo Finance. If we right-click on the ESG score (e.g., using Google Chrome), we can see the the option to ‘Inspect’.\n\nOnce you click on it, a tab to the right opens where you see the underlying code. What is even more useful is the fact that the browser highlights the corresponding elements on the website as you hover over the code. This way, it is really easy to locate the information we are after. So we click on the relevant element and we copy the XML path.\n\nSo the location of the total ESG score on the page is: '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[1]/div/div[2]/div[1]'\nWe can just point-and-click on all the items we want to scrap and collect the relevant XML paths. Once we downloaded a page, we just tell html_node() where to look for the information we want and afterwards how to parse it."
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#define-functions-for-scraping",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#define-functions-for-scraping",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Define Functions for Scraping",
    "text": "Define Functions for Scraping\nMy function to scrap ESG data takes two main inputs: the stock symbol and your user agent. We got the symbols from Wikipedia, but we need to define our own user agent. For instance, I use an agent that looks like this:\n\nagent &lt;- \"Your Name (your@email.com). Doing personal research.\"\n\nThe main functions then proceeds as follows:\n\nConstruct the link of the page we want to download.\nCheck if scraping is allowed.\nDownload the page.\nExtract individual information using the XML paths we manually extracted following the point-and-click procedure from above.\nCollect all information in a table.\n\nLet us start with a function that scrapes a page for a specific symbol:\n\nscrape_sustainability_page &lt;- function(symbol, agent, max_tries = 10) {\n  link &lt;- paste0(\n    \"https://finance.yahoo.com/quote/\", symbol, \"/sustainability?p=\", symbol\n  )\n  \n  check &lt;- suppressMessages(robotstxt::paths_allowed(link))\n  \n  if (check == TRUE) {\n    resp &lt;- request(link) |&gt; \n      req_user_agent(agent) |&gt; \n      req_retry(max_tries = max_tries) |&gt; \n      req_perform()\n    \n    page &lt;- resp$body |&gt; \n      read_html()\n    \n    return(page)\n  } else {\n    stop(paste0(\"No bots allowed on page '\", link ,\"'!\"))\n  }\n}\n\nThe second function extracts the relevant information from the scraped pages and returns it as a table.\n\nextract_esg_data &lt;- function(symbol, page) {\n  scrape_date &lt;- Sys.time()\n  \n  total_esg_score &lt;- page|&gt; \n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[1]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  total_esg_percentile &lt;- page|&gt; \n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[1]/div/div[2]/div[2]/span')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  environment_risk_score &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[2]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  social_risk_score &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[3]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number() \n  \n  governance_risk_score &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[4]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  controversy_level &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[2]/div[2]/div/div/div/div[1]/div')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  last_update_date &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[3]/span[2]/span')|&gt;\n    html_text()\n  \n  last_update_date &lt;- str_remove(last_update_date, \"Last updated on \")\n  \n  tibble(\n    symbol,\n    scrape_date,\n    total_esg_score,\n    environment_risk_score,\n    social_risk_score,\n    governance_risk_score,\n    controversy_level,\n    last_update_date\n  )\n}"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#scrape-esg-data",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#scrape-esg-data",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Scrape ESG Data",
    "text": "Scrape ESG Data\nNow, let us put everything together: we loop over all symbols to download the relevant pages and extract the relevant ESG data. I store each instance of esg_data because the scraping process is very likely to be interrupted by Yahoo Finance as it starts to block requests after some time. By using a loop, I can interrupt the execution any time and continue with the last index.\n\nfor (j in 1:nrow(symbols)) {\n  page &lt;- scrape_sustainability_page(symbols$symbol[j], agent)\n  esg_data &lt;- extract_esg_data(symbols$symbol[j], page)\n  write_rds(esg_data, paste0(\"data/esg_data_\", symbols$symbol[j], \".rds\"))\n}\n\nesg_data &lt;- list.files(\"data/\", full.names = TRUE) |&gt; \n  map_df(read_rds)\n\nThe code chunk from above takes a couple of hours in the current specification because of the increasing waiting times. The whole table then looks like this and also includes our initial example Apple:\n\nesg_data \n\n# A tibble: 503 × 8\n   symbol scrape_date         total_esg_score environment_risk_score\n   &lt;chr&gt;  &lt;dttm&gt;                        &lt;dbl&gt;                  &lt;dbl&gt;\n 1 A      2023-11-27 00:29:33              15                    0.3\n 2 AAL    2023-11-27 00:29:33              29                   11.5\n 3 AAPL   2023-11-27 00:29:35              17                    0.6\n 4 ABBV   2023-11-27 00:29:37              28                    1.1\n 5 ABNB   2023-11-27 00:29:38              NA                   NA  \n 6 ABT    2023-11-27 00:29:39              25                    3  \n 7 ACGL   2023-11-27 00:29:39              21                    1.5\n 8 ACN    2023-11-27 00:29:40              10                    0.3\n 9 ADBE   2023-11-27 00:29:41              12                    1.9\n10 ADI    2023-11-27 00:29:41              24                   10.1\n# ℹ 493 more rows\n# ℹ 4 more variables: social_risk_score &lt;dbl&gt;, governance_risk_score &lt;dbl&gt;,\n#   controversy_level &lt;dbl&gt;, last_update_date &lt;chr&gt;"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#quick-evaluation-of-esg-scores",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#quick-evaluation-of-esg-scores",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Quick Evaluation of ESG Scores",
    "text": "Quick Evaluation of ESG Scores\nLet us take a quick look at the data we collected. First, let us check the overall coverage of our sample:\n\nscales::percent(nrow(na.omit(esg_data)) / nrow(esg_data))\n\n[1] \"86%\"\n\n\nThis is not too bad. I believe that for most of the companies without ESG scores in my sample, Yahoo Finance does not provide any data. Admittedly, I should check manually at some point, but for the purpose of this post, this is definitely a success. To analyze sector-level breakdowns, I construct a summary table which I use as the main source for the following figures.\n\nesg_scores_sector &lt;- symbols |&gt;\n  left_join(esg_data, join_by(symbol)) |&gt; \n  group_by(sector)|&gt;\n  summarize(companies = n(),\n            coverage = sum(!is.na(total_esg_score)) / n(),\n            across(c(contains(\"score\"), controversy_level), \n                   ~mean(., na.rm = TRUE)))|&gt;\n  arrange(-coverage)\n\nThe first figure gives us the coverage per sector. All real estate companies have ESG scores, while only a bit more than three quarters of communication services feature this information.\n\nesg_scores_sector|&gt;\n  mutate(labels = paste0(companies * coverage, \" out of \", companies))|&gt;\n  ggplot(aes(y = reorder(sector, coverage), \n             x = coverage, fill = factor(round(coverage, 0)))) +\n  geom_col(show.legend = FALSE) + \n  theme_minimal() + \n  geom_text(aes(label = labels), hjust = 1.1, color = \"white\") +\n  coord_cartesian(xlim = c(0, 1)) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = NULL, y = NULL,\n       title = \"How many companies have ESG scores per sector?\",\n       subtitle = \"Based on Yahoo Finance and S&P 500 data as of November 2023\")\n\n\n\n\nNext, I want to look at average ESG scores by sector. For instance, the real estate sector has the lowest total ESG score, indicating the lowest degree to which a sector’s business value is at risk driven by environmental, social and governance risks. Financials exhibit the the lowest environmental risk, while the energy sector (at least the part included in the S&P 500) has the highest exposure to environmental risks.\n\nesg_scores_sector|&gt;\n  pivot_longer(cols = contains(\"score\"))|&gt;\n  mutate(name = str_to_title(str_replace_all(name, \"_\", \" \")),\n         name = factor(name),\n         sector = tidytext::reorder_within(sector, -value, name))|&gt;\n  ggplot(aes(y = sector, x = value, fill = name)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~name, scales = \"free_y\") +\n  theme_minimal() + \n  tidytext::scale_y_reordered() +\n  geom_text(aes(label = round(value, 0)), hjust = 1.1, color = \"white\") +\n  labs(y = NULL, x = NULL,\n       title = \"What are the average ESG scores per sector?\",\n       subtitle = \"Based on Yahoo Finance and S&P 500 data as of November 2023\")\n\n\n\n\nFinally, I am also interested in the average controversy level which measures to which degree companies are involved in incidents and events that may negatively impact stakeholders, the environment or their operations. I decided to plot the controversy of each sector relative to the average overall controversy. Real estate and information technology seem to be far less controverse than consumer staples and communication services.\n\nesg_scores_sector|&gt;\n  mutate(controversy_relative = controversy_level - mean(controversy_level)) |&gt; \n  ggplot(aes(y = reorder(sector, -controversy_relative), \n             x = controversy_relative, fill = (controversy_relative &lt; 0))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_minimal() + theme(legend.position = \"none\") + \n  coord_cartesian(xlim = c(-1.5, 1.5)) +\n  labs(y = NULL, x = NULL,\n       title = \"What is the average sector-level controversy relative to overall controversy?\",\n       subtitle = \"Based on Yahoo Finance and S&P 500 data as of November 2023\")\n\n\n\n\nI think there is a lot more interesting stuff to uncover using the ESG scores, but for now I’ll leave it at that. I am nonetheless surprised, how easy scraping information from websites is using these amazing packages."
  },
  {
    "objectID": "posts/clustering-binary-data/index.html",
    "href": "posts/clustering-binary-data/index.html",
    "title": "Clustering Binary Data",
    "section": "",
    "text": "In this post I tackle the challenge to extract a small number of typical respondent profiles from a large scale survey with multiple yes-no questions. This type of setting corresponds to a classification problem without knowing the true labels of the observations – also known as unsupervised learning. Since I regularly face tasks in this area, I decided to start an irregular series of blogs that touch upon practical aspects of unsupervised learning in R using tidy principles.\nTechnically speaking, I have a set of \\(N\\) observations \\((x_1, x_2, ... , x_N)\\) of a random \\(p\\)-vector \\(X\\) with joint density \\(\\text{Pr}(X)\\). The goal of classification is to directly infer the properties of this probability density without the help of the correct answers (or degree-of-error) for each observation. In this note I focus on cluster analysis that attempts to find convex regions of the \\(X\\)-space that contain modes of \\(\\text{Pr}(X)\\). This approach aims to tell whether \\(\\text{Pr}(X)\\) can be represented by a mixture of simpler densities representing distinct classes of observations.\nIntuitively, I want to find clusters of the survey responses such that respondents within each cluster are more closely related to one another than respondents assigned to different clusters. There are many possible ways to achieve that, but I focus on the most popular and most approachable ones: \\(K\\)-means, \\(K\\)-modes, as well as agglomerative and divisive hierarchical clustering. AS we see below, the 4 models yield quite different results for clustering binary data.\nI use the following packages throughout this post. In particular, I use klaR and cluster for clustering algorithms that go beyond the stats package that is included with your R installation.1 I explicitely refer to the corresponding packages when I call them below.\nlibrary(klaR)\nlibrary(cluster)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(scales)"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#creating-sample-data",
    "href": "posts/clustering-binary-data/index.html#creating-sample-data",
    "title": "Clustering Binary Data",
    "section": "Creating sample data",
    "text": "Creating sample data\nLet us start by creating some sample data where we basically exactly know which kind of answer profiles are out there. Later, we evaluate the cluster models according to how well they are doing in uncovering the clusters and assigning respondents to clusters. I assume that there are 4 yes/no questions labeled q1, q2, q3 and q4. In addition, there are 3 different answer profiles where cluster 1 answers positively to the first question only, cluster 2 answers positively to question 2 and 3 and cluster 3 answers all questions positively. I also define the the number of respondents for each cluster.\n\ncenters &lt;- tibble(\n  cluster = factor(1:3), \n  respondents = c(250, 500, 200),\n  q1 = c(1, 0, 1),\n  q2 = c(0, 1, 1),             \n  q3 = c(0, 1, 1),\n  q4 = c(0, 0, 1)\n)\n\nAlternatively, we could think of the yes/no questions as medical records that indicate whether the subject has a certain pre-condition or not.\nSince it should be a bit tricky for the clustering models to find the actual response profiles, let us add some noise in the form of respondents that deviate from their assigned cluster profile and shuffle all rows. We find out below how the cluster algorithms are able to deal with this noise.\n\nset.seed(42)\nlabelled_respondents &lt;- centers |&gt; \n  mutate(\n    across(\n      starts_with(\"q\"),\n      ~map2(respondents, .x, function(x, y) {\n        rbinom(x, 1, max((y - 0.1), 0.1))\n      }),\n      .names = \"{col}\"\n    )\n  ) |&gt; \n  select(-respondents) |&gt; \n  unnest(cols = c(q1, q2, q3, q4)) |&gt; \n  sample_n(n())\n\nThe figure below visualizes the distribution of simulated question responses by cluster.\n\nlabelled_respondents |&gt;\n  pivot_longer(cols = -cluster, \n               names_to = \"question\", values_to = \"response\") |&gt;\n  mutate(response = response == 1) |&gt;\n  ggplot(aes(x = response, y = question, color = cluster)) +\n  geom_jitter() +\n  theme_bw() +\n  labs(x = \"Response\", y = \"Question\", color = \"Cluster\",\n       title = \"Visualization of simulated question responses by cluster\")"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#k-means-clustering",
    "href": "posts/clustering-binary-data/index.html#k-means-clustering",
    "title": "Clustering Binary Data",
    "section": "\\(K\\)-means clustering",
    "text": "\\(K\\)-means clustering\nThe \\(K\\)-means algorithm is one of the most popular clustering methods (see also this tidymodels example). It is intended for situations in which all variables are of the quantitative type since it partitions all respondents into \\(k\\) groups such that the sum of squares from respondents to the assigned cluster centers are minimized. For binary data, the Euclidean distance reduces to counting the number of variables on which two cases disagree.\nThis leads to a problem (which is also described here) because of an arbitrary cluster assignment after cluster initialization. The first chosen clusters are still binary data and hence observations have integer distances from each of the centers. The corresponding ties are hard to overcome in any meaningful way. Afterwards, the algorithm computes means in clusters and revisits assignments. Nonetheless, \\(K\\)-means might produce informative results in a fast and easy to interpret way. I hence include it in this post for comparison.\nTo run the \\(K\\)-means algorithm, we first drop the cluster column.\n\nrespondents &lt;- labelled_respondents |&gt;\n  select(-cluster)\n\nIt is very straight-forward to run the built-in stats::kmeans clustering algorithm. I choose the parameter of maximum iterations to be 1000 to increase the likeliness of getting the best fitting clusters. Since the data is fairly small and the algorithm is also quite fast, I see no harm in using a high number of iterations.\n\niter_max &lt;- 1000\nkmeans_example &lt;- stats::kmeans(respondents, centers = 3, iter.max = iter_max)\n\nThe output of the algorithm is a list with different types of information including the assigned clusters for each respondent.\nAs we want to compare cluster assignment across different models and we repeatedly assign different clusters to respondents, I write up a helper function that adds assignments to the respondent data from above. The function shows that \\(K\\)-means and \\(K\\)-modes contain a field with cluster information. The two hierarchical cluster models, however, need to be cut a the desired number of clusters (more on that later).\n\nassign_clusters &lt;- function(model, k = NULL) {\n  if (class(model)[1] %in% c(\"kmeans\", \"kmodes\")) {\n    cluster_assignment &lt;- model$cluster\n  }\n  if (class(model)[1] %in% c(\"agnes\", \"diana\")) {\n    if (is.null(k)) {\n      stop(\"k required for hierarchical models!\")\n    }\n    cluster_assignment &lt;- stats::cutree(model, k = k)\n  }\n  \n  clusters &lt;- respondents |&gt;\n    mutate(cluster = cluster_assignment)\n  \n  return(clusters)\n}\n\nIn addition, I introduce a helper function that summarizes information by cluster. In particular, the function computes average survey responses (which correspond to proportion of yes answers in the current setting) and sorts the clusters according to the total number of positive answers. The latter helps us later to compare clusters across different models.\n\nsummarize_clusters &lt;- function(model, k = NULL) {\n\n  clusters &lt;- assign_clusters(model = model, k = k)\n  \n  summary_statistics &lt;- clusters |&gt;\n    group_by(cluster) |&gt;\n    summarize(across(matches(\"q\"), \\(x) mean(x, na.rm = TRUE)),\n              assigned_respondents = n()) |&gt;\n    select(-cluster) |&gt;\n    mutate(total = rowSums(across(matches(\"q\")))) |&gt;\n    arrange(-total) |&gt;\n    mutate(k = row_number(),\n           model = class(model)[1])\n  \n  return(summary_statistics)\n}\n\nWe could easily introduce other summary statistics into the function, but the current specification is sufficient for the purpose of this note.\n\nkmeans_example &lt;- summarize_clusters(kmeans_example)\n\nSince we do not know the true number of clusters in real-world settings, we want to compare the performance of clustering models for different numbers of clusters. Since we know that the true number of clusters is 3 in the current setting, let us stick to a maximum of 7 clusters. In practice, you might of course choose an arbitrary maximum number of clusters.\n\nk_min &lt;- 1\nk_max &lt;- 7\n\nkmeans_results &lt;- tibble(k = k_min:k_max) |&gt;\n  mutate(\n    kclust = map(k, ~kmeans(respondents, centers = .x, iter.max = iter_max)),\n  )\n\nA common heuristic to determine the optimal number of clusters is the elbow method where we plot the within-cluster sum of squared errors of an algorithm for increasing number of clusters. The optimal number of clusters corresponds to the point where adding another cluster does lead to much of an improvement anymore. In economic terms, we look for the point where the diminishing returns to an additional cluster are not worth the additional cost (assuming that we want the minimum number of clusters with optimal predictive power).\nThe function below computes the within-cluster sum of squares for any cluster assignments.\n\ncompute_withinss &lt;- function(model, k = NULL) {\n  \n  clusters &lt;- assign_clusters(model = model, k = k)\n  \n  centers &lt;- clusters |&gt;\n    group_by(cluster) |&gt;\n    summarize_all(mean) |&gt;\n    pivot_longer(cols = -cluster, names_to = \"question\", values_to = \"cluster_mean\")\n  \n  withinss &lt;- clusters |&gt;\n    pivot_longer(cols = -cluster, names_to = \"question\", values_to = \"response\") |&gt;\n    left_join(centers, by = c(\"cluster\", \"question\")) |&gt;\n    summarize(k = max(cluster),\n              withinss = sum((response - cluster_mean)^2)) |&gt;\n    mutate(model = class(model)[1])\n  \n  return(withinss)\n}\n\nWe can simply map the function across our list of \\(K\\)-means models. For better comparability, we normalize the within-cluster sum of squares for any number of cluster by the benchmark case of only having a single cluster. Moreover, we consider log-differences to because we care more about the percentage decrease in sum of squares rather than the absolute number.\n\nkmeans_logwithindiss &lt;- kmeans_results$kclust |&gt;\n  map(compute_withinss) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#k-modes-clustering",
    "href": "posts/clustering-binary-data/index.html#k-modes-clustering",
    "title": "Clustering Binary Data",
    "section": "\\(K\\)-modes clustering",
    "text": "\\(K\\)-modes clustering\nSince \\(K\\)-means is actually not ideal for binary (or hierarchical data in general), Huang (1997) came up with the \\(K\\)-modes algorithm. This clustering approach aims to partition respondents into \\(K\\) groups such that the distance from respondents to the assigned cluster modes is minimized. A mode is a vector of elements that minimize the dissimilarities between the vector and each object of the data. Rather than using the Euclidean distance, \\(K\\)-modes uses simple matching distance between respondents to quantify dissimilarity which translates into counting the number of mismatches in all question responses in the current setting.\nFortunately, the klaR package provides an implementation of the \\(K\\)-modes algorithm that we can apply just like the \\(K\\)-means above.\n\nkmodes_example &lt;- klaR::kmodes(respondents, iter.max = iter_max, modes = 3) |&gt;\n  summarize_clusters()\n\nSimilarly, we just map the model across different numbers of target cluster modes and compute the within-cluster sum of squares.\n\nkmodes_results &lt;- tibble(k = k_min:k_max) |&gt;\n  mutate(\n    kclust = map(k, ~klaR::kmodes(respondents, modes = ., iter.max = iter_max))\n  )\n\nkmodes_logwithindiss &lt;- kmodes_results$kclust |&gt;\n  map(compute_withinss) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))\n\nNote that I computed the within-cluster sum of squared errors rather than using the within-cluster simple-matching distance provided by the function itself. The latter counts the number of differences from assigned respondents to their cluster modes."
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#hierarchical-clustering",
    "href": "posts/clustering-binary-data/index.html#hierarchical-clustering",
    "title": "Clustering Binary Data",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nAs an alternative to computing optimal assignments for a given number of clusters, we might sometimes prefer to arrange the clusters into a natural hierarchy. This involves successively grouping the clusters themselves such that at each level of the hierarchy, clusters within the same group are more similar to each other than those in different groups. There are two fundamentally different approaches to hierarchical clustering that are fortunately implemented in the great cluster package.\nBoth hierarchical clustering approaches require a dissimilarity or distance matrix. Since we have binary data, we choose the asymmetric binary distance matrix based on the Jaccard distance. Intuitively, the Jaccard distance measures how far the overlap of responses between two groups is from perfect overlap.\n\ndissimilarity_matrix &lt;- stats::dist(respondents, method = \"binary\")\n\nAgglomerative clustering start at the bottom and at each level recursively merge a selected pair of clusters into a single cluster. This produces a clustering at the next higher level with one less cluster. The pair chosen for merging consist of the two clusters with the smallest within-cluster dissimilarity. On an intuitive level, agglomerative clustering is hence better in discovering small clusters.\nThe cluster package provides the agnes algorithm (AGglomerative NESting) that can easily applied to the dissimilarity matrix.\n\nagnes_results &lt;- cluster::agnes(\n  dissimilarity_matrix, diss = TRUE, keep.diss = TRUE, method = \"complete\"\n)\n\nThe function returns a clustering tree that we could plot (which I actually rarely found really helpful) or cut into different partitions using the stats::cutree function. This is why the helper functions from above need a number of target clusters as an input for hierarchical clustering models. However, the logic of the summary statistics are just as above.\n\nagnes_example &lt;- summarize_clusters(agnes_results, k = 3)\n\nagnes_logwithindiss &lt;- k_min:k_max |&gt;\n  map(~compute_withinss(agnes_results, .)) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))\n\nDivisive methods start at the top and at each level recursively split one of the existing clusters at that level into two new clusters. The split is chosen such that two new groups with the largest between-group dissimilarity emerge. Intuitively speaking, divisive clustering is thus better in discovering large clusters.\nThe cluster package provides the diana algorithm (DIvise ANAlysis) for this clustering approach where the logic is basically the same as for the agnes model.\n\ndiana_results &lt;- cluster::diana(\n  dissimilarity_matrix, diss = TRUE, keep.diss = TRUE\n) \n\ndiana_example &lt;- diana_results |&gt;\n  summarize_clusters(k = 3)\n\ndiana_logwithindiss &lt;-  k_min:k_max |&gt;\n  map(~compute_withinss(diana_results, .)) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#model-comparison",
    "href": "posts/clustering-binary-data/index.html#model-comparison",
    "title": "Clustering Binary Data",
    "section": "Model comparison",
    "text": "Model comparison\nLet us start the model comparison by looking at the within cluster sum of squares for different numbers of clusters. The figure shows that the \\(K\\)-modes algorithm improves the fastest towards the true number of 3 clusters. The elbow method would suggest in this case to stick with 3 clusters for this algorithm. Similarly, for the \\(K\\)-means model. The hierarchical clustering models do not seem to support 3 clusters.\n\nbind_rows(kmeans_logwithindiss, kmodes_logwithindiss,\n          agnes_logwithindiss, diana_logwithindiss) |&gt;\n  ggplot(aes(x = k, y = logwithindiss, color = model, linetype = model)) +\n  geom_line() +\n  scale_x_continuous(breaks = k_min:k_max) + \n  theme_minimal() +\n  labs(x = \"Number of Clusters\", y = bquote(log(W[k])-log(W[1])), \n       color = \"Model\", linetype = \"Model\",\n       title = \"Within cluster sum of squares relative to benchmark case of one cluster\")\n\n\n\n\nNow, let us compare the proportion of positive responses within assigned clusters across models. Recall that I ranked clusters according to the total share of positive answers to ensure comparability. This approach is only possible in this type of setting where we can easily introduce such a ranking. The figure suggests that \\(K\\)-modes performs best for the current setting as it identifies the correct responses for each cluster.\n\nbind_rows(\n  kmeans_example, kmodes_example,\n  agnes_example, diana_example) |&gt;\n  select(-c(total, assigned_respondents)) |&gt;\n  pivot_longer(cols = -c(k, model), \n               names_to = \"question\", values_to = \"response\") |&gt;\n  mutate(cluster = paste0(\"Cluster \", k)) |&gt;\n  ggplot(aes(x = response, y = question, fill = model)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~cluster) +\n  theme_bw() +\n  scale_x_continuous(labels = scales::percent) + \n  geom_hline(yintercept = seq(1.5, length(unique(colnames(respondents))) - 0.5, 1),\n             colour = 'black') +\n  labs(x = \"Proportion of responses\", y = \"Question\", fill = \"Model\",\n       title = \"Proportion of positive responses within assigned clusters\")\n\n\n\n\nFinally, let us check how well each model assigns respondents to the true cluster which is obviously not possible in real unsupervised applications. The figure below shows the true number of respondents by cluster as a dashed box and the assigned respondents as bars. The figure shows that \\(K\\)-modes is the only model that is able to consistently assign respondents to their correct cluster.\n\nbind_rows(\n  kmeans_example, kmodes_example,\n  agnes_example, diana_example) |&gt;\n  mutate(cluster = paste0(\"Cluster \", k)) |&gt;\n  select(model, cluster, assigned_respondents) |&gt;\n  ggplot() +\n  geom_col(position = \"dodge\", \n           aes(y = assigned_respondents, x = cluster, fill = model)) +\n  geom_col(data = labelled_respondents |&gt;\n             group_by(cluster = paste0(\"Cluster \", cluster)) |&gt;\n             summarize(assigned_respondents = n(),\n                       model = \"actual\"),\n           aes(y = assigned_respondents, x = cluster), \n           fill = \"white\", color = \"black\", alpha = 0, linetype = \"dashed\") +\n  theme_bw() +\n  labs(x = NULL, y = \"Number of assigned respondents\", fill = \"Model\",\n       title = \"Number of assigned respondents by cluster\",\n       subtitle = \"Dashed box indicates true number of respondents by cluster\")\n\n\n\n\nLet me end this post with a few words of caution: first, the ultimate outcome heavily depends on the seed chosen at the beginning of the post. The results might be quite different for other draws of respondents or initial conditions for clustering algorithms. Second, there are many more models out there that can be applied to the current setting. However, with this post I want to emphasize that it is important to consider different models at the same time and to compare them through a consistent set of measures. Ultimately, choosing the optimal number of clusters in practice requires a judgment call, but at least it can be informed as much as possible."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy Intelligence",
    "section": "",
    "text": "Analyzing Seasonality in DAX Returns\n\n\nDebunking the Halloween indicator for German market returns using R\n\n\n\n\n\n\n\n\n\n\n\n\n\nScraping ESG Data from Yahoo Finance\n\n\nHow to scrape environmental, social and governance risk scores from Yahoo Finance using R\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering Binary Data\n\n\nAn application of different unsupervised learning approaches to cluster simulated survey responses using R\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data: A Recipe for Efficient Data Analysis\n\n\nOn the importance of tidy data for efficient analysis using the analogy of a well-organized kitchen\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Collaborative Filtering: Building A Stock Recommender\n\n\nA simple implementation for prototyping multiple collaborative filtering algorithms using R.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dax-seasonality/index.html",
    "href": "posts/dax-seasonality/index.html",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "",
    "text": "Seasonalcharts.de claims that stock indices exhibit persistent seasonality that may be exploited through an appropriate trading strategy. As part of a job application, I had to replicate the seasonal pattern for the DAX and then test whether this pattern entails a profitable trading strategy. To sum up, I indeed find that a trading strategy that holds the index only over a specific season outperforms the market significantly, but these results might be driven by a few outliers. Note that the post below references an opinion and is for information purposes only. I do not intend to provide any investment advice.\nThe code is structured in a way that allows for a straight-forward replication of the methodology for other indices. The post uses the following packages:\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(frenchdata)\nlibrary(scales)\nlibrary(fixest)"
  },
  {
    "objectID": "posts/dax-seasonality/index.html#data-preparation",
    "href": "posts/dax-seasonality/index.html#data-preparation",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, download data from yahoo finance using the tidyquant package. Note that the DAX was officially launched in July 1988, so this is where our sample starts.\n\ndax_raw &lt;- tq_get(\n  \"^GDAXI\", get = \"stock.prices\", \n  from = \"1988-07-01\", to = \"2023-10-30\"\n) \n\nThen, select only date and the adjusted price (i.e., closing price after adjustments for all applicable splits and dividend distributions) as the relevant variables and compute summary statistics to check for missing or weird values. The results are virtually the same if we use unadjusted closing prices.\n\ndax &lt;- dax_raw |&gt;\n  select(date, price = adjusted)\n\nWe replace the missing values by the last available index value.\n\ndax &lt;- dax |&gt;\n  arrange(date) |&gt;\n  fill(price, .direction = \"down\")\n\nAs a immediate plausibility check, we plot the DAX over the whole sample period.\n\ndax |&gt;\n  ggplot(aes(x = date, y = price)) +\n  geom_line() + \n  labs(x = NULL, y = \"Adjusted Price\",\n       title = \"Adjusted DAX index price between 1988 and 2023\") +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\nThe main idea of Seasonalcharts is to implement the strategy proposed by Jacobsen and Bouman (2002) and Jacobsen and Zhan (2018) which they label ‘The Halloween Indicator’ (or ‘Sell in May Effect’). The main finding of these papers is that stock indices returns seem significantly lower during the May-October period than during the remainder of the year. The corresponding trading strategy holds an index during the months November-April, but holds the risk-free asset in the May-October period.\nTo replicate their approach (and avoid noise in the daily data), we focus on monthly returns from now on.\n\ndax_monthly &lt;- dax |&gt;\n  mutate(year = year(date),\n         month = factor(month(date))) |&gt;\n  group_by(year, month) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  arrange(date) |&gt;\n  mutate(ret = price / lag(price) - 1) |&gt;\n  drop_na()\n\nAnd as usual in empirical asset pricing, we do not care about raw returns, but returns in excess of the risk-free asset. We simply add the European risk free rate from the Fama-French data library as the corresponding reference point. Of course, one could use other measures for the risk-free rate, but the impact on the results won’t be substantial.\n\nfactors_ff3_monthly_raw &lt;- download_french_data(\"Fama/French 3 Factors\")\n\nrisk_free_monthly &lt;- factors_ff3_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    year = year(ymd(str_c(date, \"01\"))),\n    month = factor(month(ymd(str_c(date, \"01\")))),\n    rf = as.numeric(RF) / 100,\n    .keep = \"none\"\n  )\n\ndax_monthly &lt;- dax_monthly |&gt; \n  left_join(risk_free_monthly, join_by(year, month)) |&gt; \n  mutate(ret_excess = ret - rf) |&gt; \n  drop_na()"
  },
  {
    "objectID": "posts/dax-seasonality/index.html#graphical-evidence-for-seasonality",
    "href": "posts/dax-seasonality/index.html#graphical-evidence-for-seasonality",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Graphical Evidence for Seasonality",
    "text": "Graphical Evidence for Seasonality\nWe start by first plotting the average returns for each month.\n\ndax_monthly |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret)) |&gt; \n  ggplot(aes(x = month, y = ret, fill = ret &gt; 0)) +\n  geom_col() +\n  scale_y_continuous(labels = percent) + \n  labs(\n    x = \"Month\", y = \"Average DAX Return\", \n    title = \"Average monthly DAX returns between 1988 and 2023\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nThe figure shows negative returns for June, August, and September, while all other months exhibit positive returns. However, it makes more sense to look at distributions instead of simple means, which might be heavily influenced by outliers. To illustrate distributions, I follow Cedric Scherer and use raincloud plots. which combine halved violin plot, a box plot, and the raw data as some kind of scatter. These plots hence provide detailed visualizations of the distributions.\n\ndax_monthly |&gt; \n  ggplot(aes(x = month, y = ret, group = month)) + \n  ggdist::stat_halfeye(\n    adjust = .5, width = .6, .width = 0, justification = -.3, point_colour = NA\n    ) + \n  geom_boxplot(\n    width = .25, outlier.shape = NA\n    ) +\n  stat_summary(\n    fun = mean, geom=\"point\", color = \"red\", fill = \"red\"\n    ) +\n  geom_point(\n    size = 1.5, alpha = .2, position = position_jitter(seed = 42, width = .1)\n    ) +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n  labs(\n    x = \"Month\", y = \"DAX Return\", \n    title = \"Distributions of monthly DAX returns between 1988 and 2023\",\n    subtitle = \"Red dots indicate means, solid lines indicate medians\"\n  ) \n\n\n\n\nThe figure suggests that August and September exhibit considerable negative outliers."
  },
  {
    "objectID": "posts/dax-seasonality/index.html#evaluating-trading-strategies",
    "href": "posts/dax-seasonality/index.html#evaluating-trading-strategies",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Evaluating Trading Strategies",
    "text": "Evaluating Trading Strategies\nLet us now take a look at the average excess returns per month. We also add the standard deviation, 5% and 95% quantiles, and t-statistic of a t-test of the null hypothesis that average returns are zero in a given month.\n\ndax_monthly |&gt;\n  drop_na(ret_excess) |&gt; \n  group_by(Month = month) |&gt;\n  summarize(\n    Mean = mean(ret_excess),\n    SD = sd(ret_excess),\n    Q05 = quantile(ret_excess, 0.05),\n    Q95 = quantile(ret_excess, 0.95),\n    `t-Statistic` = sqrt(n()) * mean(ret_excess) / sd(ret_excess)\n  )\n\n# A tibble: 12 × 6\n   Month     Mean     SD     Q05    Q95 `t-Statistic`\n   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n 1 1      0.00621 0.0552 -0.0911 0.0868         0.666\n 2 2      0.00200 0.0541 -0.0868 0.0775         0.219\n 3 3      0.00486 0.0539 -0.0732 0.0842         0.533\n 4 4      0.0260  0.0599 -0.0502 0.119          2.57 \n 5 5      0.00470 0.0401 -0.0573 0.0610         0.693\n 6 6     -0.00353 0.0476 -0.0934 0.0587        -0.439\n 7 7      0.0131  0.0598 -0.0658 0.0902         1.29 \n 8 8     -0.0235  0.0622 -0.168  0.0347        -2.27 \n 9 9     -0.0244  0.0730 -0.176  0.0613        -2.01 \n10 10     0.0206  0.0658 -0.0964 0.122          1.85 \n11 11     0.0228  0.0483 -0.0455 0.0862         2.79 \n12 12     0.0195  0.0554 -0.0586 0.108          2.08 \n\n\nAugust and September seem to usually exhibit negative excess returns with an average of about -2.4% (statistically significant) over all years, while April is the only months that tend to exhibit statistically significant positive excess returns.\nLet us proceed to test for the presence of statistically significant excess returns due to seasonal patterns. In the above table, I only test for significance for each month separately. To test for positive returns in a joint model, I regress the monthly excess returns on month indicators. Note that I always adjust the standard errors to be heteroskedasticity robust.\n\nmodel_monthly &lt;- feols(\n  ret_excess ~ month, \n  data = dax_monthly,\n  vcov = \"hetero\"\n)\nsummary(model_monthly)\n\nOLS estimation, Dep. Var.: ret_excess\nObservations: 422 \nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)  0.006214   0.009332  0.665913 0.505841    \nmonth2      -0.004210   0.013064 -0.322287 0.747400    \nmonth3      -0.001355   0.013046 -0.103898 0.917301    \nmonth4       0.019820   0.013763  1.440120 0.150597    \nmonth5      -0.001516   0.011534 -0.131452 0.895482    \nmonth6      -0.009744   0.012318 -0.791021 0.429389    \nmonth7       0.006857   0.013752  0.498587 0.618337    \nmonth8      -0.029730   0.013951 -2.131045 0.033680 *  \nmonth9      -0.030625   0.015334 -1.997154 0.046469 *  \nmonth10      0.014389   0.014524  0.990694 0.322419    \nmonth11      0.016551   0.012399  1.334841 0.182669    \nmonth12      0.013261   0.013225  1.002767 0.316565    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.056141   Adj. R2: 0.050201\n\n\nSeems like August and September have on average indeed lower returns than January (which is the omitted reference point in this regression). Note that the size of the coefficients from the regression are the same as in the table above (i.e., constant plus coefficient), but the t-statistics are different because we are estimating a joint model now.\nAs the raincloud plots indicated that outliers might drive any statistical significant differences, we estimate the model again after trimming the data. In particular, we drop the top and bottom 1% of observations. This trimming step only drops 10 observations.\n\nret_excess_q01 &lt;- quantile(dax_monthly$ret_excess, 0.01)\nret_excess_q99 &lt;- quantile(dax_monthly$ret_excess, 0.99)\n\nmodel_monthly_trimmed &lt;- feols(\n  ret_excess ~ month, \n  data = dax_monthly |&gt; \n     filter(ret_excess &gt;= ret_excess_q01 & ret_excess &lt;= ret_excess_q99),\n  vcov = \"hetero\"\n)\n   \netable(model_monthly, model_monthly_trimmed, coefstat = \"tstat\")\n\n                    model_monthly model_monthly_t..\nDependent Var.:        ret_excess        ret_excess\n                                                   \nConstant          0.0062 (0.6659)   0.0062 (0.6657)\nmonth2          -0.0042 (-0.3223) -0.0042 (-0.3222)\nmonth3          -0.0014 (-0.1039) -0.0014 (-0.1039)\nmonth4             0.0198 (1.440)   0.0099 (0.8135)\nmonth5          -0.0015 (-0.1315) -0.0015 (-0.1314)\nmonth6          -0.0097 (-0.7910) -0.0097 (-0.7907)\nmonth7            0.0069 (0.4986)   0.0024 (0.1805)\nmonth8          -0.0297* (-2.131)  -0.0201 (-1.602)\nmonth9          -0.0306* (-1.997)  -0.0142 (-1.127)\nmonth10           0.0144 (0.9907)   0.0144 (0.9903)\nmonth11            0.0165 (1.335)    0.0128 (1.071)\nmonth12            0.0133 (1.003)   0.0087 (0.6897)\n_______________ _________________ _________________\nVCOV type       Heteroskeda.-rob. Heteroskeda.-rob.\nObservations                  422               412\nR2                        0.07502           0.03994\nAdj. R2                   0.05020           0.01354\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIndeed, now no month exhibits a statistically significant outperformance compared to January.\nNext, I follow Jacobsen and Bouman (2002) and simply regress excess returns on dummies that indicate specific seasons, i.e., I estimate the model\n\\[ y_t=\\alpha + \\beta D_t + \\epsilon_t,\\]\nwhere \\(D_t\\) is a dummy variable equal to one for the months in a specific season and zero otherwise. We consider both the ‘Halloween’ season (where the dummy is one for November-April). If \\(D_t\\) is statistically significant and positive for the corresponding season, then we take this as evidence for the presence of seasonality effects.\n\nhalloween_months &lt;- c(11, 12, 1, 2, 3, 4)\n\ndax_monthly &lt;- dax_monthly |&gt;\n  mutate(halloween = if_else(month %in% halloween_months, 1L, 0L))\n\nWe again estimate two models to analyze the ‘Halloween’ effect:\n\nmodel_halloween &lt;- feols(\n  ret_excess ~ halloween, \n  data = dax_monthly,\n  vcov = \"hetero\"\n)\n\nmodel_halloween_trimmed &lt;- feols(\n  ret_excess ~ halloween, \n  data = dax_monthly |&gt; \n     filter(ret_excess &gt;= ret_excess_q01 & ret_excess &lt;= ret_excess_q99),\n  vcov = \"hetero\"\n)\n\netable(model_halloween, model_halloween_trimmed, coefstat = \"tstat\")\n\n                  model_halloween model_hallowe..\nDependent Var.:        ret_excess      ret_excess\n                                                 \nConstant        -0.0024 (-0.5698) 0.0016 (0.4271)\nhalloween        0.0159** (2.827) 0.0088. (1.754)\n_______________ _________________ _______________\nVCOV type       Heteroskeda.-rob. Heteroske.-rob.\nObservations                  422             412\nR2                        0.01865         0.00745\nAdj. R2                   0.01632         0.00503\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe indeed find evidence that excess returns are higher during the months November-April relative to the remaining months in the full sample. However, if we remove the top and bottom 1% of observations, then the statistical significant outperformance disappears again.\nAs a last step, let us compare three different strategies: (i) buy and hold the index over the full year, (ii) go long in the index outside of the Halloween season and otherwise hold the risk-free asset, and (iii) go long in the index outside of the Halloween season and otherwise short the index. Below I compare the returns of the three different strategies on an annual basis:\n\ndax_monthly &lt;- dax_monthly |&gt;\n  mutate(\n    ret_excess_halloween = if_else(halloween == 1, ret_excess, 0),\n    ret_excess_halloween_short = if_else(halloween == 1, ret_excess, -ret_excess)\n  )\n\nWhich of these strategies might constitute a better investment opportunity? For a very simple assessment, let us compute the corresponding Sharpe ratios. Note that I annualize Sharpe ratios by multiplying them with \\(\\sqrt{12}\\) which strictly speaking only works under IID distributed returns (which is typically unlikely to be the case), but which suffices for the purpose of this post.\n\nsharpe_ratio &lt;- function(x) {\n  sqrt(12) *  mean(x) / sd(x)\n}\n\nbind_rows(\n  dax_monthly |&gt;\n    summarize(\n      `Buy and Hold` = sharpe_ratio(ret),\n      `Halloween` = sharpe_ratio(ret_excess_halloween),\n      `Halloween-Short` = sharpe_ratio(ret_excess_halloween_short)\n    ) |&gt; \n    mutate(Data = \"Full\"),\n  dax_monthly |&gt; \n    filter(ret_excess &gt;= ret_excess_q01 & ret_excess &lt;= ret_excess_q99) |&gt;\n    summarize(\n      `Buy and Hold` = sharpe_ratio(ret_excess),\n      `Halloween` = sharpe_ratio(ret_excess_halloween),\n      `Halloween-Short` = sharpe_ratio(ret_excess_halloween_short)\n    ) |&gt; \n    mutate(Data = \"Trimmed\")\n) |&gt; \n  select(Data, everything())\n\n# A tibble: 2 × 4\n  Data    `Buy and Hold` Halloween `Halloween-Short`\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n1 Full             0.465     0.597             0.473\n2 Trimmed          0.403     0.503             0.298\n\n\nThe Sharpe ratio suggests that the Halloween strategy is a better investment opportunity than the other strategies for both the full and the trimmed sample. Shorting the market in the Halloween season even leads to worse performance than just staying invested in the market the whole time once we drop the outliers.\nTo sum up, this post showed some simple data-related issues that we should consider when we analyze return data. Overall, we could find strong support for this seasonality effect from a statistical perspective once we get rid of a few extreme observations."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html",
    "href": "posts/tidy-collaborative-filtering/index.html",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "",
    "text": "Recommender systems are a key component of our digital lifes, ranging from e-commerce, online advertisements, movie recommendations, or more generally all kinds of product recommendations. A recommender system aims to efficiently deliver personalized content to users based on a large pool of potentially relevant information. In this blog post, I illustrate the concept of recommender systems by building a simple stock recommendation tool that relies on publicly available portfolios from the social trading platform wikifolio.com. The resulting recommender proposes stocks to investors who already have their own portfolios and look for new investment opportunities. The underlying assumption is that the wikifolio traders hold stock portfolios that can provide meaningful inspiration for other investors. The resulting stock recommendations of course do not constitute any investment advice and rather serve an illustrative purpose.\nwikifolio.com is the leading social trading platform in Europe, where anyone can publish and monetize their trading strategies through virtual portfolios, which are called wikifolios. The community of wikifolio traders includes full time investors and successful entrepreneurs, as well as experts from different sectors, portfolio managers, and editors of financial magazines. All traders share their trading ideas through fully transparent wikifolios. The wikifolios are easy to track and replicate, by investing in the corresponding, collateralized index certificate. As of writing, there are more than 30k published wikifolios of more than 9k traders, indicating a large diversity of available portfolios for training our recommender.\nThere are essentially three types of recommender models: recommenders via collaborative filtering, recommenders via content-based filtering, and hybrid recommenders (that mix the first two). In this blog post, I focus on the collaborative filtering approach as it requires no information other than portfolios and can provide fairly high precision with little complexity. Nonetheless, I first briefly describe the recommender approaches and refer to Ricci et al. (2011)1 for a comprehensive exposition."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#a-primer-on-recommender-systems",
    "href": "posts/tidy-collaborative-filtering/index.html#a-primer-on-recommender-systems",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "A Primer on Recommender Systems",
    "text": "A Primer on Recommender Systems\n\nCollaborative Filtering\nIn collaborative filtering, recommendations are based on past user interactions and items to produce new recommendations. The central notion is that past user-item interactions are sufficient to detect similar users or items. Broadly speaking, there are two sub-classes of collaborative filtering: the memory-based approach, which essentially searches nearest neighbors based on recorded transactions and is hence model-free, and the model-based approach, where new representations of users and items are built based on some generative pre-estimated model. Theoretically, the memory-based approach has a low bias (since no latent model is assumed) but a high variance (since the recommendations change a lot in the nearest neighbor search). The model-based approach relies on a trained interaction model. It has a relatively higher bias but a lower variance, i.e., recommendations are more stable since they come from a model.\nAdvantages of collaborative filtering include: (i) no information about users or items is required; (ii) a high precision can be achieved with little data; (iii) the more interaction between users and items is available, the more recommendations become accurate. However, the disadvantages of collaborative filtering are: (i) it is impossible to make recommendations to a new user or recommend a new item (cold start problem); (ii) calculating recommendations for millions of users or items consumes a lot of computational power (scalability problem); (iii) if the number of items is large relative to the users and most users only have interacted with a small subset of all items, then the resulting representation has many zero interactions and might hence lead to computational difficulties (sparsity problem).\n\n\nContent-Based Filering\nContent-based filtering methods exploit information about users or items to create recommendations by building a model that relates available characteristics of users or items to each other. The recommendation problem is hence cast into a classification problem (the user will like the item or not) or more generally a regression problem (which rating will the user give an item). The classification problem can be item-centered by focusing on available user information and estimating a model for each item. If there are a lot of user-item interactions available, the resulting model is fairly robust, but it is less personalized (as it ignores user characteristics apart from interactions). The classification problem can also be user-centered by working with item features and estimating a model for each user. However, if a user only has a few interactions then the resulting model becomes easily unstable. Content-based filtering can also be neither user nor item-centered by stacking the two feature vectors, hence considering both input simultaneously, and putting them into a neural network.\nThe main advantage of content-based filtering is that it can make recommendations for new users without any interaction history or recommend new items to users. The disadvantages include: (i) training needs a lot of users and item examples for reliable results; (ii) tuning might be much harder in practice than collaborative filtering; (iii) missing information might be a problem since there is no clear solution how to treat missingness in user or item characteristics.\n\n\nHybrid Recommenders\nHybrid recommender systems combine both collaborative and content-based filtering to overcome the challenges of each approach. There are different hybridization techniques available, e.g., combining the scores of different components (weighted), chosing among different component (switching), following strict priority rules (cascading), presenting outputs from different components at the same time (mixed), etc."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#train-collaborative-filtering-recommenders-in-r",
    "href": "posts/tidy-collaborative-filtering/index.html#train-collaborative-filtering-recommenders-in-r",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Train Collaborative Filtering Recommenders in R",
    "text": "Train Collaborative Filtering Recommenders in R\nFor this post, we rely on the tidyverse family of packages, scales for scale functions for visualization, and recommenderlab2 - a package that provides an infrastructure to develop and test collaborative filtering recommender algorithms.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(recommenderlab)\n\nI load a data set with stock holdings of investable wikifolios at the beginning of 2023 that I host in one of my repositories. The data contains the portfolios of 6,544 wikifolios that held in total 5,069 stocks on January 1st 2023.\n\nwikifolio_portfolios &lt;- read_csv(\"https://raw.githubusercontent.com/christophscheuch/christophscheuch.github.io/main/data/wikifolio_portfolios.csv\") \nglimpse(wikifolio_portfolios)\n\nRows: 149,916\nColumns: 2\n$ wikifolio &lt;chr&gt; \"DE000LS9AAB3\", \"DE000LS9AAB3\", \"DE000LS9AAB3\", \"DE000LS9AAB…\n$ stock     &lt;chr&gt; \"AU000000CSL8\", \"AU0000193666\", \"CA0679011084\", \"CA136635109…\n\n\nFirst, I convert the long data to a binary rating matrix.\n\nbinary_rating_matrix &lt;- wikifolio_portfolios |&gt;\n  mutate(in_portfolio = 1) |&gt; \n  pivot_wider(id_cols = wikifolio,\n              names_from = stock,\n              values_from = in_portfolio,\n              values_fill = list(in_portfolio = 0)) |&gt;\n  select(-wikifolio) |&gt;\n  as.matrix() |&gt;\n  as(\"binaryRatingMatrix\")\nbinary_rating_matrix\n\n6544 x 5069 rating matrix of class 'binaryRatingMatrix' with 149916 ratings.\n\n\nI perform cross-validation and split the data into training and test data. The training sample constitute 80% of the data and I perform 5-fold cross validation. Testing is performed by withholding items from the test portfolios (parameter given) and checking how well the algorithm predicts the withheld items. The value given=-1 means that an algorithm sees all but 1 withheld stock for the test portfolios and needs to predict the missing stock. I refer to Breese et al. (1998)3 for a discussion of other withholding strategies.\n\nscheme &lt;- binary_rating_matrix |&gt;\n  evaluationScheme(\n    method = \"cross-validation\",\n    k      = 5,\n    train  = 0.8,\n    given  = -1\n)\nscheme\n\nEvaluation scheme using all-but-1 items\nMethod: 'cross-validation' with 5 run(s).\nGood ratings: NA\nData set: 6544 x 5069 rating matrix of class 'binaryRatingMatrix' with 149916 ratings.\n\n\nHere is the list of recommenders that I consider for the backtest with some intuition:\n\nRandom Items: the benchmark case because it just stupidly chooses random stocks from all possible choices.\nPopular Items: just recommends the most popular stocks to measured by the number of wikifolios that hold the stock.\nAssociation Rules: each wikifolio and its portfolio is considered as a transaction. Association rule mining finds similar portfolios across all traders (if traders have x, y and z in their portfolio, then they are X% likely of also including w).\nItem-Based Filtering: the algorithm calculates a similarity matrix across stocks. Recommendations are then based on the list of most similar stocks to the ones the wikifolio already has in its portfolio.\nUser-Based Filtering: the algorithm finds a neighborhood of similar wikifolios for each wikifolio (for this exercise it is set to 100 most similar wikifolios). Recommendations are then based on what the most similar wikifolios have in their portfolio.\n\nFor each algorithm, I base the evaluation on 1, 3, 5, and 10 recommendations. This specification means that each algorithm proposes 1 to 10 recommendations to the test portfolios and the evaluation scheme then checks whether the proposals contain the one withheld stock. Note that the evaluation takes a couple of hours, in particular because the Item-Based and User-Based Filtering approaches are quite time-consuming.\n\nalgorithms &lt;- list(\n  \"Random Items\"         = list(name  = \"RANDOM\",  param = NULL),\n  \"Popular Items\"        = list(name  = \"POPULAR\", param = NULL),\n  \"Association Rules\"    = list(name  = \"AR\", param = list(supp = 0.01, conf = 0.1)),\n  \"Item-Based Filtering\" = list(name  = \"IBCF\", param = list(k = 10)),\n  \"User-Based Filtering\" = list(name  = \"UBCF\", param = list(method = \"Cosine\", nn = 100))\n)\n\nnumber_of_recommendations &lt;- c(1, 3, 5, 10)\nresults &lt;- evaluate(\n  scheme,\n  algorithms,\n  type = \"topNList\",\n  progress = TRUE,\n  n = number_of_recommendations\n)"
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#evaluate-recommenders",
    "href": "posts/tidy-collaborative-filtering/index.html#evaluate-recommenders",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Evaluate Recommenders",
    "text": "Evaluate Recommenders\nThe output of evaluate() already provides the evaluation metrics in a structured way. I can simply average the metrics over the cross-validation folds.\n\nresults_tbl &lt;- results |&gt;\n  avg() |&gt;\n  map(as_tibble) |&gt;\n  bind_rows(.id = \"model\")\n\nNow, for each recommender, we are interested the following numbers:\n\nTrue Negative (TN) = number of not predicted items that do not correspond to withheld items\nFalse Positive (FP) = number of incorrect predictions that do not correspond to withheld items\nFalse negative (FN) = number of not predicted items that correspond to withheld items\nTrue Positive (TP) = number of correct predictions that correspond to withheld items\n\nThe two figures below present the most common evaluation techniques for the performance of recommender algorithms in backtest settings like mine.\n\nROC curves\nThe first visualization approach comes from signal-detection and is called “Receiver Operating Characteristic” (ROC). The ROC-curve plots the algorithm’s probability of detection (TPR) against the probability of false alarm (FPR).\n\nTPR = TP / (TP + FN) (i.e., share of true positive recommendations relative to all known portfolios)\nFPR = FP / (FP + TN) (i.e., share of incorrect recommendations relative to )\n\nIntuitively, the bigger the area under the ROC curve, the better is the corresponding algorithm.\n\nresults_tbl |&gt;\n  ggplot(aes(FPR, TPR, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"False Positive Rate (FPR)\",\n    y = \"True Positive Rate (TPR)\",\n    title = \"ROC curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\nThe figure shows that recommending random items exhibits the lowest TPR for any FPR, so it is the worst among all algorithms (which is not surprising). Association rules, on the other hand, constitute the best algorithm among the current selection. This result is neat because association rule mining is a computationally cheap algorithm, so we could potentially fine-tune or reestimate the model easily.\n\n\nPrecision-Recall Curves\nThe second popular approach is to plot Precision-Recall curves. The two measures are often used in information retrieval problems:\n\nPrecision = TP / (TP + FP) (i.e., correctly recommended items relative to total recommended items)\nRecall = TP / (TP + FN) (i.e., correctly recommended items relative to total number of known useful recommendations)\n\nThe goal is to have a higher precision for any level of recall. In fact, there is trade-off between the two measures since high precision means low recall and vice-versa.\n\nresults_tbl |&gt;\n  ggplot(aes(x = recall, y = precision, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"Recall\", y = \"Precision\",\n    title = \"Precision-Recall curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\nAgain, proposing random items exhibits the worst performance, as for any given level of recall, this approach has the lowest precision. Association rules are also the best algorithm with this visualization approach."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#create-predictions",
    "href": "posts/tidy-collaborative-filtering/index.html#create-predictions",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Create Predictions",
    "text": "Create Predictions\nThe final step is to create stock recommendations for investors who already have portfolios. I pick the AR algorithm to create such recommendations because it excelled in the analyses above. Note that in the case of association rules, I also need to provide the support and confidence parameters.\n\nrecommender &lt;- Recommender(binary_rating_matrix, method = \"AR\", param = list(supp = 0.01, conf = 0.1))\n\nAs an example, suppose you currently have a portfolio that consists of Nvidia (US67066G1040) and Apple (US0378331005). I have to transform this sample portfolio into a rating matrix with the same dimensions as the data we used as input for our training. The predict() function then delivers a prediction for the example portfolio.\n\nsample_portfolio &lt;- c(\"US67066G1040\", \"US0378331005\")\nsample_rating_matrix &lt;- tibble(distinct(wikifolio_portfolios, stock)) |&gt;\n  mutate(in_portfolio = if_else(stock %in% sample_portfolio, 1, 0)) |&gt;\n  pivot_wider(names_from = stock,\n              values_from = in_portfolio,\n              values_fill = list(in_portfolio = 0)) |&gt;\n  as.matrix() |&gt;\n  as(\"binaryRatingMatrix\")\n\nprediction &lt;- predict(recommender, sample_rating_matrix, n = 1)\nas(prediction, \"list\")[[1]]\n\n[1] \"US5949181045\"\n\n\nSo the AR algorithm recommends Microsoft (US5949181045) as a stock if you are already invested in Nvidia and Apple, which makes a lot of sense given the similarity in business model. Of course, this recommendation is not serious investment advice, but rather serves an illustrative purpose of how recommenderlab can be used to quickly prototype different collaborative filtering recommender algorithms."
  }
]