[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy Intelligence",
    "section": "",
    "text": "Text-to-Speech with Goolge AI\n\n\nCreating an audio book using Google’s Text-to-Speech API\n\n\n\n\n\n\n\n\n\n\n\n\n\nRapid RAG Prototyping\n\n\nBuilding a Retrieval-Augmented Generation (RAG) prototype with ellmer and duckdb in R\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Debt Networks\n\n\nExplore external debt relationships with interactive networks in R\n\n\n\n\n\n\n\n\n\n\n\n\n\nSakura Visualizations\n\n\nVisualizing first and full cherry blossom bloom dates for Japanese regions using R and Python\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Manipulation: dplyr vs siuba\n\n\nA comparison of R’s dplyr and Python’s siuba data manipulation packages\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Classification Models: Customer Churn Prediction\n\n\nA comparison of classification approaches using the tidymodels R package\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Fixed Effects Regressions: fixest vs pyfixest\n\n\nA comparison of packages for fast fixed-effects estimation in R and Python\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Visualization: ggplot2 vs matplotlib\n\n\nA comparison of the two most popular data visualization tools for R and Python\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Data Visualization with Python\n\n\nA comparison of the dynamic visualization libraries plotly, bokeh and altair for the programming language Python\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Data Visualization with R\n\n\nA comparison of the dynamic visualization packages ggiraph, plotly and highcharter for the programming language R\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data: Tabular Data Storage Comparison\n\n\nA comparison of popular open-source data storage technologies using R and Python\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Visualization: ggplot2 vs seaborn\n\n\nA comparison of two popular data visualization tools for R and Python\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Visualization: ggplot2 vs plotnine\n\n\nA comparison of implementations of the grammar of graphics in R and Python.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Manipulation: dplyr vs TidierData\n\n\nA comparison of R’s dplyr and Julia’s TidierData data manipulation packages\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Manipulation: dplyr vs pandas\n\n\nA comparison of R’s dplyr and Python’s pandas data manipulation packages\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Manipulation: dplyr vs ibis\n\n\nA comparison of R’s dplyr and Python’s ibis data manipulation packages\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Manipulation: dplyr vs polars\n\n\nA comparison of R’s dplyr and Python’s polars data manipulation packages\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Seasonality in DAX Returns\n\n\nDebunking the Halloween indicator for German market returns using R\n\n\n\n\n\n\n\n\n\n\n\n\n\nScraping ESG Data from Yahoo Finance\n\n\nHow to scrape environmental, social and governance risk scores using R\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering Binary Data\n\n\nAn application of different unsupervised learning approaches to cluster simulated survey responses using R\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data: A Recipe for Efficient Data Analysis\n\n\nOn the importance of tidy data for efficient analysis using the analogy of a well-organized kitchen\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Collaborative Filtering: Building A Stock Recommender\n\n\nA simple implementation for prototyping multiple collaborative filtering algorithms using R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/international-external-debt-network/index.html",
    "href": "posts/international-external-debt-network/index.html",
    "title": "Global Debt Networks",
    "section": "",
    "text": "In this blog post, I visualize external debt using interactive networks with R. Understanding external debt flows between countries and creditors is a crucial aspect in analyzing global economic relationships. Using tools such as the recently released wbids package to download data from the World Bank International Debt statistics and visNetwork for visualization, we can uncover interesting insights.\nThis post relies on the following packages.\nlibrary(tidyverse)\nlibrary(wbids)\nlibrary(visNetwork)\nlibrary(scales)"
  },
  {
    "objectID": "posts/international-external-debt-network/index.html#data-preparation",
    "href": "posts/international-external-debt-network/index.html#data-preparation",
    "title": "Global Debt Networks",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe wbids package provides easy access to download debtor-creditor relationships for all available countries. Here, we pull external debt data (IDS series “DT.DOD.DPPG.CD”) for 2022 using ids_get() and enrich it with geographical and counterpart details. Note that we drop “World” and “Region” counterparts because we rather want to look at their individual components.\n\ngeographies &lt;- ids_list_geographies() |&gt; \n  filter(geography_type != \"Region\")\n\nexternal_debt_raw &lt;- geographies$geography_id |&gt; \n  map_df(\\(x) ids_get(x, \"DT.DOD.DPPG.CD\", \"all\", 2022, 2022))\n\ncounterparts &lt;- ids_list_counterparts()\n\nexternal_debt &lt;- external_debt_raw |&gt; \n  filter(value &gt; 0) |&gt; \n  left_join(geographies, join_by(geography_id)) |&gt; \n  left_join(counterparts, join_by(counterpart_id)) |&gt; \n  filter(!counterpart_name %in% c(\"World\", \"Region\")) |&gt; \n  select(from = geography_name, to = counterpart_name, value, counterpart_type) |&gt; \n  mutate(to = str_squish(to))"
  },
  {
    "objectID": "posts/international-external-debt-network/index.html#debtor-centric-view",
    "href": "posts/international-external-debt-network/index.html#debtor-centric-view",
    "title": "Global Debt Networks",
    "section": "Debtor-Centric View",
    "text": "Debtor-Centric View\nFor now, let’s narrow our focus to specific debtor countries. This step allows us to examine the relationships from a debtor’s perspective, understanding how much debt they owe and to whom. In the following, we focus on Nigeria and its neighbor Cameroon.\n\nselected_geographies &lt;- c(\"Nigeria\", \"Cameroon\")\n\nexternal_debt_sub &lt;- external_debt |&gt;\n  filter(from %in% selected_geographies) \n\nvisNetwork requires two data frames: nodes and edges, each with corresponding properties. To enhance the visualization, we create helper functions for formatting node titles and labels:\n\nformat_label &lt;- function(id) {\n  label &lt;- str_wrap(id, width = 20)\n  label\n}\n\nformat_debt &lt;- function(x, decimals = 2) {\n  debt &lt;- sapply(x, function(value) {\n    if (is.na(value)) {\n      return(NA_character_)\n    }\n    if (abs(value) &lt; 1e7) {\n      formatted_value &lt;- sprintf(paste0(\"%.\", decimals, \"f\"), value / 1e6)\n      return(paste0(formatted_value, \"M\"))\n    } else {\n      formatted_value &lt;- sprintf(paste0(\"%.\", decimals, \"f\"), value / 1e9)\n      return(paste0(formatted_value, \"B\"))\n    }\n  })\n  debt\n}\n\nformat_title &lt;- function(id, value_from, value_to) {\n  title &lt;- case_when(\n    value_from &gt; 0 & value_to &gt; 0 ~ str_c(\n      id, \n      \"&lt;br&gt;Received: \", format_debt(value_from), \n      \"&lt;br&gt;Provided: \", format_debt(value_to)\n    ),\n    value_from &gt; 0 ~ str_c(\n      id, \n      \"&lt;br&gt;Received: \", format_debt(value_from)\n    ),\n    value_to &gt; 0 ~ str_c(\n      id, \n      \"&lt;br&gt;Provided: \", format_debt(value_to)\n    ),\n    TRUE ~ NA_character_\n  )\n  title\n}\n\nWe now construct the nodes and edges for the network. Nodes represent entities (countries or institutions), and edges represent debt relationships. The data looks like this:\n\ncreate_nodes &lt;- function(external_debt_sub) {\n  \n  total_debt &lt;- sum(external_debt_sub$value)\n  \n  nodes &lt;- external_debt_sub |&gt; \n    group_by(id = from, color = \"Country\") |&gt; \n    summarize(value_from = sum(value),\n              .groups = \"drop\") |&gt; \n    bind_rows(\n      external_debt_sub |&gt; \n        group_by(id = to, color = counterpart_type) |&gt; \n        summarize(value_to = sum(value),\n                  .groups = \"drop\")\n    ) |&gt; \n    group_by(id, color) |&gt; \n    summarize(across(c(value_from, value_to), \\(x) sum(x, na.rm = TRUE)),\n              .groups = \"drop\") |&gt; \n    mutate(\n      title = format_title(id, value_from, value_to),\n      label = format_label(id),\n      value = coalesce(value_from, 0) + coalesce(value_to, 0),\n      size = value / total_debt,\n      color = case_when(\n        color == \"Other\" ~ \"#C46231\",\n        color == \"Country\" ~ \"#3193C4\",\n        color == \"Global MDBs\" ~ \"#AB31C4\",\n        color == \"Bondholders\" ~ \"#4AC431\"\n      )\n    )\n  nodes\n}\n\nnodes &lt;- create_nodes(external_debt_sub)\nnodes\n\n# A tibble: 33 × 8\n   id                      color value_from value_to title label   value    size\n   &lt;chr&gt;                   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 African Dev. Bank       #C46…    0        4.18e 9 Afri… \"Afr… 4.18e 9 8.07e-2\n 2 African Export-Import … #C46…    0        7.60e 7 Afri… \"Afr… 7.60e 7 1.47e-3\n 3 Arab Bank for Economic… #C46…    0        7.23e 7 Arab… \"Ara… 7.23e 7 1.39e-3\n 4 Austria                 #319…    0        8.78e 6 Aust… \"Aus… 8.78e 6 1.69e-4\n 5 Belgium                 #319…    0        5.69e 7 Belg… \"Bel… 5.69e 7 1.10e-3\n 6 Bondholders             #4AC…    0        1.73e10 Bond… \"Bon… 1.73e10 3.33e-1\n 7 Cameroon                #319…    1.18e10  0       Came… \"Cam… 1.18e10 2.28e-1\n 8 Central Bank of West A… #C46…    0        2.28e 7 Cent… \"Cen… 2.28e 7 4.40e-4\n 9 China                   #319…    0        8.08e 9 Chin… \"Chi… 8.08e 9 1.56e-1\n10 Dev. Bank of the Centr… #C46…    0        3.80e 6 Dev.… \"Dev… 3.80e 6 7.33e-5\n# ℹ 23 more rows\n\n\nEdges add the connective tissue to the network, showing who owes whom. Here is how the example data looks like:\n\ncreate_edges &lt;- function(external_debt_sub) {\n  edges &lt;- external_debt_sub |&gt; \n    select(from, to) |&gt; \n    mutate(\n      shadow = TRUE, \n      color = \"grey\",\n      smooth = TRUE\n    )\n  edges\n}\n\nedges &lt;- create_edges(external_debt_sub)\nedges\n\n# A tibble: 45 × 5\n   from     to                               shadow color smooth\n   &lt;chr&gt;    &lt;chr&gt;                            &lt;lgl&gt;  &lt;chr&gt; &lt;lgl&gt; \n 1 Cameroon World Bank-IDA                   TRUE   grey  TRUE  \n 2 Cameroon World Bank-IBRD                  TRUE   grey  TRUE  \n 3 Cameroon United States                    TRUE   grey  TRUE  \n 4 Cameroon United Kingdom                   TRUE   grey  TRUE  \n 5 Cameroon United Arab Emirates             TRUE   grey  TRUE  \n 6 Cameroon Turkiye                          TRUE   grey  TRUE  \n 7 Cameroon Switzerland                      TRUE   grey  TRUE  \n 8 Cameroon Spain                            TRUE   grey  TRUE  \n 9 Cameroon Saudi Arabia                     TRUE   grey  TRUE  \n10 Cameroon OPEC Fund for International Dev. TRUE   grey  TRUE  \n# ℹ 35 more rows\n\n\nThe visNetwork library brings everything together, producing an interactive network. This visualization provides a debtor-centric perspective, illustrating how selected countries distribute their debt obligations among creditors.\n\nvisualize_network &lt;- function(external_debt_sub) {\n  nodes &lt;- create_nodes(external_debt_sub)\n  edges &lt;- create_edges(external_debt_sub)\n  \n  visNetwork(\n    nodes, edges, width = \"100%\", height = \"600px\"\n  ) |&gt; \n    visNodes(shape = \"dot\")\n}\n\nvisualize_network(external_debt_sub)\n\n\n\n\n\nVisualizing the network from a debtor’s perspective sheds light on the diversity of funding sources for countries like Nigeria and Cameroon. While both nations share some creditors, the wider spread of Cameroon’s creditor network could indicate stronger diversification in funding sources—a potentially advantageous position for economic resilience."
  },
  {
    "objectID": "posts/international-external-debt-network/index.html#creditor-centric-view",
    "href": "posts/international-external-debt-network/index.html#creditor-centric-view",
    "title": "Global Debt Networks",
    "section": "Creditor-Centric View",
    "text": "Creditor-Centric View\nIf you wondered why we wrapped everything into functions, then here is the resolution: we can use the same function to examine the network from creditors’ perspective. For example, let’s focus on Austria (my home country) and Germany (where I currently live).\n\nselected_counterparts &lt;- c(\"Austria\", \"Germany, Fed. Rep. of\")\n\nexternal_debt_sub &lt;- external_debt |&gt;\n  filter(to %in% selected_counterparts) \n\nvisualize_network(external_debt_sub)\n\n\n\n\n\nThis approach reveals which countries owe these creditors and in what amounts. For me, it is interesting to see that Austria shares a lof of counterparts with Germany (which is not surprising), but that Germany provides credit to many more counterparts around the World. Germany’s broader network of counterparts underscores its role as a significant lender globally, while Austria’s overlapping but smaller network highlights the nuanced dynamics of regional lending patterns."
  },
  {
    "objectID": "posts/international-external-debt-network/index.html#putting-everyhing-into-an-app",
    "href": "posts/international-external-debt-network/index.html#putting-everyhing-into-an-app",
    "title": "Global Debt Networks",
    "section": "Putting Everyhing into an App",
    "text": "Putting Everyhing into an App\nDo you want to quickly look at different countries, counterparts, or time periods? The code above actually constitutes the buildings blocks of a shiny app that allows you to explore the data interactively - check out the Debt Network Visualizer!"
  },
  {
    "objectID": "posts/international-external-debt-network/index.html#concluding-remarks",
    "href": "posts/international-external-debt-network/index.html#concluding-remarks",
    "title": "Global Debt Networks",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nBy combining wbids for data retrieval, tidyverse for manipulation, and visNetwork for visualization, you can quickly uncover intricate patterns in global debt relationships. Try adapting this workflow to your own analysis!"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html",
    "href": "posts/dplyr-vs-tidierdata/index.html",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "",
    "text": "There are a myriad of options to perform essential data manipulation tasks in R and Julia. However, if we want to do tidy data science in R, there is a clear forerunner: dplyr. In the world of Julia, TidierData is a relatively new kid on the block that allows R users to dabble in Julia without learning a lot of new syntax. In this blog post, I illustrate their syntactic similarities and highlight differences between these two packages that emerge for a few key tasks.\nBefore we dive into the comparison, a short introduction to the packages: the dplyr package in R allows users to refer to columns without quotation marks due to its implementation of non-standard evaluation (NSE). NSE is a programming technique used in R that allows functions to capture the expressions passed to them as arguments, rather than just the values of those arguments. The primary goal of NSE in the context of dplyr is to create a more user-friendly and intuitive syntax. This makes data manipulation tasks more straightforward and aligns with the general philosophy of the tidyverse to make data science faster, easier, and more fun.1\nTidierData is a 100% Julia implementation of the dplyr and tidyr R packages with three goals: (i) stick as closely to the tidyverse syntax as possible, so that R users find it easier to switch; (ii) make broadcasting2 mostly invisible, so that many functions are automatically vectorized for users; (iii) make scalars and tuples mostly interchangeable, so that users can provide a scalar or a tuple as arguments as they see fit. Check out the package website for more information, in particular with respect to the features of Julia."
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#loading-packages-and-data",
    "href": "posts/dplyr-vs-tidierdata/index.html#loading-packages-and-data",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Loading packages and data",
    "text": "Loading packages and data\nWe start by loading the main packages of interest and the popular palmerpenguins package that exists for both R and Julia. Note that packages in Julia follow a Pascal case convention, so we have TidierData and PalmerPenguins. We then use the penguins data frame as the data to compare all functions and methods below. Note that ENV[\"LINES\"] = 19 sets the print output of Julia data frames to show only 10 rows.3\n\ndplyrTidierData\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins &lt;- palmerpenguins::penguins\n\n\n\n\nusing TidierData\nusing PalmerPenguins\n\npenguins = DataFrame(PalmerPenguins.load())\nENV[\"LINES\"] = 19"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#work-with-rows",
    "href": "posts/dplyr-vs-tidierdata/index.html#work-with-rows",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Work with rows",
    "text": "Work with rows\n\nFilter rows\nFiltering rows with dplyr is based on NSE and the dplyr::filter() function. To replicate the same results with TidierData, you can use TidierData.@filter() method which accepts a remarkably similar notation to dplyr with the only exceptions that you need && or || for boolean operators and that you can omit the percentage signs around in.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  filter(species == \"Adelie\" & \n           island %in% c(\"Biscoe\", \"Dream\"))\n\n# A tibble: 100 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 90 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n@chain penguins begin\n  @filter(species == \"Adelie\" &&\n            island in (\"Biscoe\", \"Dream\"))\nend\n\n100×7 DataFrame\n Row │ species   island    bill_length_mm  bill_depth_mm  flipper_length_mm  b ⋯\n     │ String15  String15  Float64?        Float64?       Int64?             I ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Adelie    Biscoe              37.8           18.3                174    ⋯\n   2 │ Adelie    Biscoe              37.7           18.7                180\n   3 │ Adelie    Biscoe              35.9           19.2                189\n   4 │ Adelie    Biscoe              38.2           18.1                185\n   5 │ Adelie    Biscoe              38.8           17.2                180    ⋯\n  ⋮  │    ⋮         ⋮            ⋮               ⋮                ⋮            ⋱\n  96 │ Adelie    Dream               36.6           18.4                184\n  97 │ Adelie    Dream               36.0           17.8                195\n  98 │ Adelie    Dream               37.8           18.1                193\n  99 │ Adelie    Dream               36.0           17.1                187    ⋯\n 100 │ Adelie    Dream               41.5           18.5                201\n                                                   2 columns and 90 rows omitted\n\n\n\n\n\n\n\nSlice rows\ndplyr::slice() takes integers with row numbers as inputs, so you can use ranges and arbitrary vectors of integers. TidierData.@slice() does exactly the same.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  slice(10:20)\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           42            20.2               190        4250\n 2 Adelie  Torgersen           37.8          17.1               186        3300\n 3 Adelie  Torgersen           37.8          17.3               180        3700\n 4 Adelie  Torgersen           41.1          17.6               182        3200\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           36.6          17.8               185        3700\n 8 Adelie  Torgersen           38.7          19                 195        3450\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           34.4          18.4               184        3325\n11 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n@chain penguins begin\n  @slice(10:20)\nend\n\n11×7 DataFrame\n Row │ species   island     bill_length_mm  bill_depth_mm  flipper_length_mm   ⋯\n     │ String15  String15   Float64?        Float64?       Int64?              ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Adelie    Torgersen            42.0           20.2                190   ⋯\n   2 │ Adelie    Torgersen            37.8           17.1                186\n   3 │ Adelie    Torgersen            37.8           17.3                180\n   4 │ Adelie    Torgersen            41.1           17.6                182\n   5 │ Adelie    Torgersen            38.6           21.2                191   ⋯\n   6 │ Adelie    Torgersen            34.6           21.1                198\n   7 │ Adelie    Torgersen            36.6           17.8                185\n   8 │ Adelie    Torgersen            38.7           19.0                195\n   9 │ Adelie    Torgersen            42.5           20.7                197   ⋯\n  10 │ Adelie    Torgersen            34.4           18.4                184\n  11 │ Adelie    Torgersen            46.0           21.5                194\n                                                               2 columns omitted\n\n\n\n\n\n\n\nArrange rows\nTo orders the rows of a data frame by the values of selected columns, we have dplyr::arrange() and TidierData.@arrange(). Note that both approaches arrange rows in an an ascending order and puts missing values last as defaults.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  arrange(island, desc(bill_length_mm))\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n@chain penguins begin\n  @arrange(island, desc(bill_length_mm))\nend\n\n344×7 DataFrame\n Row │ species   island     bill_length_mm  bill_depth_mm  flipper_length_mm   ⋯\n     │ String15  String15   Float64?        Float64?       Int64?              ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Gentoo    Biscoe          missing        missing              missing   ⋯\n   2 │ Gentoo    Biscoe               59.6           17.0                230\n   3 │ Gentoo    Biscoe               55.9           17.0                228\n   4 │ Gentoo    Biscoe               55.1           16.0                230\n   5 │ Gentoo    Biscoe               54.3           15.7                231   ⋯\n  ⋮  │    ⋮          ⋮            ⋮               ⋮                ⋮           ⋱\n 340 │ Adelie    Torgersen            34.6           21.1                198\n 341 │ Adelie    Torgersen            34.6           17.2                189\n 342 │ Adelie    Torgersen            34.4           18.4                184\n 343 │ Adelie    Torgersen            34.1           18.1                193   ⋯\n 344 │ Adelie    Torgersen            33.5           19.0                190\n                                                  2 columns and 334 rows omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#work-with-columns",
    "href": "posts/dplyr-vs-tidierdata/index.html#work-with-columns",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Work with columns",
    "text": "Work with columns\n\nSelect columns\nSelecting a subset of columns works exactly the same withdplyr::select() and TidierData.@select().\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  select(bill_length_mm, sex)\n\n# A tibble: 344 × 2\n   bill_length_mm sex   \n            &lt;dbl&gt; &lt;fct&gt; \n 1           39.1 male  \n 2           39.5 female\n 3           40.3 female\n 4           NA   &lt;NA&gt;  \n 5           36.7 female\n 6           39.3 male  \n 7           38.9 female\n 8           39.2 male  \n 9           34.1 &lt;NA&gt;  \n10           42   &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n\n\n@chain penguins begin\n  @select(bill_length_mm, sex)\nend\n\n344×2 DataFrame\n Row │ bill_length_mm  sex\n     │ Float64?        String7\n─────┼─────────────────────────\n   1 │           39.1  male\n   2 │           39.5  female\n   3 │           40.3  female\n   4 │      missing    missing\n   5 │           36.7  female\n  ⋮  │       ⋮            ⋮\n 340 │           55.8  male\n 341 │           43.5  female\n 342 │           49.6  male\n 343 │           50.8  male\n 344 │           50.2  female\n               334 rows omitted\n\n\n\n\n\n\n\nRename columns\nRenaming columns also works exactly the same with dplyr::rename() and TidierData.rename().\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  rename(bill_length = bill_length_mm,\n         bill_depth = bill_depth_mm)\n\n# A tibble: 344 × 8\n   species island    bill_length bill_depth flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen        39.1       18.7               181        3750 male  \n 2 Adelie  Torgersen        39.5       17.4               186        3800 female\n 3 Adelie  Torgersen        40.3       18                 195        3250 female\n 4 Adelie  Torgersen        NA         NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen        36.7       19.3               193        3450 female\n 6 Adelie  Torgersen        39.3       20.6               190        3650 male  \n 7 Adelie  Torgersen        38.9       17.8               181        3625 female\n 8 Adelie  Torgersen        39.2       19.6               195        4675 male  \n 9 Adelie  Torgersen        34.1       18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen        42         20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\n\n\n@chain penguins begin\n  @rename(bill_length = bill_length_mm,\n          bill_depth = bill_depth_mm)\nend\n\n344×7 DataFrame\n Row │ species    island     bill_length  bill_depth  flipper_length_mm  body_ ⋯\n     │ String15   String15   Float64?     Float64?    Int64?             Int64 ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Adelie     Torgersen         39.1        18.7                181        ⋯\n   2 │ Adelie     Torgersen         39.5        17.4                186\n   3 │ Adelie     Torgersen         40.3        18.0                195\n   4 │ Adelie     Torgersen    missing     missing              missing      m\n   5 │ Adelie     Torgersen         36.7        19.3                193        ⋯\n  ⋮  │     ⋮          ⋮           ⋮           ⋮               ⋮                ⋱\n 340 │ Chinstrap  Dream             55.8        19.8                207\n 341 │ Chinstrap  Dream             43.5        18.1                202\n 342 │ Chinstrap  Dream             49.6        18.2                193\n 343 │ Chinstrap  Dream             50.8        19.0                210        ⋯\n 344 │ Chinstrap  Dream             50.2        18.7                198\n                                                  2 columns and 334 rows omitted\n\n\n\n\n\n\n\nMutate columns\nTransforming existing columns or creating new ones is an essential part of data analysis. dplyr::mutate() and TidierData.@mutate() are the work horses for these tasks. Note that you have to split up variable assignments if you want to refer to a newly created variable in TidierData, while you can refer to the new variables in the same mutate block in dplyr.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  mutate(ones = 1,\n         bill_length = bill_length_mm / 10,\n         bill_length_squared = bill_length^2) |&gt; \n  select(ones, bill_length_mm, bill_length, bill_length_squared)\n\n# A tibble: 344 × 4\n    ones bill_length_mm bill_length bill_length_squared\n   &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1     1           39.1        3.91                15.3\n 2     1           39.5        3.95                15.6\n 3     1           40.3        4.03                16.2\n 4     1           NA         NA                   NA  \n 5     1           36.7        3.67                13.5\n 6     1           39.3        3.93                15.4\n 7     1           38.9        3.89                15.1\n 8     1           39.2        3.92                15.4\n 9     1           34.1        3.41                11.6\n10     1           42          4.2                 17.6\n# ℹ 334 more rows\n\n\n\n\n\n@chain penguins begin\n  @mutate(ones = 1,\n          bill_length = bill_length_mm / 10)\n  @mutate(bill_length_squared = bill_length^2)\n  @select(ones, bill_length_mm, bill_length, bill_length_squared)\nend\n\n344×4 DataFrame\n Row │ ones   bill_length_mm  bill_length  bill_length_squared\n     │ Int64  Float64?        Float64?     Float64?\n─────┼─────────────────────────────────────────────────────────\n   1 │     1            39.1         3.91              15.2881\n   2 │     1            39.5         3.95              15.6025\n   3 │     1            40.3         4.03              16.2409\n   4 │     1       missing     missing            missing\n   5 │     1            36.7         3.67              13.4689\n  ⋮  │   ⋮          ⋮              ⋮                ⋮\n 340 │     1            55.8         5.58              31.1364\n 341 │     1            43.5         4.35              18.9225\n 342 │     1            49.6         4.96              24.6016\n 343 │     1            50.8         5.08              25.8064\n 344 │     1            50.2         5.02              25.2004\n                                               334 rows omitted\n\n\n\n\n\n\n\nRelocate columns\ndplyr::relocate() provides options to change the positions of columns in a data frame, using the same syntax as dplyr::select(). In addition, there are the options .after and .before to provide users with additional shortcuts.\nThe recommended way to relocate columns in TidierData is to use the TidierData.@select() method, but there are no options as in dplyr::relocate(). In fact, the safest way to consistently get the correct order of columns is to explicitly specify them.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  relocate(c(species, bill_length_mm), .before = sex)\n\n# A tibble: 344 × 8\n   island    bill_depth_mm flipper_length_mm body_mass_g species bill_length_mm\n   &lt;fct&gt;             &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1 Torgersen          18.7               181        3750 Adelie            39.1\n 2 Torgersen          17.4               186        3800 Adelie            39.5\n 3 Torgersen          18                 195        3250 Adelie            40.3\n 4 Torgersen          NA                  NA          NA Adelie            NA  \n 5 Torgersen          19.3               193        3450 Adelie            36.7\n 6 Torgersen          20.6               190        3650 Adelie            39.3\n 7 Torgersen          17.8               181        3625 Adelie            38.9\n 8 Torgersen          19.6               195        4675 Adelie            39.2\n 9 Torgersen          18.1               193        3475 Adelie            34.1\n10 Torgersen          20.2               190        4250 Adelie            42  \n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n@chain penguins begin\n  @select(island, bill_depth_mm, flipper_length_mm, body_mass_g, \n          species, bill_length_mm, sex)\nend\n\n344×7 DataFrame\n Row │ island     bill_depth_mm  flipper_length_mm  body_mass_g  species    bi ⋯\n     │ String15   Float64?       Int64?             Int64?       String15   Fl ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Torgersen           18.7                181         3750  Adelie        ⋯\n   2 │ Torgersen           17.4                186         3800  Adelie\n   3 │ Torgersen           18.0                195         3250  Adelie\n   4 │ Torgersen      missing              missing      missing  Adelie\n   5 │ Torgersen           19.3                193         3450  Adelie        ⋯\n  ⋮  │     ⋮            ⋮                ⋮               ⋮           ⋮         ⋱\n 340 │ Dream               19.8                207         4000  Chinstrap\n 341 │ Dream               18.1                202         3400  Chinstrap\n 342 │ Dream               18.2                193         3775  Chinstrap\n 343 │ Dream               19.0                210         4100  Chinstrap     ⋯\n 344 │ Dream               18.7                198         3775  Chinstrap\n                                                  2 columns and 334 rows omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#work-with-groups-of-rows",
    "href": "posts/dplyr-vs-tidierdata/index.html#work-with-groups-of-rows",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Work with groups of rows",
    "text": "Work with groups of rows\n\nSimple summaries by group\nLet’s suppose we want to compute summaries by groups such as means or medians. Both packages are virtually the same again: on the R side you have dplyr::group_by() and dplyr::summarize(), while on the Julia side you have TidierData.@group_by() and TidierData.@summarize(). Note that you have to include the skipmissing() wrapper in order to drop missing values in the mean() function.\nMoreover, dplyr also automatically arranges the results by the group, so the reproduce the results of dplyr, we need to add TidierData.@arrange() to the chain.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  island    bill_depth_mean\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Biscoe               15.9\n2 Dream                18.3\n3 Torgersen            18.4\n\n\n\n\n\n@chain penguins begin\n  @group_by(island) \n  @summarize(bill_depth_mean = mean(skipmissing(bill_depth_mm)))\n  @arrange(island)\nend\n\n3×2 DataFrame\n Row │ island     bill_depth_mean\n     │ String15   Float64\n─────┼────────────────────────────\n   1 │ Biscoe             15.8749\n   2 │ Dream              18.3444\n   3 │ Torgersen          18.4294\n\n\n\n\n\n\n\nMore complicated summaries by group\nTypically, you want to create multiple different summaries by groups. dplyr provides a lot of flexibility to create new variables on the fly, as does TidierData. For instance, we can pass expressions to them mean functions in order to create the share of female penguins per island in the summary statement.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(count = n(),\n            bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE),\n            flipper_length_median = median(flipper_length_mm, na.rm = TRUE),\n            body_mass_sd = sd(body_mass_g, na.rm = TRUE),\n            share_female = mean(sex == \"female\", na.rm = TRUE))\n\n# A tibble: 3 × 6\n  island   count bill_depth_mean flipper_length_median body_mass_sd share_female\n  &lt;fct&gt;    &lt;int&gt;           &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Biscoe     168            15.9                   214         783.        0.491\n2 Dream      124            18.3                   193         417.        0.496\n3 Torgers…    52            18.4                   191         445.        0.511\n\n\n\n\n\n@chain penguins begin\n  @group_by(island) \n  @summarize(count = n(),\n             bill_depth_mean = mean(skipmissing(bill_depth_mm)),\n             flipper_length_median = median(skipmissing(flipper_length_mm)),\n             body_mass_sd = std(skipmissing(body_mass_g)),\n             share_female = mean(skipmissing(sex == \"female\")))\n  @arrange(island)\nend\n\n3×6 DataFrame\n Row │ island     count  bill_depth_mean  flipper_length_median  body_mass_sd  ⋯\n     │ String15   Int64  Float64          Float64                Float64       ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Biscoe       168          15.8749                  214.0       782.856  ⋯\n   2 │ Dream        124          18.3444                  193.0       416.644\n   3 │ Torgersen     52          18.4294                  191.0       445.108\n                                                                1 column omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#conclusion",
    "href": "posts/dplyr-vs-tidierdata/index.html#conclusion",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Conclusion",
    "text": "Conclusion\nThis post highlights syntactic similarities and differences across R’s dplyr and Julia’s TidierData packages. The key difference is between pipes and chains: dplyr uses the pipe operator |&gt; to chain functions, while TidierData uses the @chain df begin ... end syntax for piping a value through a series of transformation expressions. Nonetheless, the similarities are remarkable and demonstrate the flexibility of Julia to seemingly replicate the NSE capabilities of R. If you want to play around with Julia or some of its packages, I can highly recommend to take a shortcut using TidierData."
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#footnotes",
    "href": "posts/dplyr-vs-tidierdata/index.html#footnotes",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the unifying principles of the tidyverse: https://design.tidyverse.org/unifying.html.↩︎\nBroadcasting expands singleton dimensions in array arguments to match the corresponding dimension in the other array without using extra memory.↩︎\nIn Julia, setting the number of rows to display for all DataFrames globally isn’t straightforward as there isn’t a direct global setting for this in the DataFrame package. ENV[\"COLUMNS\"] or ENV[\"LINES\"] control the display based on the size of your terminal.↩︎"
  },
  {
    "objectID": "posts/rapid-rag-prototyping/index.html",
    "href": "posts/rapid-rag-prototyping/index.html",
    "title": "Rapid RAG Prototyping",
    "section": "",
    "text": "One of the standout features of R is its ability to rapidly prototype algorithms and applications. In this blog post, I’ll show how the recently released ellmer package and DuckDB can be combined to build a large language model (LLM) enhanced with domain-specific information.\nA common challenge with LLMs is their inability to reference specific, up-to-date information beyond their training data. This is where retrieval-augmented generation (RAG) comes into play. RAG systems bridge this gap by integrating LLMs with external knowledge bases, enabling them to generate more accurate and contextually grounded responses.\nThe ellmer package in R simplifies working with LLMs by offering a seamless interface to multiple providers. It comes with built-in features like tool calling and structured data extraction, making it easy to extend LLM capabilities. On the other hand, DuckDB is a high-performance, embedded analytics database that runs directly within your application. Known for its ability to process large datasets efficiently, DuckDB can query diverse file formats and, with its vector search extension, becomes an excellent lightweight tool for implementing retrieval-based features.\nBy combining ellmer and DuckDB, you get a powerful stack for quickly prototyping LLM-powered applications. Whether you’re building a proof-of-concept or laying the groundwork for a more complex system, this setup offers the flexibility and performance to bring your ideas to life—fast."
  },
  {
    "objectID": "posts/rapid-rag-prototyping/index.html#response-without-augmentation",
    "href": "posts/rapid-rag-prototyping/index.html#response-without-augmentation",
    "title": "Rapid RAG Prototyping",
    "section": "Response without Augmentation",
    "text": "Response without Augmentation\n\nlibrary(httr2)\nlibrary(ellmer)\nlibrary(duckdb)\nlibrary(tibble)\n\nThroughout this post, I’m using OpenAI’s API. The integration is handled seamlessly through ellmer, which provides a clean interface for interacting with various LLM providers. As an example, we start by asking Chat what it knows about me:\n\nchat &lt;- chat_openai(model = \"gpt-4o-mini\")\nchat$chat(\"Who is Christoph Scheuch?\")\n\nAs of my last knowledge update in October 2021, there isn't widely recognized \ninformation about an individual named Christoph Scheuch in popular media, \nacademia, or notable public affairs. It's possible that he could be a private \nindividual or a professional in a specific field not covered in mainstream \nsources. If he has gained prominence after that date or if he is known in a \nspecific niche, I would not have that information. You may want to check the \nlatest news or specific professional networks for up-to-date information.\n\n\nToo bad, I’m not a widely recognized public figure, so Chat cannot say anything about me. This example perfectly illustrates a common limitation of LLMs - while they excel at general knowledge and patterns learned during training, they lack access to specific, current, or private information. This is where RAG becomes invaluable. By augmenting the model’s capabilities with our own knowledge base, we can help it provide accurate, contextual responses about topics it wasn’t originally trained on."
  },
  {
    "objectID": "posts/rapid-rag-prototyping/index.html#store-embeddings",
    "href": "posts/rapid-rag-prototyping/index.html#store-embeddings",
    "title": "Rapid RAG Prototyping",
    "section": "Store Embeddings",
    "text": "Store Embeddings\nEmbeddings are numerical representations of text that capture semantic meaning in a way that computers can process. They convert words and sentences into high-dimensional vectors where similar meanings result in similar vectors. This mathematical representation is crucial for implementing efficient information retrieval in RAG systems.\nI’m using the text-embedding-3-small model that is optimized for latency and costs. This model returns a vector of length 1536, striking a balance between computational efficiency and semantic representation quality. While larger models might offer marginally better performance, they often come with increased latency and cost. Feel free to experiment with other models based on your specific needs.\n\nget_embedding &lt;- function(\n  text,\n  model = \"text-embedding-3-small\",\n  api_key = Sys.getenv(\"OPENAI_API_KEY\")\n) {\n\n  base_url &lt;- \"https://api.openai.com/v1\"\n  req &lt;- request(base_url)\n  req &lt;- req_url_path_append(req, \"/embeddings\")\n  req &lt;- req_auth_bearer_token(req, api_key)\n  req &lt;- req_body_json(req, list(\n    input = as.list(text),\n    model = model\n  ))\n\n  resp &lt;- req_perform(req)\n  json &lt;- resp_body_json(resp)\n\n  embedding &lt;- as.vector(unlist(json$data[[1]]$embedding))\n\n  embedding\n}\n\nHere is an example that demonstrates how a simple question gets transformed into its vector representation. For brevity, I just print the first 5 numbers:\n\nembedding &lt;- get_embedding(\"Who is Christoph Scheuch?\")\nembedding[1:5]\n\n[1] -0.015694630 -0.026622834 -0.008347419  0.025807848 -0.036254470\n\n\nThese numbers obviously don’t mean anything to us humans. But through vector similarity, they are tremendously useful to find relevant content. When we later search for information, we’ll compare these vector representations to find semantically similar content, rather than relying on simple keyword matching. This allows our RAG system to understand and retrieve contextually relevant information even when the exact words don’t match.\nWhile there are many specialized vector databases available, DuckDB offers a lightweight yet powerful solution for implementing vector similarity search through its extensions system. The vss extension enables efficient similarity searches directly within DuckDB, making it an excellent choice for prototyping and smaller-scale applications.\nYou can create a local database and install and load the extension as follows:\n\ncon &lt;- dbConnect(duckdb(), \"database.duckdb\")\ndbSendQuery(con, \"INSTALL vss;\")\ndbSendQuery(con, \"LOAD vss;\")\n\nNext, we’ll create the foundation of our knowledge base - a table called docs that will store both the original text and its vector representation. The table schema reflects hence the two components of our RAG system.\n\ndbSendQuery(con, \"DROP TABLE IF EXISTS docs;\")\ndbSendQuery(con, \"CREATE TABLE docs (text TEXT, embedding FLOAT[1536]);\")\n\nThe array size of 1536 matches the dimensionality of our chosen embedding model, ensuring compatibility between the embeddings we generate and our storage solution.\nNext, we’ll create a set of documents with varying degrees of relevance to our original question. Let’s start with a comprehensive document about me:\n\ndoc_1 &lt;- c(\n  \"Who is Christoph Scheuch? Christoph Scheuch is an independent BI & Data Science consultant specializing in financial topics. He provides services and trainings related to data infrastructure, data analysis, machine learning, and general data science topics. Previously, he was the Head of AI, Director of Product, and Head of BI & Data Science at the social trading platform wikifolio.com. He is also the co-creator and maintainer of the open-source project Tidy Finance. In his free time, he occasionally designs shirts and mugs, available under the Tidy Swag brand.\"\n)\n\nNow we need a way to store these documents and their embeddings in our DuckDB database. Due to current limitations in the duckdb R package (see the issue here), we need to construct our SQL query manually. While this approach isn’t ideal for production systems, it serves our prototyping purposes well:\n\nstore_embedding &lt;- function(text) {\n  embedding &lt;- get_embedding(text)\n  embedding_sql &lt;- paste0(\"array_value(\", paste(embedding, collapse = \", \"), \")\")\n  query &lt;- sprintf(\"INSERT INTO docs VALUES ('%s', %s);\", text, embedding_sql)\n  result &lt;- dbExecute(con, query)\n}\nstore_embedding(doc_1)\n\nTo better understand the retrieval mechanism, let’s add documents with varying levels of relevance. First, a shorter, more focused document that contains only key information:\n\ndoc_2 &lt;- c(\n  \"Christoph Scheuch is an independent BI & Data Science consultant specializing in financial topics\"\n)\nstore_embedding(doc_2)\n\nFinally, we’ll add a control document that should be semantically distant from our query despite some surface-level similarities. I asked ChatGPT to come up with a complete nonesen about a person called “Christian Schuch”. Hopefully, this document is not relevant to the question about who I am.\n\ndoc_3 &lt;- c(\n  \"Christian Schuch is a renowned intergalactic cartographer, best known for mapping the uncharted regions of the Andromeda galaxy using only a compass, a kazoo, and an uncanny sense of direction. In 2017, he won the prestigious “Golden Platypus Award” for his groundbreaking research on the migratory patterns of space jellyfish. When he’s not busy decoding ancient alien snack recipes, Christian enjoys competitive yodeling in zero gravity and has a side hustle crafting bespoke hats for sentient cacti. His latest project involves teaching quantum physics to squirrels, aiming to bridge the gap between rodent intelligence and parallel universes.\"\n)\nstore_embedding(doc_3)\n\nNow that we have added some documents to the database, it is time to create an index. The index helps us to quickly retrieve relevant documents using the vector search feature of DuckDB. Without an index, finding similar vectors would require comparing our query vector against every single document vector - a process that becomes prohibitively slow as your document collection grows.\nIf you want to reuse the database in an app or share it with others, you need to persist the index. You can do so by enabling the currently experimental persistence feature (learn more here). Also, I’m creating an index using cosine similarity distance because it’s particularly well-suited for comparing semantic similarity between text embeddings. Cosine similarity measures the angle between vectors while ignoring their magnitude, making it effective for comparing texts of different lengths.\n\ndbSendQuery(con, \"SET hnsw_enable_experimental_persistence = true;\")\ndbSendQuery(con, \"DROP INDEX IF EXISTS hnsw_index;\")\ndbSendQuery(con, \"CREATE INDEX hnsw_index ON docs USING HNSW (embedding) WITH (metric = 'cosine');\")\n\nThe index type we’re using here is HNSW (Hierarchical Navigable Small World), which is a sophisticated algorithm for approximate nearest neighbor search. It creates a layered graph structure that allows for efficient navigation through the high-dimensional space of our embeddings. While it doesn’t guarantee finding the absolute nearest neighbors, it provides an excellent trade-off between search speed and accuracy, making it ideal for RAG applications."
  },
  {
    "objectID": "posts/rapid-rag-prototyping/index.html#retrieve-relevant-text",
    "href": "posts/rapid-rag-prototyping/index.html#retrieve-relevant-text",
    "title": "Rapid RAG Prototyping",
    "section": "Retrieve Relevant Text",
    "text": "Retrieve Relevant Text\nNow comes the crucial part of our RAG system - retrieving the most semantically relevant information based on user input. Let’s start by getting an embedding for our question:\n\nuser_input &lt;- \"Who is Christoph Scheuch?\"\nembedding &lt;- get_embedding(user_input)\n\nTo retrieve the relevant documents from the database, we use the similarity search feature. The query is designed to demonstrate how vector similarity works in practice. We set relatively permissive parameters - a minimum cosine-similarity of 0.1 and a limit of 3 documents - to see how our different test documents compare:\n\nembedding_sql &lt;- paste0(\"[\", paste(embedding, collapse = \",\"), \"]::FLOAT[1536]\")\nquery &lt;- sprintf(\n  \"SELECT array_cosine_similarity(embedding, %s) AS similarity, text FROM docs WHERE array_cosine_similarity(embedding, %s) &gt;= 0.1 ORDER BY array_cosine_similarity(embedding, %s) DESC LIMIT 3;\", embedding_sql, embedding_sql, embedding_sql\n)\ndbGetQuery(con, query) |&gt; \n  as_tibble()\n\n# A tibble: 3 × 2\n  similarity text                                                               \n       &lt;dbl&gt; &lt;chr&gt;                                                              \n1      0.717 Who is Christoph Scheuch? Christoph Scheuch is an independent BI &…\n2      0.658 Christoph Scheuch is an independent BI & Data Science consultant s…\n3      0.464 Christian Schuch is a renowned intergalactic cartographer, best kn…\n\n\nThe results reveal several interesting insights about our vector similarity search:\n\nThe comprehensive biography (doc_1) shows the highest similarity, as expected.\nThe shorter professional description (doc_2) also shows strong similarity.\nThe fictional story about “Christian Schuch” (doc_3) either shows much lower similarity despite containing a similar name.\n\nThis demonstrates that our embeddings are capturing semantic meaning rather than just matching keywords. For a production RAG system, we would want to be more selective. A typical approach is to both increase the similarity threshold (0.7 is typically a sensible value to start with) and limit the number of retrieved items (here I’m using only the most relevant document, so limit 1). These restrictions help ensure that only highly relevant information is used to augment the LLM’s response.\nLet’s encapsulate the retrieval logic in a dedicated function. This abstraction will make it easier to experiment with different parameters and integrate the retrieval mechanism into larger applications.\n\nget_relevant_text &lt;- function(text, min_similarity = 0.7, max_n = 1) {\n  embedding &lt;- get_embedding(text)\n  embedding_sql &lt;- paste0(\"[\", paste(embedding, collapse = \",\"), \"]::FLOAT[1536]\")\n  query &lt;- paste(\n    \"SELECT text\",\n    \"FROM docs\",\n    \"WHERE array_cosine_similarity(embedding, \", embedding_sql, \") &gt;= \", min_similarity,\n    \"ORDER BY array_cosine_similarity(embedding, \", embedding_sql, \") DESC\",\n    \"LIMIT \", max_n, \";\"\n  )\n  result &lt;- dbGetQuery(con, query) \n\n  result$text\n}\n\nThe function takes three parameters:\n\ntext: The input text to find relevant documents for.\nmin_similarity: The minimum cosine similarity threshold (defaulting to 0.7).\nmax_n: The maximum number of documents to retrieve (defaulting to 1)."
  },
  {
    "objectID": "posts/rapid-rag-prototyping/index.html#augment-response-generation",
    "href": "posts/rapid-rag-prototyping/index.html#augment-response-generation",
    "title": "Rapid RAG Prototyping",
    "section": "Augment Response Generation",
    "text": "Augment Response Generation\nPutting things together, we now can augment the original input by including relevant content. The augmentation process involves combining our retrieved context with clear instructions for the LLM:\n\nuser_input &lt;- \"Who is Christoph Scheuch?\"\nrelevant_text &lt;- get_relevant_text(user_input)\nuser_input_augmented &lt;- paste(\n  \"Use the information below to answer the subsequent question.\",\n  \"If the answer cannot be found, write 'I don't know.'\",\n  \"Info: \", relevant_text,\n  \"Question: \", user_input\n)\n\nNow let’s pass the augmented query to Chat and see whether it now gets the information about me right:\n\nchat$chat(user_input_augmented)\n\nChristoph Scheuch is an independent BI & Data Science consultant specializing \nin financial topics. He provides services and training related to data \ninfrastructure, data analysis, machine learning, and general data science \ntopics. Previously, he held positions as the Head of AI, Director of Product, \nand Head of BI & Data Science at the social trading platform wikifolio.com. He \nis also the co-creator and maintainer of the open-source project Tidy Finance. \nIn his free time, he occasionally designs shirts and mugs, which are available \nunder the Tidy Swag brand.\n\n\nLo and behold, Chat now gets it right! We’ve successfully transformed our LLM from having no knowledge about a specific topic to providing accurate, contextualized responses based on our provided information.\nThis example demonstrates the fundamental building blocks of a RAG system in R, but there are many ways to enhance and extend this framework. What’s your use case? How would you extend this framework?"
  },
  {
    "objectID": "posts/text-to-speech-with-google-ai/index.html",
    "href": "posts/text-to-speech-with-google-ai/index.html",
    "title": "Text-to-Speech with Goolge AI",
    "section": "",
    "text": "Lately, I’ve found myself increasingly drawn to audiobooks and became curious about how I could create one myself using AI. In this post, I’ll walk you through how to leverage Google’s powerful Text-to-Speech (TTS) API to transform written content into high-quality audio. As an example, we’ll use the first chapter of Franz Kafka’s The Metamorphosis and turn this classic piece of literature into an audiobook - step by step.\nBefore we dive into generating our audiobook, let’s set up the Python environment with the necessary libraries. We’ll use Google Cloud’s Text-to-Speech client to convert text into speech, along with some utility libraries for handling environment variables and audio processing.\nThe key packages we’ll need are:\nimport os\nimport time\nimport re\n\nfrom dotenv import load_dotenv\nfrom google.cloud import texttospeech\nfrom pydub import AudioSegment\n\nload_dotenv()"
  },
  {
    "objectID": "posts/text-to-speech-with-google-ai/index.html#prepare-text",
    "href": "posts/text-to-speech-with-google-ai/index.html#prepare-text",
    "title": "Text-to-Speech with Goolge AI",
    "section": "Prepare Text",
    "text": "Prepare Text\nTo begin, we need the text we want to convert into audio. I downloaded the first chapter of Franz Kafka’s The Metamorphosis from Project Gutenberg, which offers a vast collection of public domain books.\n\nwith open(\"metamorphosis_chapter1.txt\", \"r\", encoding=\"utf-8\") as file:\n    text = file.read()\n\ntext[:500]\n\n'One morning, when Gregor Samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermin. He lay on his\\narmour-like back, and if he lifted his head a little he could see his\\nbrown belly, slightly domed and divided by arches into stiff sections.\\nThe bedding was hardly able to cover it and seemed ready to slide off\\nany moment. His many legs, pitifully thin compared with the size of the\\nrest of him, waved about helplessly as he looked.\\n\\n“What’s happened to me?” he th'\n\n\nBefore moving forward, skim through the loaded text to ensure there aren’t any unwanted headers, footers, or formatting issues (like excessive line breaks) that might affect the audio quality or API compatibility.\nOne of the challenges in working with text-to-speech APIs is handling large chunks of text. Google’s API has limitations on input size, so we need to split our text intelligently. The following approach breaks the text into manageable paragraphs while preserving the natural flow of the narrative. It splits text on double line breaks (paragraphs), ensures each chunk stays within a 2000-byte limit, and maintains paragraph integrity where possible.\n\ndef split_text_by_paragraphs(\n    text: str, \n    max_bytes: int = 2000\n) -&gt; list[str]:\n    \"\"\"\n    Split the text into chunks based on paragraphs (empty lines) and ensure each chunk is within the byte limit.\n\n    Args:\n        text (str): The input text to split.\n        max_bytes (int): Maximum byte size for each chunk.\n\n    Returns:\n        list[str]: List of text chunks.\n    \"\"\"\n    paragraphs = text.split(\"\\n\\n\")\n    chunks = []\n    current_chunk = \"\"\n    current_bytes = 0\n\n    for paragraph in paragraphs:\n        paragraph_bytes = len(paragraph.encode(\"utf-8\"))\n        if current_bytes + paragraph_bytes + 1 &gt; max_bytes:\n            chunks.append(current_chunk.strip())\n            current_chunk = paragraph\n            current_bytes = paragraph_bytes\n        else:\n            if current_chunk:\n                current_chunk += \"\\n\\n\" + paragraph\n            else:\n                current_chunk = paragraph\n            current_bytes += paragraph_bytes + 2 \n\n    if current_chunk:\n        chunks.append(current_chunk.strip())\n\n    return chunks\n\nLet’s create paragraphs and inspect the first one:\n\nparagraphs = split_text_by_paragraphs(text)\nlen(paragraphs)\nparagraphs[0]\n\n'One morning, when Gregor Samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermin. He lay on his\\narmour-like back, and if he lifted his head a little he could see his\\nbrown belly, slightly domed and divided by arches into stiff sections.\\nThe bedding was hardly able to cover it and seemed ready to slide off\\nany moment. His many legs, pitifully thin compared with the size of the\\nrest of him, waved about helplessly as he looked.\\n\\n“What’s happened to me?” he thought. It wasn’t a dream. His room, a\\nproper human room although a little too small, lay peacefully between\\nits four familiar walls. A collection of textile samples lay spread out\\non the table—Samsa was a travelling salesman—and above it there hung a\\npicture that he had recently cut out of an illustrated magazine and\\nhoused in a nice, gilded frame. It showed a lady fitted out with a fur\\nhat and fur boa who sat upright, raising a heavy fur muff that covered\\nthe whole of her lower arm towards the viewer.\\n\\nGregor then turned to look out the window at the dull weather. Drops of\\nrain could be heard hitting the pane, which made him feel quite sad.\\n“How about if I sleep a little bit longer and forget all this\\nnonsense”, he thought, but that was something he was unable to do\\nbecause he was used to sleeping on his right, and in his present state\\ncouldn’t get into that position. However hard he threw himself onto his\\nright, he always rolled back to where he was. He must have tried it a\\nhundred times, shut his eyes so that he wouldn’t have to look at the\\nfloundering legs, and only stopped when he began to feel a mild, dull\\npain there that he had never felt before.'\n\n\nRaw text often contains formatting that doesn’t translate well to speech or even triggers error codes in the API. The following clean_chunk() function emerged in another project that I was working on and prepares the text by: converting single line breaks to spaces, removing double periods, cleaning up special characters and quotation marks, eliminating parenthetical content, and handling Unicode and control characters. This cleaning process is crucial for producing natural-sounding speech without awkward pauses or artifacts. Note that the function below is not generally applicable and needs to be adapted to your specific context.\n\ndef clean_chunk(chunk) -&gt; str: \n    \"\"\"\n    Cleans and formats a text chunk by removing unwanted characters, normalizing whitespace, and improving readability.\n\n    Args:\n        chunk (str): The text chunk to be cleaned.\n\n    Returns:\n        str: The cleaned and formatted text.\n    \"\"\"\n    cleaned_chunk = re.sub(r'(?&lt;!\\n)\\n(?!\\n)', ' ', chunk) \n    cleaned_chunk = re.sub(r'\\n{2,}', '. ', cleaned_chunk)\n    cleaned_chunk = cleaned_chunk.replace(\"..\", \".\").replace(\"»\", \"\").replace(\"«\", \"\")\n    cleaned_chunk = re.sub(r'\\s-\\s+', '', cleaned_chunk)\n    cleaned_chunk = re.sub(r'\\([^)]*\\)', '', cleaned_chunk).strip()\n    cleaned_chunk = cleaned_chunk.replace(\"\\u2028\", \" \")\n    cleaned_chunk = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', ' ', cleaned_chunk)\n\n    return cleaned_chunk\n\nclean_chunk(paragraphs[0])\n\n'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked. “What’s happened to me?” he thought. It wasn’t a dream. His room, a proper human room although a little too small, lay peacefully between its four familiar walls. A collection of textile samples lay spread out on the table—Samsa was a travelling salesman—and above it there hung a picture that he had recently cut out of an illustrated magazine and housed in a nice, gilded frame. It showed a lady fitted out with a fur hat and fur boa who sat upright, raising a heavy fur muff that covered the whole of her lower arm towards the viewer. Gregor then turned to look out the window at the dull weather. Drops of rain could be heard hitting the pane, which made him feel quite sad. “How about if I sleep a little bit longer and forget all this nonsense”, he thought, but that was something he was unable to do because he was used to sleeping on his right, and in his present state couldn’t get into that position. However hard he threw himself onto his right, he always rolled back to where he was. He must have tried it a hundred times, shut his eyes so that he wouldn’t have to look at the floundering legs, and only stopped when he began to feel a mild, dull pain there that he had never felt before.'"
  },
  {
    "objectID": "posts/text-to-speech-with-google-ai/index.html#convert-text-to-speech",
    "href": "posts/text-to-speech-with-google-ai/index.html#convert-text-to-speech",
    "title": "Text-to-Speech with Goolge AI",
    "section": "Convert Text to Speech",
    "text": "Convert Text to Speech\nThe heart of our solution lies in the text_to_speech() function, which interfaces with Google’s API. I’ve configured it with specific parameters to create a more engaging listening experience: aAdjusting pitch (-20) for a more natural sound and setting to a comfortable speaking rate (0.8). The function includes error handling and retry logic, making it robust enough for processing longer texts like books.\n\ndef text_to_speech(\n    text: str, \n    output_file: str, \n    model: str = \"en-US-Studio-Q\",\n    pitch: float = -20,\n    speaking_rate: float = 0.8,\n    max_retries: int = 5, \n    base_delay: float = 1.0\n):\n    \"\"\"\n    Convert text to speech and save the output as an MP3 file, with exponential backoff for retries.\n    \n    Args:\n        text (str): The text to convert to speech.\n        output_file (str): The path to save the output MP3 file.\n        model (str): The model used.\n        pitch (float): The pitch parameter of the model.\n        speaking_rate (float): The speaking_rate parameter of the model. \n        max_retries (int): Maximum number of retries on failure.\n        base_delay (float): Base delay in seconds for exponential backoff.\n    \"\"\"\n    client = texttospeech.TextToSpeechClient()\n\n    synthesis_input = texttospeech.SynthesisInput(text=text)\n\n    voice = texttospeech.VoiceSelectionParams(\n        language_code=model[:5],\n        name=model\n    )\n\n    audio_config = texttospeech.AudioConfig(\n        audio_encoding=texttospeech.AudioEncoding.MP3,\n        pitch=pitch,\n        speaking_rate=speaking_rate\n    )\n\n    retries = 0\n    while retries &lt; max_retries:\n        try:\n            response = client.synthesize_speech(\n                input=synthesis_input,\n                voice=voice,\n                audio_config=audio_config\n            )\n            with open(output_file, \"wb\") as out:\n                out.write(response.audio_content)\n                print(f\"Audio content written to file: {output_file}\")\n            return\n        except Exception as e:\n            if hasattr(e, 'code') and e.code == 500:\n                retries += 1\n                delay = base_delay * (2 ** (retries - 1))\n                print(f\"Error 500: Retrying in {delay:.2f} seconds... (Attempt {retries}/{max_retries})\")\n                time.sleep(delay)\n            else:\n                print(f\"Non-retryable error: {e}\")\n                raise\n\n    print(f\"Failed to process text after {max_retries} retries.\")\n    raise RuntimeError(\"Max retries reached.\")\n\nTo create an audio of the first paragraph of the example chapter and store it locally, you just run:\n\ntext_to_speech(paragraphs[0], \"out/part1.mp3\")"
  },
  {
    "objectID": "posts/text-to-speech-with-google-ai/index.html#process-text",
    "href": "posts/text-to-speech-with-google-ai/index.html#process-text",
    "title": "Text-to-Speech with Goolge AI",
    "section": "Process Text",
    "text": "Process Text\nNow that we have the text split into manageable chunks, cleaned them for better text-to-speech conversion, and created a function to interface with Google’s API, it’s time to process the parapgrahs and generate MP3 files. The process_text() function puts the pieces from above together and stores MP3 files for each paragraph separately.\n\ndef process_text(text: list, output_folder: str):\n    \"\"\"\n    Process a text, split it into chunks, and generate MP3 files in the output folder.\n\n    Args:\n        text (str): A list of file paths to text files.\n        output_folder (str): The folder to save the generated MP3 files.\n    \"\"\"\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    text_chunks = split_text_by_paragraphs(text)\n\n    for i, chunk in enumerate(text_chunks):\n        output_file_name = f\"part{i+1}.mp3\"\n        output_file_path = os.path.join(output_folder, output_file_name)\n                \n        cleaned_chunk = clean_chunk(chunk)\n\n        text_to_speech(cleaned_chunk, output_file_path)\n        time.sleep(1)\n\nTo create audio files for each paragraph separately, just run:\n\nprocess_text(text, \"out\")"
  },
  {
    "objectID": "posts/text-to-speech-with-google-ai/index.html#combine-individual-segments",
    "href": "posts/text-to-speech-with-google-ai/index.html#combine-individual-segments",
    "title": "Text-to-Speech with Goolge AI",
    "section": "Combine Individual Segments",
    "text": "Combine Individual Segments\nAfter converting individual text chunks to speech, we need to stitch them together into a cohesive audiobook. This final step uses pydub’s AudioSegment to combine the individual MP3 files seamlessly, ensuring smooth transitions between segments. Note that you might have to run pip install audioop-lts to make this work.\n\ninput_dir = \"out\"\noutput_dir = \"out\"\n\ndef get_number(filename):\n    return int(filename.replace('part', '').replace('.mp3', ''))\n\nmp3_files = sorted(\n    [file for file in os.listdir(input_dir) if file.endswith(\".mp3\")],\n    key=get_number\n)\n\ncombined_audio = None\nfor file in mp3_files:\n    audio = AudioSegment.from_file(os.path.join(input_dir, file))\n    combined_audio = audio if combined_audio is None else combined_audio + audio\ncombined_audio.export(\"out/chapter1.mp3\", format=\"mp3\", bitrate=\"320k\")\n\nYou can listen to the first chapter of Kafka’s The Metamorphosis generated with the Google TTS API here:\n\nCreating audiobooks with Google’s Text-to-Speech API is surprisingly straightforward. While the output may not match the nuanced performance of human narrators, it provides a quick and effective way to convert text content into listenable audio. This approach is particularly valuable for, e.g., rapid prototyping of audio content, or creating accessible versions of text materials. When using this system, keep the API costs and rate limits, as well as the importance of proper text processing in mind."
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html",
    "href": "posts/dplyr-vs-ibis/index.html",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "",
    "text": "There are a myriad of options to perform essential data manipulation tasks in R and Python (see, for instance, my other posts on dplyr vs pandas and dplyr vs polars). However, if we want to do tidy data science in R, there is a clear forerunner: dplyr. In the world of Python, ibis has been around since 2015 but recently gained traction due to its appealing flexibility with respect to data backends. In this blog post, I illustrate their syntactic similarities and highlight differences between these two packages that emerge for a few key tasks.\nBefore we dive into the comparison, a short introduction to the packages: the dplyr package in R allows users to refer to columns without quotation marks due to its implementation of non-standard evaluation (NSE). NSE is a programming technique used in R that allows functions to capture the expressions passed to them as arguments, rather than just the values of those arguments. The primary goal of NSE in the context of dplyr is to create a more user-friendly and intuitive syntax. This makes data manipulation tasks more straightforward and aligns with the general philosophy of the tidyverse to make data science faster, easier, and more fun.1\nibis is a Python library that provides a lightweight and universal interface for data wrangling using many different data backends. The core idea behind ibis is to provide Python users with a familiar pandas-like syntax while allowing them to work with larger datasets that don’t fit into memory. As you see in the post below, the ibis syntax can be surprisingly closer to dplyr than to the original idea of resembling pandas. In addition, ibis builds an expression tree as you write code. This tree is then translated into the native query language of the target data source, be it SQL or something else, and executed remotely (similar to the dbplyr package in R). This approach ensures that only the final results are loaded into Python, significantly reducing memory overhead."
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#loading-packages-and-data",
    "href": "posts/dplyr-vs-ibis/index.html#loading-packages-and-data",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Loading packages and data",
    "text": "Loading packages and data\nWe start by loading the main packages of interest and the popular palmerpenguins package that exists for both R and Python. We then use the penguins data frame as the data to compare all functions and methods below. Note that we also enable the interactive mode in ibis to limit the print output of ibis data frames to 10 rows.\n\n\n\n\n\n\nibis-framework vs ibis\n\n\n\nNote that the ibis-framework package is not the same as the ibis package in PyPI. These two libraries cannot coexist in the same Python environment, as they are both imported with the ibis module name. So be careful to install the correct ibis-framework package via: pip install 'ibis-framework[duckdb]'\n\n\n\ndplyribis\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins &lt;- palmerpenguins::penguins\n\n\n\n\nimport ibis\nimport ibis.selectors as s\nfrom ibis import _\nfrom palmerpenguins import load_penguins\n\nibis.options.interactive = True\n\npenguins = ibis.memtable(load_penguins(), name = \"penguins\")"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#work-with-rows",
    "href": "posts/dplyr-vs-ibis/index.html#work-with-rows",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Work with rows",
    "text": "Work with rows\n\nFilter rows\nFiltering rows works very similarly for both packages, they even have the same function names: dplyr::filter() and ibis.filter(). To select columns in ibis, you need the ibis._ selector. Note that you have to provide a dictionary to ibis.filter() in case you want to have multiple conditions.\n\ndplyribis\n\n\n\npenguins |&gt; \n  filter(species == \"Adelie\" & \n           island %in% c(\"Biscoe\", \"Dream\"))\n\n# A tibble: 100 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 90 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .filter([\n    _.species == \"Adelie\", \n    _.island.isin([\"Biscoe\", \"Dream\"])\n  ]) \n)\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ … │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Biscoe │           37.8 │          18.3 │             174.0 │ … │\n│ Adelie  │ Biscoe │           37.7 │          18.7 │             180.0 │ … │\n│ Adelie  │ Biscoe │           35.9 │          19.2 │             189.0 │ … │\n│ Adelie  │ Biscoe │           38.2 │          18.1 │             185.0 │ … │\n│ Adelie  │ Biscoe │           38.8 │          17.2 │             180.0 │ … │\n│ Adelie  │ Biscoe │           35.3 │          18.9 │             187.0 │ … │\n│ Adelie  │ Biscoe │           40.6 │          18.6 │             183.0 │ … │\n│ Adelie  │ Biscoe │           40.5 │          17.9 │             187.0 │ … │\n│ Adelie  │ Biscoe │           37.9 │          18.6 │             172.0 │ … │\n│ Adelie  │ Biscoe │           40.5 │          18.9 │             180.0 │ … │\n│ …       │ …      │              … │             … │                 … │ … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴───┘\n\n\n\n\n\n\n\nSlice rows\ndplyr::slice() takes integers with row numbers as inputs, so you can use ranges and arbitrary vectors of integers. ibis.limit() only takes the number of rows to slice and the number of rows to skip as inputs. For instance, to the the same result of slicing rows 10 to 20, the code looks as follows (note that indexing starts at 0 in Python, while it starts at 1 in R):\n\ndplyribis\n\n\n\npenguins |&gt; \n  slice(10:20)\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           42            20.2               190        4250\n 2 Adelie  Torgersen           37.8          17.1               186        3300\n 3 Adelie  Torgersen           37.8          17.3               180        3700\n 4 Adelie  Torgersen           41.1          17.6               182        3200\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           36.6          17.8               185        3700\n 8 Adelie  Torgersen           38.7          19                 195        3450\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           34.4          18.4               184        3325\n11 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .limit(11, offset = 9) \n)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           42.0 │          20.2 │             190.0 │ … │\n│ Adelie  │ Torgersen │           37.8 │          17.1 │             186.0 │ … │\n│ Adelie  │ Torgersen │           37.8 │          17.3 │             180.0 │ … │\n│ Adelie  │ Torgersen │           41.1 │          17.6 │             182.0 │ … │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │             191.0 │ … │\n│ Adelie  │ Torgersen │           34.6 │          21.1 │             198.0 │ … │\n│ Adelie  │ Torgersen │           36.6 │          17.8 │             185.0 │ … │\n│ Adelie  │ Torgersen │           38.7 │          19.0 │             195.0 │ … │\n│ Adelie  │ Torgersen │           42.5 │          20.7 │             197.0 │ … │\n│ Adelie  │ Torgersen │           34.4 │          18.4 │             184.0 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\n\n\n\n\n\n\n\nArrange rows\nTo orders the rows of a data frame by the values of selected columns, we have dplyr::arrange() and ibis.order_by(). Both approaches arrange rows in an an ascending order and puts missing values last. Again, you need to provide a dictionary to ibis.order_by().\n\ndplyribis\n\n\n\npenguins |&gt; \n  arrange(island, desc(bill_length_mm))\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .order_by([_.island, _.bill_length_mm.desc()])\n)\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ … │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Gentoo  │ Biscoe │           59.6 │          17.0 │             230.0 │ … │\n│ Gentoo  │ Biscoe │           55.9 │          17.0 │             228.0 │ … │\n│ Gentoo  │ Biscoe │           55.1 │          16.0 │             230.0 │ … │\n│ Gentoo  │ Biscoe │           54.3 │          15.7 │             231.0 │ … │\n│ Gentoo  │ Biscoe │           53.4 │          15.8 │             219.0 │ … │\n│ Gentoo  │ Biscoe │           52.5 │          15.6 │             221.0 │ … │\n│ Gentoo  │ Biscoe │           52.2 │          17.1 │             228.0 │ … │\n│ Gentoo  │ Biscoe │           52.1 │          17.0 │             230.0 │ … │\n│ Gentoo  │ Biscoe │           51.5 │          16.3 │             230.0 │ … │\n│ Gentoo  │ Biscoe │           51.3 │          14.2 │             218.0 │ … │\n│ …       │ …      │              … │             … │                 … │ … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴───┘"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#work-with-columns",
    "href": "posts/dplyr-vs-ibis/index.html#work-with-columns",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Work with columns",
    "text": "Work with columns\n\nSelect columns\nSelecting a subset of columns works essentially the same for both and dplyr::select() and ibis.select() even have the same name. Note that you don’t have to use ibis._ but can also just pass strings in the ibis.select() method.\n\ndplyribis\n\n\n\npenguins |&gt; \n  select(bill_length_mm, sex)\n\n# A tibble: 344 × 2\n   bill_length_mm sex   \n            &lt;dbl&gt; &lt;fct&gt; \n 1           39.1 male  \n 2           39.5 female\n 3           40.3 female\n 4           NA   &lt;NA&gt;  \n 5           36.7 female\n 6           39.3 male  \n 7           38.9 female\n 8           39.2 male  \n 9           34.1 &lt;NA&gt;  \n10           42   &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n\n\n(penguins\n  .select(_.bill_length_mm, _.sex)\n)\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n┃ bill_length_mm ┃ sex    ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n│ float64        │ string │\n├────────────────┼────────┤\n│           39.1 │ male   │\n│           39.5 │ female │\n│           40.3 │ female │\n│           NULL │ NULL   │\n│           36.7 │ female │\n│           39.3 │ male   │\n│           38.9 │ female │\n│           39.2 │ male   │\n│           34.1 │ NULL   │\n│           42.0 │ NULL   │\n│              … │ …      │\n└────────────────┴────────┘\n\n\n\n\n\n\n\nRename columns\nRenaming columns also works very similarly with the major difference that ibis.rename() does not accept the column selector ibis._ on the right-hand side, while dplyr::rename() takes variable names via the usual NSE.\n\ndplyribis\n\n\n\npenguins |&gt; \n  rename(bill_length = bill_length_mm,\n         bill_depth = bill_depth_mm)\n\n# A tibble: 344 × 8\n   species island    bill_length bill_depth flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen        39.1       18.7               181        3750 male  \n 2 Adelie  Torgersen        39.5       17.4               186        3800 female\n 3 Adelie  Torgersen        40.3       18                 195        3250 female\n 4 Adelie  Torgersen        NA         NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen        36.7       19.3               193        3450 female\n 6 Adelie  Torgersen        39.3       20.6               190        3650 male  \n 7 Adelie  Torgersen        38.9       17.8               181        3625 female\n 8 Adelie  Torgersen        39.2       19.6               195        4675 male  \n 9 Adelie  Torgersen        34.1       18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen        42         20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .rename(bill_length = \"bill_length_mm\", \n          bill_depth = \"bill_depth_mm\")\n)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length ┃ bill_depth ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64     │ float64    │ float64           │ … │\n├─────────┼───────────┼─────────────┼────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │        39.1 │       18.7 │             181.0 │ … │\n│ Adelie  │ Torgersen │        39.5 │       17.4 │             186.0 │ … │\n│ Adelie  │ Torgersen │        40.3 │       18.0 │             195.0 │ … │\n│ Adelie  │ Torgersen │        NULL │       NULL │              NULL │ … │\n│ Adelie  │ Torgersen │        36.7 │       19.3 │             193.0 │ … │\n│ Adelie  │ Torgersen │        39.3 │       20.6 │             190.0 │ … │\n│ Adelie  │ Torgersen │        38.9 │       17.8 │             181.0 │ … │\n│ Adelie  │ Torgersen │        39.2 │       19.6 │             195.0 │ … │\n│ Adelie  │ Torgersen │        34.1 │       18.1 │             193.0 │ … │\n│ Adelie  │ Torgersen │        42.0 │       20.2 │             190.0 │ … │\n│ …       │ …         │           … │          … │                 … │ … │\n└─────────┴───────────┴─────────────┴────────────┴───────────────────┴───┘\n\n\n\n\n\n\n\nMutate columns\nTransforming existing columns or creating new ones is an essential part of data analysis. dplyr::mutate() and ibis.mutate() are the work horses for these tasks. A big difference between dplyr::mutate() and ibis.mutate() is that in the latter you have to chain separate mutate calls together when you reference newly-created columns in the same mutate whereas in dplyr, you can put them all in the same call.\n\ndplyribis\n\n\n\npenguins |&gt; \n  mutate(ones = 1,\n         bill_length = bill_length_mm / 10,\n         bill_length_squared = bill_length^2) |&gt; \n  select(ones, bill_length_mm, bill_length, bill_length_squared)\n\n# A tibble: 344 × 4\n    ones bill_length_mm bill_length bill_length_squared\n   &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1     1           39.1        3.91                15.3\n 2     1           39.5        3.95                15.6\n 3     1           40.3        4.03                16.2\n 4     1           NA         NA                   NA  \n 5     1           36.7        3.67                13.5\n 6     1           39.3        3.93                15.4\n 7     1           38.9        3.89                15.1\n 8     1           39.2        3.92                15.4\n 9     1           34.1        3.41                11.6\n10     1           42          4.2                 17.6\n# ℹ 334 more rows\n\n\n\n\n\n(penguins \n  .mutate(ones = 1, \n          bill_length = _.bill_length_mm / 10)\n  .mutate(bill_length_squared = _.bill_length**2)\n  .select(_.ones, _.bill_length_mm, _.bill_length, _.bill_length_squared)\n)\n\n┏━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ ones ┃ bill_length_mm ┃ bill_length ┃ bill_length_squared ┃\n┡━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ int8 │ float64        │ float64     │ float64             │\n├──────┼────────────────┼─────────────┼─────────────────────┤\n│    1 │           39.1 │        3.91 │             15.2881 │\n│    1 │           39.5 │        3.95 │             15.6025 │\n│    1 │           40.3 │        4.03 │             16.2409 │\n│    1 │           NULL │        NULL │                NULL │\n│    1 │           36.7 │        3.67 │             13.4689 │\n│    1 │           39.3 │        3.93 │             15.4449 │\n│    1 │           38.9 │        3.89 │             15.1321 │\n│    1 │           39.2 │        3.92 │             15.3664 │\n│    1 │           34.1 │        3.41 │             11.6281 │\n│    1 │           42.0 │        4.20 │             17.6400 │\n│    … │              … │           … │                   … │\n└──────┴────────────────┴─────────────┴─────────────────────┘\n\n\n\n\n\n\n\nRelocate columns\ndplyr::relocate() provides options to change the positions of columns in a data frame, using the same syntax as dplyr::select(). In addition, there are the options .after and .before to provide users with additional shortcuts.\nThe recommended way to relocate columns in ibis is to use the ibis.select() method, but there are no options as in dplyr::relocate(). In fact, the safest way to consistently get the correct order of columns is to explicitly specify them.\n\ndplyribis\n\n\n\npenguins |&gt; \n  relocate(c(species, bill_length_mm), .before = sex)\n\n# A tibble: 344 × 8\n   island    bill_depth_mm flipper_length_mm body_mass_g species bill_length_mm\n   &lt;fct&gt;             &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1 Torgersen          18.7               181        3750 Adelie            39.1\n 2 Torgersen          17.4               186        3800 Adelie            39.5\n 3 Torgersen          18                 195        3250 Adelie            40.3\n 4 Torgersen          NA                  NA          NA Adelie            NA  \n 5 Torgersen          19.3               193        3450 Adelie            36.7\n 6 Torgersen          20.6               190        3650 Adelie            39.3\n 7 Torgersen          17.8               181        3625 Adelie            38.9\n 8 Torgersen          19.6               195        4675 Adelie            39.2\n 9 Torgersen          18.1               193        3475 Adelie            34.1\n10 Torgersen          20.2               190        4250 Adelie            42  \n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .select(_.island, _.bill_depth_mm, _.flipper_length_mm, _.body_mass_g, \n          _.species, _.bill_length_mm, _.sex)\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━┳━━━┓\n┃ island    ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ species ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━╇━━━┩\n│ string    │ float64       │ float64           │ float64     │ string  │ … │\n├───────────┼───────────────┼───────────────────┼─────────────┼─────────┼───┤\n│ Torgersen │          18.7 │             181.0 │      3750.0 │ Adelie  │ … │\n│ Torgersen │          17.4 │             186.0 │      3800.0 │ Adelie  │ … │\n│ Torgersen │          18.0 │             195.0 │      3250.0 │ Adelie  │ … │\n│ Torgersen │          NULL │              NULL │        NULL │ Adelie  │ … │\n│ Torgersen │          19.3 │             193.0 │      3450.0 │ Adelie  │ … │\n│ Torgersen │          20.6 │             190.0 │      3650.0 │ Adelie  │ … │\n│ Torgersen │          17.8 │             181.0 │      3625.0 │ Adelie  │ … │\n│ Torgersen │          19.6 │             195.0 │      4675.0 │ Adelie  │ … │\n│ Torgersen │          18.1 │             193.0 │      3475.0 │ Adelie  │ … │\n│ Torgersen │          20.2 │             190.0 │      4250.0 │ Adelie  │ … │\n│ …         │             … │                 … │           … │ …       │ … │\n└───────────┴───────────────┴───────────────────┴─────────────┴─────────┴───┘"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#work-with-groups-of-rows",
    "href": "posts/dplyr-vs-ibis/index.html#work-with-groups-of-rows",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Work with groups of rows",
    "text": "Work with groups of rows\n\nSimple summaries by group\nLet’s suppose we want to compute summaries by groups such as means or medians. Both packages are very similar again: on the R side you have dplyr::group_by() and dplyr::summarize(), while on the Python side you have ibis.group_by() and ibis.aggregate().\nNote that dplyr::group_by() also automatically arranges the results by the group, so the reproduce the results of dplyr, we need to add ibis.order_by() to the chain.\n\ndplyribis\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  island    bill_depth_mean\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Biscoe               15.9\n2 Dream                18.3\n3 Torgersen            18.4\n\n\n\n\n\n(penguins\n  .group_by(\"island\")\n  .aggregate(bill_depth_mean = _.bill_depth_mm.mean())\n  .order_by(\"island\")\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ island    ┃ bill_depth_mean ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ float64         │\n├───────────┼─────────────────┤\n│ Biscoe    │       15.874850 │\n│ Dream     │       18.344355 │\n│ Torgersen │       18.429412 │\n└───────────┴─────────────────┘\n\n\n\n\n\n\n\nMore complicated summaries by group\nTypically, you want to create multiple different summaries by groups. dplyr provides a lot of flexibility to create new variables on the fly, as does ibis. For instance, we can pass expressions to them mean functions in order to create the share of female penguins per island in the summary statement.\n\ndplyribis\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(\n    count = n(),\n    bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE),\n    flipper_length_median = median(flipper_length_mm, na.rm = TRUE),\n    body_mass_sd = sd(body_mass_g, na.rm = TRUE),\n    share_female = mean(sex == \"female\", na.rm = TRUE)\n  )\n\n# A tibble: 3 × 6\n  island   count bill_depth_mean flipper_length_median body_mass_sd share_female\n  &lt;fct&gt;    &lt;int&gt;           &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Biscoe     168            15.9                   214         783.        0.491\n2 Dream      124            18.3                   193         417.        0.496\n3 Torgers…    52            18.4                   191         445.        0.511\n\n\n\n\n\n(penguins\n  .group_by(\"island\")\n  .aggregate(\n    count = _.count(),\n    bill_depth_mean = _.bill_depth_mm.mean(),\n    flipper_length_median = _.flipper_length_mm.median(),\n    body_mass_sd = _.body_mass_g.std(),\n    share_female = (_.sex == \"female\").mean()\n  )\n  .order_by(\"island\")\n)\n\n┏━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━┓\n┃ island    ┃ count ┃ bill_depth_mean ┃ flipper_length_medi… ┃ body_mass_sd ┃  ┃\n┡━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━┩\n│ string    │ int64 │ float64         │ float64              │ float64      │  │\n├───────────┼───────┼─────────────────┼──────────────────────┼──────────────┼──┤\n│ Biscoe    │   168 │       15.874850 │                214.0 │   782.855743 │  │\n│ Dream     │   124 │       18.344355 │                193.0 │   416.644112 │  │\n│ Torgersen │    52 │       18.429412 │                191.0 │   445.107940 │  │\n└───────────┴───────┴─────────────────┴──────────────────────┴──────────────┴──┘"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#conclusion",
    "href": "posts/dplyr-vs-ibis/index.html#conclusion",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Conclusion",
    "text": "Conclusion\nThis post highlights syntactic similarities and differences across R’s dplyr and Python’s ibis packages. Two key points emerge: (i) dplyr heavily relies on NSE to enable a syntax that refrains from using strings and column selectors, something that is not possible in Python; (ii) the syntax is remarkably similar across both packages. I want to close this post by emphasizing that both languages and packages have their own merits and I won’t strictly recommend one over the other. However, I definitely prefer the print output of dplyr to ibis because the latter is silent about additional columns of the underlying data. I’m a big fan of the concise data printing capabilities that are part of dplyr."
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#footnotes",
    "href": "posts/dplyr-vs-ibis/index.html#footnotes",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the unifying principles of the tidyverse: https://design.tidyverse.org/unifying.html.↩︎"
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html",
    "href": "posts/tidy-collaborative-filtering/index.html",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "",
    "text": "Recommender systems are a key component of our digital lifes, ranging from e-commerce, online advertisements, movie recommendations, or more generally all kinds of product recommendations. A recommender system aims to efficiently deliver personalized content to users based on a large pool of potentially relevant information. In this blog post, I illustrate the concept of recommender systems by building a simple stock recommendation tool that relies on publicly available portfolios from the social trading platform wikifolio.com. The resulting recommender proposes stocks to investors who already have their own portfolios and look for new investment opportunities. The underlying assumption is that the wikifolio traders hold stock portfolios that can provide meaningful inspiration for other investors. The resulting stock recommendations of course do not constitute any investment advice and rather serve an illustrative purpose.\nwikifolio.com is the leading social trading platform in Europe, where anyone can publish and monetize their trading strategies through virtual portfolios, which are called wikifolios. The community of wikifolio traders includes full time investors and successful entrepreneurs, as well as experts from different sectors, portfolio managers, and editors of financial magazines. All traders share their trading ideas through fully transparent wikifolios. The wikifolios are easy to track and replicate, by investing in the corresponding, collateralized index certificate. As of writing, there are more than 30k published wikifolios of more than 9k traders, indicating a large diversity of available portfolios for training our recommender.\nThere are essentially three types of recommender models: recommenders via collaborative filtering, recommenders via content-based filtering, and hybrid recommenders (that mix the first two). In this blog post, I focus on the collaborative filtering approach as it requires no information other than portfolios and can provide fairly high precision with little complexity. Nonetheless, I first briefly describe the recommender approaches and refer to Ricci et al. (2011)1 for a comprehensive exposition."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#a-primer-on-recommender-systems",
    "href": "posts/tidy-collaborative-filtering/index.html#a-primer-on-recommender-systems",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "A Primer on Recommender Systems",
    "text": "A Primer on Recommender Systems\n\nCollaborative Filtering\nIn collaborative filtering, recommendations are based on past user interactions and items to produce new recommendations. The central notion is that past user-item interactions are sufficient to detect similar users or items. Broadly speaking, there are two sub-classes of collaborative filtering: the memory-based approach, which essentially searches nearest neighbors based on recorded transactions and is hence model-free, and the model-based approach, where new representations of users and items are built based on some generative pre-estimated model. Theoretically, the memory-based approach has a low bias (since no latent model is assumed) but a high variance (since the recommendations change a lot in the nearest neighbor search). The model-based approach relies on a trained interaction model. It has a relatively higher bias but a lower variance, i.e., recommendations are more stable since they come from a model.\nAdvantages of collaborative filtering include: (i) no information about users or items is required; (ii) a high precision can be achieved with little data; (iii) the more interaction between users and items is available, the more recommendations become accurate. However, the disadvantages of collaborative filtering are: (i) it is impossible to make recommendations to a new user or recommend a new item (cold start problem); (ii) calculating recommendations for millions of users or items consumes a lot of computational power (scalability problem); (iii) if the number of items is large relative to the users and most users only have interacted with a small subset of all items, then the resulting representation has many zero interactions and might hence lead to computational difficulties (sparsity problem).\n\n\nContent-Based Filtering\nContent-based filtering methods exploit information about users or items to create recommendations by building a model that relates available characteristics of users or items to each other. The recommendation problem is hence cast into a classification problem (the user will like the item or not) or more generally a regression problem (which rating will the user give an item). The classification problem can be item-centered by focusing on available user information and estimating a model for each item. If there are a lot of user-item interactions available, the resulting model is fairly robust, but it is less personalized (as it ignores user characteristics apart from interactions). The classification problem can also be user-centered by working with item features and estimating a model for each user. However, if a user only has a few interactions then the resulting model becomes easily unstable. Content-based filtering can also be neither user nor item-centered by stacking the two feature vectors, hence considering both input simultaneously, and putting them into a neural network.\nThe main advantage of content-based filtering is that it can make recommendations for new users without any interaction history or recommend new items to users. The disadvantages include: (i) training needs a lot of users and item examples for reliable results; (ii) tuning might be much harder in practice than collaborative filtering; (iii) missing information might be a problem since there is no clear solution how to treat missingness in user or item characteristics.\n\n\nHybrid Recommenders\nHybrid recommender systems combine both collaborative and content-based filtering to overcome the challenges of each approach. There are different hybridization techniques available, e.g., combining the scores of different components (weighted), chosing among different component (switching), following strict priority rules (cascading), presenting outputs from different components at the same time (mixed), etc."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#train-collaborative-filtering-recommenders-in-r",
    "href": "posts/tidy-collaborative-filtering/index.html#train-collaborative-filtering-recommenders-in-r",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Train Collaborative Filtering Recommenders in R",
    "text": "Train Collaborative Filtering Recommenders in R\nFor this post, we rely on the tidyverse family of packages, scales for scale functions for visualization, and recommenderlab2 - a package that provides an infrastructure to develop and test collaborative filtering recommender algorithms.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(recommenderlab)\n\nI load a data set with stock holdings of investable wikifolios at the beginning of 2023 that I host in one of my repositories. The data contains the portfolios of 6,544 wikifolios that held in total 5,069 stocks on January 1st 2023.\n\nwikifolio_portfolios &lt;- read_csv(\"https://raw.githubusercontent.com/christophscheuch/christophscheuch.github.io/main/data/wikifolio_portfolios.csv\") \nglimpse(wikifolio_portfolios)\n\nRows: 149,916\nColumns: 2\n$ wikifolio &lt;chr&gt; \"DE000LS9AAB3\", \"DE000LS9AAB3\", \"DE000LS9AAB3\", \"DE000LS9AAB…\n$ stock     &lt;chr&gt; \"AU000000CSL8\", \"AU0000193666\", \"CA0679011084\", \"CA136635109…\n\n\nFirst, I convert the long data to a binary rating matrix.\n\nbinary_rating_matrix &lt;- wikifolio_portfolios |&gt;\n  mutate(in_portfolio = 1) |&gt; \n  pivot_wider(id_cols = wikifolio,\n              names_from = stock,\n              values_from = in_portfolio,\n              values_fill = list(in_portfolio = 0)) |&gt;\n  select(-wikifolio) |&gt;\n  as.matrix() |&gt;\n  as(\"binaryRatingMatrix\")\nbinary_rating_matrix\n\n6544 x 5069 rating matrix of class 'binaryRatingMatrix' with 149916 ratings.\n\n\nI perform cross-validation and split the data into training and test data. The training sample constitute 80% of the data and I perform 5-fold cross validation. Testing is performed by withholding items from the test portfolios (parameter given) and checking how well the algorithm predicts the withheld items. The value given=-1 means that an algorithm sees all but 1 withheld stock for the test portfolios and needs to predict the missing stock. I refer to Breese et al. (1998)3 for a discussion of other withholding strategies.\n\nscheme &lt;- binary_rating_matrix |&gt;\n  evaluationScheme(\n    method = \"cross-validation\",\n    k      = 5,\n    train  = 0.8,\n    given  = -1\n)\nscheme\n\nEvaluation scheme using all-but-1 items\nMethod: 'cross-validation' with 5 run(s).\nGood ratings: NA\nData set: 6544 x 5069 rating matrix of class 'binaryRatingMatrix' with 149916 ratings.\n\n\nHere is the list of recommenders that I consider for the backtest with some intuition:\n\nRandom Items: the benchmark case because it just stupidly chooses random stocks from all possible choices.\nPopular Items: just recommends the most popular stocks to measured by the number of wikifolios that hold the stock.\nAssociation Rules: each wikifolio and its portfolio is considered as a transaction. Association rule mining finds similar portfolios across all traders (if traders have x, y and z in their portfolio, then they are X% likely of also including w).\nItem-Based Filtering: the algorithm calculates a similarity matrix across stocks. Recommendations are then based on the list of most similar stocks to the ones the wikifolio already has in its portfolio.\nUser-Based Filtering: the algorithm finds a neighborhood of similar wikifolios for each wikifolio (for this exercise it is set to 100 most similar wikifolios). Recommendations are then based on what the most similar wikifolios have in their portfolio.\n\nFor each algorithm, I base the evaluation on 1, 3, 5, and 10 recommendations. This specification means that each algorithm proposes 1 to 10 recommendations to the test portfolios and the evaluation scheme then checks whether the proposals contain the one withheld stock. Note that the evaluation takes a couple of hours, in particular because the Item-Based and User-Based Filtering approaches are quite time-consuming.\n\nalgorithms &lt;- list(\n  \"Random Items\"         = list(name  = \"RANDOM\",  param = NULL),\n  \"Popular Items\"        = list(name  = \"POPULAR\", param = NULL),\n  \"Association Rules\"    = list(name  = \"AR\", param = list(supp = 0.01, conf = 0.1)),\n  \"Item-Based Filtering\" = list(name  = \"IBCF\", param = list(k = 10)),\n  \"User-Based Filtering\" = list(name  = \"UBCF\", param = list(method = \"Cosine\", nn = 100))\n)\n\nnumber_of_recommendations &lt;- c(1, 3, 5, 10)\nresults &lt;- evaluate(\n  scheme,\n  algorithms,\n  type = \"topNList\",\n  progress = TRUE,\n  n = number_of_recommendations\n)"
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#evaluate-recommenders",
    "href": "posts/tidy-collaborative-filtering/index.html#evaluate-recommenders",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Evaluate Recommenders",
    "text": "Evaluate Recommenders\nThe output of evaluate() already provides the evaluation metrics in a structured way. I can simply average the metrics over the cross-validation folds.\n\nresults_tbl &lt;- results |&gt;\n  avg() |&gt;\n  map(as_tibble) |&gt;\n  bind_rows(.id = \"model\")\n\nNow, for each recommender, we are interested the following numbers:\n\nTrue Negative (TN) = number of not predicted items that do not correspond to withheld items\nFalse Positive (FP) = number of incorrect predictions that do not correspond to withheld items\nFalse negative (FN) = number of not predicted items that correspond to withheld items\nTrue Positive (TP) = number of correct predictions that correspond to withheld items\n\nThe two figures below present the most common evaluation techniques for the performance of recommender algorithms in backtest settings like mine.\n\nROC curves\nThe first visualization approach comes from signal-detection and is called “Receiver Operating Characteristic” (ROC). The ROC-curve plots the algorithm’s probability of detection (TPR) against the probability of false alarm (FPR).\n\nTPR = TP / (TP + FN) (i.e., share of true positive recommendations relative to all known portfolios)\nFPR = FP / (FP + TN) (i.e., share of incorrect recommendations relative to )\n\nIntuitively, the bigger the area under the ROC curve, the better is the corresponding algorithm.\n\nresults_tbl |&gt;\n  ggplot(aes(FPR, TPR, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"False Positive Rate (FPR)\",\n    y = \"True Positive Rate (TPR)\",\n    title = \"ROC curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\n\nThe figure shows that recommending random items exhibits the lowest TPR for any FPR, so it is the worst among all algorithms (which is not surprising). Association rules, on the other hand, constitute the best algorithm among the current selection. This result is neat because association rule mining is a computationally cheap algorithm, so we could potentially fine-tune or reestimate the model easily.\n\n\nPrecision-Recall Curves\nThe second popular approach is to plot Precision-Recall curves. The two measures are often used in information retrieval problems:\n\nPrecision = TP / (TP + FP) (i.e., correctly recommended items relative to total recommended items)\nRecall = TP / (TP + FN) (i.e., correctly recommended items relative to total number of known useful recommendations)\n\nThe goal is to have a higher precision for any level of recall. In fact, there is trade-off between the two measures since high precision means low recall and vice-versa.\n\nresults_tbl |&gt;\n  ggplot(aes(x = recall, y = precision, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"Recall\", y = \"Precision\",\n    title = \"Precision-Recall curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\n\nAgain, proposing random items exhibits the worst performance, as for any given level of recall, this approach has the lowest precision. Association rules are also the best algorithm with this visualization approach."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#create-predictions",
    "href": "posts/tidy-collaborative-filtering/index.html#create-predictions",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Create Predictions",
    "text": "Create Predictions\nThe final step is to create stock recommendations for investors who already have portfolios. I pick the AR algorithm to create such recommendations because it excelled in the analyses above. Note that in the case of association rules, I also need to provide the support and confidence parameters.\n\nrecommender &lt;- Recommender(binary_rating_matrix, method = \"AR\", param = list(supp = 0.01, conf = 0.1))\n\nAs an example, suppose you currently have a portfolio that consists of Nvidia (US67066G1040) and Apple (US0378331005). I have to transform this sample portfolio into a rating matrix with the same dimensions as the data we used as input for our training. The predict() function then delivers a prediction for the example portfolio.\n\nsample_portfolio &lt;- c(\"US67066G1040\", \"US0378331005\")\nsample_rating_matrix &lt;- tibble(distinct(wikifolio_portfolios, stock)) |&gt;\n  mutate(in_portfolio = if_else(stock %in% sample_portfolio, 1, 0)) |&gt;\n  pivot_wider(names_from = stock,\n              values_from = in_portfolio,\n              values_fill = list(in_portfolio = 0)) |&gt;\n  as.matrix() |&gt;\n  as(\"binaryRatingMatrix\")\n\nprediction &lt;- predict(recommender, sample_rating_matrix, n = 1)\nas(prediction, \"list\")[[1]]\n\n[1] \"US5949181045\"\n\n\nSo the AR algorithm recommends Microsoft (US5949181045) as a stock if you are already invested in Nvidia and Apple, which makes a lot of sense given the similarity in business model. Of course, this recommendation is not serious investment advice, but rather serves an illustrative purpose of how recommenderlab can be used to quickly prototype different collaborative filtering recommender algorithms."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#footnotes",
    "href": "posts/tidy-collaborative-filtering/index.html#footnotes",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRicci, F, Rokach, L., Shapira, B. and Kantor, P. (2011). “Recommender Systems Handbook”. https://link.springer.com/book/10.1007/978-0-387-85820-3.↩︎\nHahsler, M. (2022). “recommenderlab: An R Framework for Developing and Testing Recommendation Algorithms”, R package version 1.0.3. https://CRAN.R-project.org/package=recommenderlab.↩︎\nBreese, J.S., Heckerman, D. and Kadie, C. (1998). “Empirical Analysis of Predictive Algorithms for Collaborative Filtering”, Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, Madison, 43-52. https://arxiv.org/pdf/1301.7363.pdf.↩︎"
  },
  {
    "objectID": "posts/ggplot2-vs-matplotlib/index.html",
    "href": "posts/ggplot2-vs-matplotlib/index.html",
    "title": "Tidy Data Visualization: ggplot2 vs matplotlib",
    "section": "",
    "text": "ggplot2 is based on Leland Wilkinson”s Grammar of Graphics, a set of principles for creating consistent and effective statistical graphics, and was developed by Hadley Wickham. The package is a cornerstone of the R community and integrates seamlessly with other tidyverse packages. One of the key strengths of ggplot2 is its use of a consistent syntax, making it relatively easy to learn and enabling users to create a wide range of graphics with a common set of functions. The package is also highly customizable, allowing detailed adjustments to almost every element of a plot.\nmatplotlib is a widely-used data visualization library in Python, renowned for its ability to produce high-quality graphs and charts. Rooted in an imperative programming style, matplotlib provides a detailed control over plot elements, making it possible to fine-tune the aesthetics and layout of graphs to a high degree. Its compatibility with a variety of output formats and integration with other data science libraries like numpy and pandas makes it a cornerstone in the Python scientific computing stack.\nThe types of plots that I chose for the comparison heavily draw on the examples given in R for Data Science - an amazing resource if you want to get started with data visualization."
  },
  {
    "objectID": "posts/ggplot2-vs-matplotlib/index.html#loading-packages-and-data",
    "href": "posts/ggplot2-vs-matplotlib/index.html#loading-packages-and-data",
    "title": "Tidy Data Visualization: ggplot2 vs matplotlib",
    "section": "Loading packages and data",
    "text": "Loading packages and data\nWe start by loading the main packages of interest and the popular penguins data that exists as packages for both . We then use the penguins data frame as the data to compare all functions and methods below. Note that I drop all rows with missing values because I don’t want to get into related messages in this post.\n\nggplot2matplotlib\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\npenguins &lt;- na.omit(palmerpenguins::penguins)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\n\npenguins = load_penguins().dropna()"
  },
  {
    "objectID": "posts/ggplot2-vs-matplotlib/index.html#a-full-blown-example",
    "href": "posts/ggplot2-vs-matplotlib/index.html#a-full-blown-example",
    "title": "Tidy Data Visualization: ggplot2 vs matplotlib",
    "section": "A full-blown example",
    "text": "A full-blown example\nLet”s start with an advancved example that combines many different aesthetics at the same time: we plot two columns against each other, use color and shape aesthetics do differentiate species, include separate regression lines for each species, manually set nice labels, and use a theme. As you can see in this example already, ggplot2 and matplotlib have a fundamentally different syntactic approach. While ggplot2 works with layers and easily allows the creation of regression lines for each species, you have to use a loop to get the same results with matplotlib. We also can see the difference between the declarative and imperative programming styles.\n\nggplot2matplotlib\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = bill_depth_mm, \n           color = species, shape = species)) + \n  geom_point(size = 2) + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n  labs(x = \"Bill length (mm)\", y = \"Bill width (mm)\", \n       title = \"Bill length vs. bill width\", \n       subtitle = \"Using geom_point and geom_smooth of the ggplot2 package\",\n       color = \"Species\", shape = \"Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nspecies_unique = sorted(penguins[\"species\"].unique())\nmarkers = [\"o\", \"s\", \"^\"]\ncolors = [\"red\", \"green\", \"blue\"]\n\nfor species, marker, color in zip(species_unique, markers, colors):\n  species_data = penguins[penguins[\"species\"] == species]\n  plt.scatter(\n    species_data[\"bill_length_mm\"], species_data[\"bill_depth_mm\"], \n    s = 50, alpha = 0.7, label = species, marker = marker, color = color\n  )\n    \n  X = species_data[\"bill_length_mm\"]\n  Y = species_data[\"bill_depth_mm\"]\n  m, b = np.polyfit(X, Y, 1)\n  plt.plot(X, m*X + b, color = color)\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Bill width (mm)\")\nplt.title(\"Bill length vs. bill width\")\nplt.legend(title = \"Species\")\n\nplt.show()"
  },
  {
    "objectID": "posts/ggplot2-vs-matplotlib/index.html#visualizing-distributions",
    "href": "posts/ggplot2-vs-matplotlib/index.html#visualizing-distributions",
    "title": "Tidy Data Visualization: ggplot2 vs matplotlib",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\n\nA categorical variable\nLet’s break down the differences in smaller steps by focusing on simpler examples. If you have a categorical variable and want to compare its relevance in your data, then ggplot2::geom_bar() and matplotlib.bar() are your friends. I manually specify the order and values in the matplotlib figure to mimic the automatic behavior of ggplot2.\n\nggplot2matplotlib\n\n\n\nggplot(penguins, \n       aes(x = island)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\ncategorical_variable = penguins[\"island\"].value_counts()\nplt.bar(categorical_variable.index, categorical_variable.values)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nA numerical variable\nIf you have a numerical variable, usually histograms are a good starting point to get a better feeling for the distribution of your data. ggplot2::geom_histogram() and matplotlib.hist with options to control bin widths or number of bins are the functions for this task. Note that you have to manually compute the range of values for matplotlib.\n\nggplot2matplotlib\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\n\n\nnumerical_variable = penguins[\"bill_length_mm\"].dropna()\nplt.hist(numerical_variable, \n         bins = range(int(numerical_variable.min()), \n                      int(numerical_variable.max()) + 2, 2))\n                      \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nBoth packages also support density curves, but I personally wouldn”t recommend to start with densities because they are estimated curves that might obscure underlying data features. However, we look at densities in the next section."
  },
  {
    "objectID": "posts/ggplot2-vs-matplotlib/index.html#visualizing-relationships",
    "href": "posts/ggplot2-vs-matplotlib/index.html#visualizing-relationships",
    "title": "Tidy Data Visualization: ggplot2 vs matplotlib",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\n\nA numerical and a categorical variable\nTo visualize relationships, you need to have at least two columns. If you have a numerical and a categorical variable, then histograms or densities with groups are a good starting point. The next example illustrates the use of density curves via ggplot2::geom_density(). In matplotlib, we have to manually estimate the densities and plot the corresponding lines in a loop. Visually, we get quite similar results.\n\nggplot2matplotlib\n\n\n\nggplot(penguins, \n       aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(linewidth = 0.75, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import gaussian_kde\n\nspecies_list = penguins[\"species\"].unique()\n\nfor species in species_list:\n\n  species_data = penguins[penguins[\"species\"] == species][\"body_mass_g\"]\n    \n  density = gaussian_kde(species_data)\n  xs = np.linspace(species_data.min(), species_data.max(), 200)\n  density.covariance_factor = lambda : .25\n  density._compute_covariance()\n    \n  plt.plot(xs, density(xs), lw = 0.75, label = species)\n  plt.fill_between(xs, density(xs), alpha = 0.5)\n\nplt.xlabel(\"body_mass_g\")\nplt.ylabel(\"density\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo categorical columns\nStacked bar plots are a good way to display the relationship between two categorical columns. geom_bar() with the position argument and matplotlib.bar() are your aesthetics of choice for this task. For matplotlib, we have to first compute the shares, then sequentially fill subplots. Note that you can easily switch to counts by using position = \"identity\" in ggplot2 instead of relative frequencies as in the example below.\n\nggplot2matplotlib\n\n\n\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\n\nshares = (penguins\n  .pipe(lambda x: pd.crosstab(x[\"species\"], x[\"island\"]))\n  .pipe(lambda x: x.div(x.sum(axis = 1), axis = 0))\n)\n\nfig, ax = plt.subplots()\nbottom = None\nfor island in shares.columns:\n  ax.bar(shares.index, shares[island], bottom = bottom, label = island)\n  bottom = shares[island] if bottom is None else bottom + shares[island]\n  \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo numerical columns\nScatter plots and regression lines are definitely the most common approach for visualizing the relationship between two numerical columns and we focus on scatter plots for this example (see the first visualization example if you want to see again how to add a regression line). Here, the size parameter controls the size of the shapes that you use for the data points in ggplot2::geom_point() relative to the base size (i.e., it is not tied to any unit of measurement like pixels). For matplotlib.scatter() you have the s parameter to control point sizes manually, where size is typically given in squared points (where a point is a unit of measure in typography, equal to 1/72 of an inch).\n\nggplot2matplotlib\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(size = 2)\n\n\n\n\n\n\n\n\n\n\n\nplt.scatter(x = penguins[\"bill_length_mm\"], y = penguins[\"flipper_length_mm\"], \n            s = 50) \nplt.xlabel(\"bill_length_mm\")\nplt.ylabel(\"flipper_length_mm\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree or more columns\nYou can include more information by mapping columns to additional aesthetics. For instance, we can map colors and shapes to species and create separate plots for each island by using facets. Facets are actually a great way to extend your figures, so I highly recommend playing around with them using your own data.\nIn ggplot2 you add the facet layer at the end, whereas in matplotlib you have to start with the facet grid at the beginning and map scatter plots across facets. Note that I use variable assignment to penguins_facet in order to prevent matplotlib from printing the figure twice while rendering this post (no idea why though).\n\nggplot2matplotlib\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\n\n\n\n\n\n\n\n\n\nspecies = sorted(penguins[\"species\"].unique())\nislands = sorted(penguins[\"island\"].unique())\n\ncolor_map = dict(zip(species, [\"blue\", \"green\", \"red\"]))\nshape_map = dict(zip(species, [\"o\", \"^\", \"s\"]))\n\nfig, axes = plt.subplots(ncols = len(islands))\n\nfor ax, island in zip(axes, islands):\n  island_data = penguins[penguins[\"island\"] == island]\n  for spec in species:\n    spec_data = island_data[island_data[\"species\"] == spec]\n    ax.scatter(spec_data[\"bill_length_mm\"], spec_data[\"flipper_length_mm\"], \n               color = color_map[spec], marker = shape_map[spec], label = spec)\n  ax.set_title(island)\n  ax.set_xlabel(\"Bill Length (mm)\")\n  ax.set_ylabel(\"Flipper Length (mm)\")\n\naxes[0].legend(title = \"Species\")\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "posts/ggplot2-vs-matplotlib/index.html#time-series",
    "href": "posts/ggplot2-vs-matplotlib/index.html#time-series",
    "title": "Tidy Data Visualization: ggplot2 vs matplotlib",
    "section": "Time series",
    "text": "Time series\nAs a last example, we quickly dive into time series plots where you typically want to show multiple lines over some time period. Here, I aggregate the number of penguins by year and island and plot the corresponding lines. All packages behave as expected and show similar output.\nmatplotlib does not directly support setting line styles based on a variable like island within its syntax. Instead, you have to iterate over each island and manually set the line style for each island.\n\nggplot2matplotlib\n\n\n\npenguins |&gt; \n  count(year, island) |&gt; \n  ggplot(aes(x = year, y = n, color = island)) +\n  geom_line(aes(linetype = island))\n\n\n\n\n\n\n\n\n\n\n\ncount_data = (penguins\n  .groupby([\"year\", \"island\"])\n  .size()\n  .reset_index(name=\"count\")\n)\n\nislands = count_data[\"island\"].unique()\ncolors = [\"red\", \"green\", \"blue\"]\nline_styles = [\"-\", \"--\", \"-.\"]\n\nplt.figure()\nfor j, island in enumerate(islands):\n  island_data = count_data[count_data[\"island\"] == island]\n  plt.plot(island_data[\"year\"], island_data[\"count\"], \n           color = colors[j], \n           linestyle = line_styles[j], \n           label = island)\n\nplt.xlabel(\"year\")\nplt.ylabel(\"count\")\nplt.legend(title = \"island\")\n\nplt.show()"
  },
  {
    "objectID": "posts/ggplot2-vs-matplotlib/index.html#saving-plots",
    "href": "posts/ggplot2-vs-matplotlib/index.html#saving-plots",
    "title": "Tidy Data Visualization: ggplot2 vs matplotlib",
    "section": "Saving plots",
    "text": "Saving plots\nAs a final comparison, let us look at saving plots. ggplot2::ggsave() provides the most important options as function paramters. In matplotlib, you have to tweak the figure size before you can save the figure.\n\nggplot2matplotlib\n\n\n\npenguins_figure &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) + \n  geom_point()\n\nggsave(penguins_figure, filename = \"penguins-figure.png\",\n       width = 7, height = 5, dpi = 300)\n\n\n\n\nplt.figure(figsize = (7, 5))\nplt.scatter(x = penguins[\"bill_length_mm\"], y = penguins[\"flipper_length_mm\"])\nplt.xlabel(\"bill_length_mm\")\nplt.ylabel(\"flipper_length_mm\")\n\nplt.savefig(\"penguins-figure.png\", dpi = 300)"
  },
  {
    "objectID": "posts/ggplot2-vs-matplotlib/index.html#conclusion",
    "href": "posts/ggplot2-vs-matplotlib/index.html#conclusion",
    "title": "Tidy Data Visualization: ggplot2 vs matplotlib",
    "section": "Conclusion",
    "text": "Conclusion\nIn terms of syntax, ggplot2 and matplotlib are considerably different. ggplot2 uses a declarative style where you declare what the plot should contain. You specify mappings between data and aesthetics (like color, size) and add layers to build up the plot. This makes it quite structured and consistent. matplotlib, on the other hand, follows an imperative style where you build plots step by step. Each element of the plot (like lines, labels, legend) is added and customized using separate commands. It allows for a high degree of customization but can be verbose for complex plots.\nI think both approaches are powerful and have their unique advantages, and the choice between them often depends on your programming language preference and specific requirements of the data visualization task at hand.\n\n   Related articles\n    \n      \n        \n            \n            \n               Tidy Data Visualization: ggplot2 vs seaborn\n               A comparison of two popular data visualization tools for R and Python\n            \n        \n        \n        \n        \n        \n            \n            \n                Interactive Data Visualization with R\n                A comparison of the dynamic visualization libraries ggiraph, plotly and highcharter for R\n            \n        \n        \n        \n        \n        \n            \n            \n                Tidy Data Visualization: ggplot2 vs plotnine\n                A comparison of two popular data visualization tools for R and Python"
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "",
    "text": "Imagine trying to cook a meal in a disorganized kitchen where ingredients are mixed up and nothing is labeled. It would be chaotic and time-consuming to look for the right ingredients and there might be some trial error involved, possibly ruining your planned meal.\nTidy data are like well-organized shelves in your kitchen. Each shelf provides a collection of containers that semantically belong together, e.g., spices or dairies. Each container on the shelf holds one type of ingredient, and the labels on the containers clearly describe what is inside, e.g., pepper or milk. In the same way, tidy data organizes information into a clear and consistent format, where each type of observational unit forms a table, each variable is in a column, and each observation is in a row (Wickham 2014).\nTidying data is about structuring datasets to facilitate analysis, visualization, report generation, or modelling. By following the principle that each variable forms a column, each observation forms a row, and each type of observational unit forms a table, data analysis becomes more intuitive, akin to cooking in a well-organized kitchen where everything has its place and you spend less time on searching for ingredients."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#example-for-tidy-data",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#example-for-tidy-data",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "Example for tidy data",
    "text": "Example for tidy data\nTo illustrate the concept of tidy data in our tidy kitchen, suppose we have a table called ingredient that contains information about all the ingredients that we currently have in our kitchen. It might look as follows:\n\n\n\nname\nquantity\nunit\ncategory\n\n\n\n\nflour\n500\ngrams\nbaking\n\n\nsugar\n200\ngrams\nbaking\n\n\nbutter\n100\ngrams\ndairy\n\n\neggs\n4\nunits\ndairy\n\n\nmilk\n1\nliters\ndairy\n\n\nsalt\n10\ngrams\nseasoning\n\n\nolive oil\n0.2\nliters\noil\n\n\ntomatoes\n300\ngrams\nvegetable\n\n\nchicken\n400\ngrams\nmeat\n\n\nrice\n250\ngrams\ngrain\n\n\n\nEach row refers to a specific ingredient and each column has a dedicated type and meaning. For instance, the column quantity contains information about how much of the ingredient called name we currently have and which unit we use to measure it.\nSimilarly, we could have a table just for dairy that might look as follows:\n\n\n\nname\nquantity\nunit\n\n\n\n\nmilk\n1\nliters\n\n\nbutter\n200\ngrams\n\n\nyogurt\n150\ngrams\n\n\ncheese\n100\ngrams\n\n\ncream\n0.5\nliters\n\n\ncottage cheese\n250\ngrams\n\n\nsour cream\n150\ngrams\n\n\nghee\n100\ngrams\n\n\nwhipping cream\n0.3\nliters\n\n\nice cream\n500\ngrams\n\n\n\nNotice that there is no category column in this table? It would actually be redundant to have this column because all rows in the `dairy`` table have the same category."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-colum-headers-are-values-not-variable-names",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-colum-headers-are-values-not-variable-names",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When colum headers are values, not variable names",
    "text": "When colum headers are values, not variable names\nNow let us move to data structures that are untidy. Consider the following variant of our dairy table:\n\n\n\ntype\nliters\ngrams\n\n\n\n\nmilk\n1\n\n\n\nbutter\n\n200\n\n\nyogurt\n\n150\n\n\ncheese\n\n100\n\n\ncream\n0.5\n\n\n\ncottage cheese\n\n250\n\n\nsour cream\n\n150\n\n\nghee\n\n100\n\n\nwhipping cream\n0.3\n\n\n\nice cream\n\n500\n\n\n\nWhat is the issue here? Each row still refers to a specific dairy product. However, instead of dedicated quantity and unit columns, we have a liters and grams column. Since the units differ across dairy products, the table even contains missing values in the form of emtpy cells. So if you want to find out how much of ice cream you still have, you need to also check out the column name. In practice, we would create dedicated quantity and unit columns. we might even decide to have the same unit for all ingredients (e.g., measure everything in grams) and just keep a quantity column."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-multiple-variables-are-stored-in-one-column",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-multiple-variables-are-stored-in-one-column",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When multiple variables are stored in one column",
    "text": "When multiple variables are stored in one column\nLet us consider the following untidy version of our ingredient table.\n\n\n\ntype\nquantity_and_unit\n\n\n\n\nflour\n500 grams\n\n\nsugar\n200 grams\n\n\nbutter\n100 grams\n\n\neggs\n4 units\n\n\nmilk\n1 liter\n\n\nsalt\n10 grams\n\n\nolive oil\n0.2 liters\n\n\ntomatoes\n300 grams\n\n\nchicken\n400 grams\n\n\nrice\n250 grams\n\n\n\nThis one is really annoying, since the quantity_and_unit column combines both the quantity and the unit of measurement into one string for each ingredient. Why is this an issue? This format actually makes it harder to perform numerical operations on the quantities or to filter or aggregate the data based on the unit of measurement. So in practice, we would actually start our data analysis by splitting out the quantity_and_unit column into quantity and unit."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-variables-are-stored-in-both-rows-and-columns",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-variables-are-stored-in-both-rows-and-columns",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When variables are stored in both rows and columns",
    "text": "When variables are stored in both rows and columns\nLet us extend our kitchen analogy by additionally considering recipes. For simplicity, a recipe just denotes how much of each ingredient is required. The following table contains two variants of a recipe for pancakes:\n\n\n\ningredient\nrecipe1_quantity\nrecipe2_quantity\n\n\n\n\nflour\n500 grams\n300 grams\n\n\nsugar\n200 grams\n150 grams\n\n\nbutter\n100 grams\n50 grams\n\n\neggs\n4 units\n3 units\n\n\nmilk\n1 liters\n0.5 liters\n\n\n\nThe quantity for each ingredient for two different recipes is stored in separate columns. This structure makes it harder to perform operations like filtering or summarizing the data by recipe or ingredient.\nTo convert this data to a tidy format, you would typically want to gather the quantities into a single column, and include additional columns to specify the recipe and unit of measurement for each quantity. We can then filer"
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-there-are-multiple-types-of-data-in-the-same-column",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-there-are-multiple-types-of-data-in-the-same-column",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When there are multiple types of data in the same column",
    "text": "When there are multiple types of data in the same column\nA recipe typically contains information on the required utensils and how much time a step requires. Consider the following table with different types of data:\n\n\n\ntype\nquantity\ncategory\n\n\n\n\nflour\n500 grams\ningredient\n\n\nbutter\n100 grams\ningredient\n\n\nwhisk\n1 unit\nutensil\n\n\nsugar\n200 grams\ningredient\n\n\nbaking time\n30 minutes\ntime\n\n\n\nThe table is trying to describe a recipe but combines different types of data within the same columns. There are ingredients with their quantities, a utensil, and cooking time, all mixed together.\nA tidy approach would typically separate these different types of data into separate tables or at least into distinct sets of columns, making it clear what each part of the data represents and facilitating further analysis and visualization."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-some-data-is-missing",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-some-data-is-missing",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When some data is missing",
    "text": "When some data is missing\nAs a last example for untidy data, let us consider the original ingredient table again, but with a few empty cells.\n\n\n\nname\nquantity\nunit\n\n\n\n\nflour\n\ngrams\n\n\nsugar\n200\ngrams\n\n\nbutter\n100\ngrams\n\n\neggs\n4\nunits\n\n\nmilk\n10\n\n\n\nsalt\n10\ngrams\n\n\nolive oil\n0.2\nliters\n\n\ntomatoes\n300\ngrams\n\n\nchicken\n400\ngrams\n\n\n\n250\ngrams\n\n\n\nWhat is the issue here? There are actually a couple of them:\n\nThe flour row does have any information about quantity, so we just don’t know how much we have.\nThe milk row does not contain a unit, so we might have 10 liters, 10 milliliters, or 10 cups of milk.\nThe last row does not have any name, so we have 250 grams of something that we just can’t identify.\n\nWhy is this important? It makes a huge difference how me treat the missing information. For instance, we might make an educated guess for milk if we always record that information in litres, then the missing unit is very likely litres. For flour, we could play it safe and just say that the available quantity is zero. For the ingredient without a name, we might have to throw it away or ask somebody else to tell us what it is.\nOverall, these examples highlight the most important issues that you might have to consider when preparing data for your analysis."
  },
  {
    "objectID": "posts/classification-customer-churn/index.html",
    "href": "posts/classification-customer-churn/index.html",
    "title": "Tidy Classification Models: Customer Churn Prediction",
    "section": "",
    "text": "In this post, we’ll dive into how tidymodels can be leveraged to quickly prototype and compare classification models with tidy syntax. Classification models are a subset of machine learning models that are used to predict or classify the categories (aka classes) of new observations based on past observations with known category labels. Classification falls into the domain of supervised learning.\ntidymodels is an ecosystem of R packages designed for data modeling and statistical analysis that adheres to the principles of the tidyverse. It provides a comprehensive framework for building and evaluating models, streamlining workflows from data pre-processing and feature engineering to model training, validation, and fine-tuning. As tidymodels provides a unified interface for various modeling techniques, it simplifies the process of creating reproducible and scalable data analysis pipelines, catering to both novice and experienced data scientists.\nI’ll use the Telco Customer Churn data from IBM Sample Data Sets because it is a popular open-source data set used in predictive analytics, particularly for demonstrating the process of churn prediction. The data contains a mix of customer attributes, service usage patterns, and billing information, along with a churn indicator that specifies whether a customer has left the company."
  },
  {
    "objectID": "posts/classification-customer-churn/index.html#download-clean-data",
    "href": "posts/classification-customer-churn/index.html#download-clean-data",
    "title": "Tidy Classification Models: Customer Churn Prediction",
    "section": "Download & clean data",
    "text": "Download & clean data\nWe can download the telco customer churn data directly from the IBM GitHub account using the readr package. We immediately harmonize the column names using the janitor package. The raw data contains the following columns:\n\ncustomer_id: A unique identifier for each customer.\ngender: The customer’s gender (male/female).\nsenior_citizen: Indicates whether the customer is a senior citizen.\npartner: Indicates whether the customer has a partner.\ndependents: Indicates whether the customer has dependents.\ntenure: The number of months the customer has been with the company.\nphone_service: Indicates whether the customer has phone service.\nmultiple_lines: Indicates whether the customer has multiple lines.\ninternet_service: Type of internet service (DSL, Fiber optic, No).\nonline_security: Indicates whether the customer has online security services.\nonline_backup: Indicates whether the customer has online backup services.\ndevice_protection: Indicates whether the customer has device protection plans.\ntech_support: Indicates whether the customer has tech support services.\nstreaming_tv: Indicates whether the customer has streaming TV services.\nstreaming_movies: Indicates whether the customer has streaming movies services.\ncontract: The contract term of the customer (Month-to-month, One year, Two year).\npaperless_billing: Indicates whether the customer has paperless billing.\npayment_method: The customer’s payment method (Electronic check, Mailed check, Bank transfer, Credit card).\nmonthly_charges: The amount charged to the customer monthly.\ntotal_charges: The total amount charged to the customer.\nchurn: Whether the customer churned or not (Yes or No).\n\nWe start the data preparation by dropping all rows with missing values. We only use 11 observations, so there is no need to investigate or impute. We can also remove customer_id because we don’t need it for pre-processing. I make sure that all binary variables are encoded with binary indicators. We’ll apply one-hot encoding to the remaining variables later.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\n\ndata_url &lt;- \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\ncustomer_raw &lt;- read_csv(data_url) |&gt; \n  clean_names()\n\ncustomer &lt;- customer_raw |&gt; \n  select(-customer_id) |&gt; \n  na.omit() |&gt; \n  mutate(churn = factor(if_else(churn==\"Yes\", 1L, 0L)),\n         female = if_else(gender==\"Female\", 1L, 0L),\n         senior_citizen = as.integer(senior_citizen)) |&gt; \n  select(-gender) |&gt; \n  mutate(\n    across(c(partner, dependents, phone_service, paperless_billing), \n           ~if_else(. == \"Yes\", 1L, 0L)),\n    across(c(multiple_lines, internet_service, online_security, online_backup, \n             device_protection, tech_support, streaming_tv, streaming_movies,\n             contract, paperless_billing, payment_method),\n           ~tolower(gsub(\"[ |\\\\-]\", \"_\", ., \" |\\\\-\", \"_\")))\n    )\n\nWe next prepare the data for machine learning model training and evaluation by creating a reproducible initial split into training and test sets, followed by setting up a stratified 5-fold cross-validation scheme on the training data. This approach is crucial for developing, tuning, and selecting models based on their performance metrics, ensuring that the chosen model is robust and performs well on unseen data.\n\nlibrary(tidymodels)\n\nset.seed(1234)\ncustomer_split &lt;- initial_split(customer, prop = 4/ 5, strata = churn)\ncustomer_folds &lt;- vfold_cv(training(customer_split), v = 5, strata = churn)"
  },
  {
    "objectID": "posts/classification-customer-churn/index.html#pre-process-data",
    "href": "posts/classification-customer-churn/index.html#pre-process-data",
    "title": "Tidy Classification Models: Customer Churn Prediction",
    "section": "Pre-process data",
    "text": "Pre-process data\nData pre-processing is implemented via recipe() in the tidymodels framework. A recipe defines a series of data pre-processing steps to prepare the training data for modeling. I chose to transforme total_charges (to handle its skewed distribution), normalize tenure and monthly_charges, and encode categorical variables as dummy variables. one_hot = TRUE specifies that one-hot encoding should be used, where each level of the categorical variables is transformed into a new binary column, with a value of 1 if the observation belongs to that level and 0 otherwise. This is necessary because many machine learning models cannot directly handle categorical variables and require them to be converted into a numeric format.\n\ncustomer_recipe &lt;- recipe(churn ~ ., data = training(customer_split)) |&gt; \n  step_log(c(total_charges)) |&gt; \n  step_normalize(c(tenure, monthly_charges)) |&gt;\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) \n\nThis pre-processing is crucial for ensuring that the data is in the right format and scale for the algorithms to work effectively, ultimately leading to more accurate and reliable model predictions.\nA workflow() in the tidymodels framework encapsulates all the components needed to train a machine learning model, including pre-processing instructions (via recipes) and the model itself. So, we start by initializing our workflow with the recipe and add other components later.\n\ncustomer_workflow &lt;- workflow() |&gt; \n  add_recipe(customer_recipe)"
  },
  {
    "objectID": "posts/classification-customer-churn/index.html#build-models",
    "href": "posts/classification-customer-churn/index.html#build-models",
    "title": "Tidy Classification Models: Customer Churn Prediction",
    "section": "Build models",
    "text": "Build models\n\nLogistic regressionRandom ForestXGBoostK-nearest neighborNeural network\n\n\nLogistic Regression is a statistical method for predicting binary outcomes. The outcome is modeled as a function of predictor variables using the logistic function to ensure that the predictions fall between 0 and 1, which are then interpreted as probabilities of belonging to a particular class.\nglmnet is an R package that fits generalized linear models via penalized maximum likelihood. The penalty term is a combination of the L1 norm (lasso) and the L2 norm (ridge) of the coefficients. If you want to learn more about this model, check out our chapter on Factor Selection via Machine Learning in Tidy Finance with R.\nBy specifying a logistic regression model with a slight L1 penalty (lasso) using the glmnet engine, we’re aiming to build a model that can predict binary outcomes, while also performing feature selection to keep the model simple and prevent overfitting.\n\nlibrary(glmnet)\n\nspec_logistic &lt;- logistic_reg(penalty = 0.0001, mixture = 1) |&gt;\n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"classification\")\n\n\n\nAt its core, a Random Forest model consists of many decision trees. A decision tree is a simple model that makes decisions based on answering a series of questions based on the features of the data. The ranger package provides a fast and efficient implementation of Random Forests and is able to handle large data sets and high-dimensional data well.\n\nlibrary(ranger)\n\nspec_random_forest &lt;- rand_forest() |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\n\n\nXGBoost stands for eXtreme Gradient Boosting. It is part of a family of boosting algorithms which are based on the principle of gradient boosting. Boosting, in the context of machine learning, refers to the technique of sequentially building models, where each new model attempts to correct the errors of the previous ensemble of models. Unlike Random Forests, where each tree is built independently, XGBoost builds one tree at a time. Each new tree in XGBoost learns from the mistakes of the previous trees.\n\nlibrary(xgboost)\n\nspec_xgboost &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"classification\")\n\n\n\nThe k-Nearest Neighbors algorithm operates on a very simple principle: “Tell me who your neighbors are, and I will tell you who you are.” For a given data point that we wish to classify, the algorithm looks at the ‘k’ closest points (neighbors) to it in the data, based on a distance metric (typically Euclidean distance). The class that is most common among these neighbors is then assigned to the data point.\n\nlibrary(kknn)\n\nspec_knn &lt;- nearest_neighbor(neighbors = 4) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"classification\")\n\n\n\nIntuitively, neural networks try to mimic the way human brains operate, albeit in a very simplified form. A neural network consists of layers of interconnected nodes (neurons), where each connection (synapse) carries a weight that signifies the importance of the input it’s receiving. The basic idea is that, through a process of learning, the network adjusts these weights to make accurate predictions or classifications.\nA mulit-layer-perceptron (MLP) is a type of neural network that includes one or more hidden layers between the input and output layers. Each layer’s output is passed through an activation function, which allows the network to capture complex patterns in the data. hidden_units = 10 specifies that there are 10 neurons in a single hidden layer and epochs = 500 indicates the number of times the learning algorithm will work through the entire training data.\n\nlibrary(torch)\nlibrary(brulee)\n\nspec_neural_net &lt;- mlp(epochs = 500, hidden_units = 10) |&gt;\n  set_engine(\"brulee\") |&gt;\n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/classification-customer-churn/index.html#fit-models-on-training-data",
    "href": "posts/classification-customer-churn/index.html#fit-models-on-training-data",
    "title": "Tidy Classification Models: Customer Churn Prediction",
    "section": "Fit models on training data",
    "text": "Fit models on training data\nNow that we have defined a couple of models, we want to fit them to the training data next. As stated above, we use cross-validation and we look at multiple metrics (recall, precision, and accuracy). Since the approach is the same across model specifications, I wrote a little helper function that adds the model to the workflow, fits the model using the customer_folds from above, and collects the metrics of interest. We can then simply map this function across specifications and get a table of metrics.\n\ncreate_metrics_training &lt;- function(spec) {\n  customer_workflow |&gt; \n    add_model(spec) |&gt; \n    fit_resamples(\n      resamples = customer_folds,\n      metrics = metric_set(recall, precision, accuracy),\n      control = control_resamples(save_pred = TRUE)\n    ) |&gt; \n    collect_metrics(summarize = TRUE) |&gt; \n    mutate(model = attributes(spec)$class[1])\n}\n\nmetrics_training &lt;- list(\n  spec_logistic, spec_random_forest, spec_xgboost, \n  spec_knn, spec_neural_net\n) |&gt; \n  map_df(create_metrics_training)"
  },
  {
    "objectID": "posts/classification-customer-churn/index.html#evaluate-models",
    "href": "posts/classification-customer-churn/index.html#evaluate-models",
    "title": "Tidy Classification Models: Customer Churn Prediction",
    "section": "Evaluate models",
    "text": "Evaluate models\nThe first step in model evaluation is to compare the metrics of interest based on training data. We focus on the average for each metric and model across the different folds. However, I also add confidence intervals to the metrics to illustrate the dispersion across folds.\nWe can see that the nearest neighbor approach shows the worst performance across all metrics, while the other exhibit very similar performance. The logistic regression and neural network seem to be in the lead, but there is no clear winner because their confidence intervals overlap.\n\nmetrics_training |&gt; \n  mutate(ci_lower = mean - std_err / sqrt(n) * qnorm(0.99),\n         ci_upper = mean + std_err / sqrt(n) * qnorm(0.99)) |&gt; \n  ggplot(aes(x = mean, y = model, fill = model)) +\n  geom_col() + \n  geom_errorbar(aes(xmin = ci_lower, xmax = ci_upper), width = .2,\n                position = position_dodge(.9)) +\n  facet_wrap(~.metric, ncol = 1) +\n  labs(x = NULL, y = NULL,\n       title = \"Comparison of metrics for different classification models\",\n       subtitle = \"Based on training data. Error bars indicate 99% confidence intervals.\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  coord_cartesian(xlim = c(0, 1)) \n\n\n\n\n\n\n\n\nThe second important evaluation step involves fitting the models to the training data and then using the fitted models to create predictions in the test sample. Again, I created a helper function that can be mapped across different model specifications.\n\ncreate_metrics_test &lt;- function(spec) {\n\n  test_predictions &lt;-  customer_workflow |&gt;\n    add_model(spec) |&gt; \n    fit(data =  training(customer_split)) |&gt; \n    predict(new_data = testing(customer_split))\n  \n  test_results &lt;- bind_cols(testing(customer_split), test_predictions)\n  \n  bind_rows(\n    recall(test_results, truth = churn, estimate = .pred_class),\n    precision(test_results, truth = churn, estimate = .pred_class),\n    accuracy(test_results, truth = churn, estimate = .pred_class)\n  ) |&gt; \n    mutate(model = attributes(spec)$class[1])\n}\n\nmetrics_test &lt;- list(\n  spec_logistic, spec_random_forest, spec_xgboost, \n  spec_knn, spec_neural_net\n) |&gt; \n  map_df(create_metrics_test)\n\nLet’s compare the performance of models in the training to the test sample. We can see that the models perform similarly across samples, which is good.\n\nmetrics &lt;- bind_rows(\n  metrics_training |&gt; \n    mutate(sample = \"Training\") |&gt; \n    select(metric = .metric, estimate = mean, model, sample),\n  metrics_test |&gt; \n    mutate(sample = \"Test\") |&gt; \n    select(metric = .metric, estimate = .estimate, model, sample)\n) \n\nmetrics |&gt; \n  ggplot(aes(x = estimate, y = model, fill = sample)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~metric, ncol = 1) +\n  labs(x = NULL, y = NULL, fill = \"Sample\",\n       title = \"Comparison of metrics for different classification models\",\n       subtitle = paste0(\"Training is the mean across 5-fold cross validation results.\\n\",\n                         \"Test is based on 20% of the initial data.\")) +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 1))"
  },
  {
    "objectID": "posts/classification-customer-churn/index.html#tune-models",
    "href": "posts/classification-customer-churn/index.html#tune-models",
    "title": "Tidy Classification Models: Customer Churn Prediction",
    "section": "Tune models",
    "text": "Tune models\nTo fit the models above, we simply imposed a couple of values for the hyperparameters of each model. I recommend you to check out the tidymodels documentation to learn more about the defaults that it implements in its functions. Model tuning is the process of optimally selecting such hyperparameters. tidymodels provides extensive tuning options based on cross-validation.\nThe general idea is to create a grid of parameter values that you want to tune and then fit the model to the training data using cross-validation for each parameter combination. This step is typically computationally quite expensive because it involves a lot of estimations. So in practice, you’d typically choose the best model from the previous evaluation step and tune it. The code chunks below show you how to tune each model and the resulting best model. However, note that I only pick 2 hyperparameters to tune for each model although most models have more paraters.\n\nLogistic regressionRandom forestXGBoostK-nearest neighborNeural net\n\n\n\nspec_logistic_tune &lt;- logistic_reg(\n  penalty = tune(), \n  mixture = tune()\n) |&gt;\n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"classification\")\n\ngrid_logistic &lt;- grid_regular(\n  penalty(range = c(-10, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  levels = 5 \n)\n\ntune_logistic &lt;- tune_grid(\n  customer_workflow |&gt; \n    add_model(spec_logistic_tune),\n  resamples = customer_folds,\n  grid = grid_logistic,\n  metrics = metric_set(recall, precision, accuracy) \n)\n\ntuned_model_logistic &lt;- finalize_model(\n  spec_logistic_tune, select_best(tune_logistic, \"accuracy\")\n)\ntuned_model_logistic\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\n\n\nspec_random_forest_tune &lt;- rand_forest(\n  mtry = tune(), \n  min_n = tune()\n) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\ngrid_random_forest &lt;- grid_regular(\n  mtry(range = c(1, 5)),\n  min_n(range = c(2, 40)),\n  levels = 5\n)\n\ntune_random_forest  &lt;- tune_grid(\n  customer_workflow |&gt; \n    add_model(spec_random_forest_tune),\n  resamples = customer_folds,\n  grid = grid_random_forest,\n  metrics = metric_set(recall, precision, accuracy) \n)\n\ntuned_model_random_forest &lt;- finalize_model(\n  spec_random_forest_tune, select_best(tune_random_forest, \"accuracy\")\n)\ntuned_model_random_forest\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 5\n  min_n = 40\n\nComputational engine: ranger \n\n\n\n\n\nspec_xgboost_tune &lt;- boost_tree(\n  mtry = tune(), \n  min_n = tune()\n) |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"classification\")\n\ngrid_xgboost &lt;- grid_regular(\n  mtry(range = c(1L, 5L)),\n  min_n(range = c(1L, 40L)),\n  levels = 5\n)\n\ntune_xgboost  &lt;- tune_grid(\n  customer_workflow |&gt; \n    add_model(spec_xgboost_tune),\n  resamples = customer_folds,\n  grid = grid_xgboost,\n  metrics = metric_set(recall, precision, accuracy) \n)\n\ntuned_model_xgboost &lt;- finalize_model(\n  spec_xgboost_tune, select_best(tune_xgboost, \"accuracy\")\n)\ntuned_model_xgboost\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 5\n  min_n = 40\n\nComputational engine: xgboost \n\n\n\n\n\ngrid_knn &lt;- grid_regular(\n  neighbors(range = c(1L, 20L)),\n  levels = 5 \n)\n\nspec_knn_tune &lt;- nearest_neighbor(\n  neighbors = tune()\n) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"classification\")\n\ntune_knn &lt;- tune_grid(\n  customer_workflow |&gt; \n    add_model(spec_knn_tune),\n  resamples = customer_folds,\n  grid = grid_knn,\n  metrics = metric_set(recall, precision, accuracy) \n)\n\ntuned_model_knn &lt;- finalize_model(spec_knn_tune, select_best(tune_knn, \"accuracy\"))\ntuned_model_knn\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 20\n\nComputational engine: kknn \n\n\n\n\n\ngrid_neural_net &lt;- grid_regular(\n  hidden_units(range = c(1L, 10L)),\n  epochs(range = c(10L, 1000L)),\n  levels = 5 \n)\n\nspec_neural_net_tune &lt;- mlp(\n  epochs = tune(), \n  hidden_units = tune()\n) |&gt;\n  set_engine(\"brulee\") |&gt;\n  set_mode(\"classification\") \n\ntune_neural_net &lt;- tune_grid(\n  customer_workflow |&gt; \n    add_model(spec_neural_net_tune),\n  resamples = customer_folds,\n  grid = grid_neural_net,\n  metrics = metric_set(recall, precision, accuracy) \n)\n\ntuned_model_neural_net &lt;- finalize_model(spec_neural_net_tune, select_best(tune_neural_net, \"accuracy\"))\ntuned_model_neural_net\n\nSingle Layer Neural Network Model Specification (classification)\n\nMain Arguments:\n  hidden_units = 10\n  epochs = 1000\n\nComputational engine: brulee"
  },
  {
    "objectID": "posts/classification-customer-churn/index.html#comparing-tuned-models",
    "href": "posts/classification-customer-churn/index.html#comparing-tuned-models",
    "title": "Tidy Classification Models: Customer Churn Prediction",
    "section": "Comparing tuned models",
    "text": "Comparing tuned models\nNow, that we have used tuning to find better models than our initial guesses, we can again compare the model performance in different samples.\n\nmetrics_tuned &lt;- list(\n  tuned_model_logistic, tuned_model_random_forest, tuned_model_xgboost,\n  tuned_model_knn, tuned_model_neural_net\n) |&gt; \n  map_df(create_metrics_test)\n\nmetrics_comparison &lt;- bind_rows(\n  metrics,\n  metrics_tuned |&gt; \n    mutate(sample = \"Tuned\") |&gt; \n    select(metric = .metric, estimate = .estimate, model, sample)\n)\n\nI only plot the accuracy measure below to avoid visual overload. We can see that the fine tuning significantly improved the performance of the nearest neighbor algorithm, but it had little impact on the other approaches. One explanation for the lack of dramatic increases in model performance might be that I picked the wrong parameters for tuning, so I encourage you to play around with the hyperparameters.\n\nmetrics_comparison |&gt; \n  filter(metric == \"accuracy\") |&gt; \n  ggplot(aes(x = estimate, y = model, fill = sample)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~metric, ncol = 1) +\n  labs(x = NULL, y = NULL, fill = \"Sample\",\n       title = \"Comparison of accuracy for different classification models\",\n       subtitle = paste0(\"Training is the mean across 5-fold cross validation results of inital models.\\n\", \n                         \"Test is based on 20% of the initial data using the initial models.\\n\",\n                         \"Tuned is based on 20% of the initial data using the fine-tuned models.\")) +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 1))"
  },
  {
    "objectID": "posts/classification-customer-churn/index.html#concluding-remarks",
    "href": "posts/classification-customer-churn/index.html#concluding-remarks",
    "title": "Tidy Classification Models: Customer Churn Prediction",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nSo what is the best model now for the classification problem? There are so many different measures and ways to evaluate models that it might be hard to draw conclusions. I usually go with the following heuristic: which model has on average the best rank across all measures and samples? It turns out that the logistic regression is most often on top, followed by the neural network. This finding is interesting because the logistic regression is quite simple and computationally relatively inexpensive. A natural next step would be to extend the neural network to multiple layers and find a deep neural net that is able to consistently beat the logistic regression.\n\nmetrics_comparison |&gt; \n  group_by(sample, metric) |&gt; \n  arrange(-estimate) |&gt; \n  mutate(rank = row_number()) |&gt; \n  group_by(model) |&gt; \n  summarize(rank = mean(rank)) |&gt; \n  arrange(rank)\n\n# A tibble: 5 × 2\n  model             rank\n  &lt;chr&gt;            &lt;dbl&gt;\n1 mlp               1.89\n2 logistic_reg      2.11\n3 rand_forest       2.11\n4 boost_tree        3.89\n5 nearest_neighbor  5   \n\n\nOverall, this blog post illustrates what I love most about tidymodels: its scalability with respect to models and metrics. You can quickly prototype different approaches using very little code.\nIs your favorite algorithm missing? Let me know in the comments below, and I’ll try to incorporate it in this blog post using tidymodels.\n\n   Related articles\n    \n      \n        \n            \n            \n               Tidy Fixed Effects Regressions: fixest vs pyfixest\n               A comparison of packages for fast fixed-effects estimation in R and Python\n            \n        \n        \n        \n        \n        \n            \n            \n                Clustering Binary Data\n                An application of different unsupervised learning approaches to cluster simulated survey responses using R\n            \n        \n        \n        \n        \n        \n            \n            \n                Tidy Collaborative Filtering: Building A Stock Recommender\n                A simple implementation for prototyping multiple collaborative filtering algorithms using R"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html",
    "href": "posts/dplyr-vs-pandas/index.html",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "",
    "text": "There are a myriad of options to perform essential data manipulation tasks in R and Python (see, for instance, my other posts on dplyr vs ibis and dplyr vs polars). However, if we want to do tidy data science in R, there is a clear forerunner: dplyr. In the world of Python, pandas is the most popular data analysis library. In this blog post, I illustrate their syntactic similarities and highlight differences between these two packages that emerge for a few key tasks.\nBefore we dive into the comparison, a short introduction to the packages: the dplyr package in R allows users to refer to columns without quotation marks due to its implementation of non-standard evaluation (NSE). NSE is a programming technique used in R that allows functions to capture the expressions passed to them as arguments, rather than just the values of those arguments. The primary goal of NSE in the context of dplyr is to create a more user-friendly and intuitive syntax. This makes data manipulation tasks more straightforward and aligns with the general philosophy of the tidyverse to make data science faster, easier, and more fun.1\npandas is also designed for data analysis and provides a comprehensive range of functionalities for data manipulation and it is designed to efficiently handle in-memory data. The package has a large community, given Python’s popularity in various fields. The learning curve might be steeper for beginners due to Python’s general-purpose nature and the verbosity of pandas syntax, but it integrates well with web apps, machine learning models, etc."
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#loading-packages-and-data",
    "href": "posts/dplyr-vs-pandas/index.html#loading-packages-and-data",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Loading packages and data",
    "text": "Loading packages and data\nWe start by loading the main packages of interest and the popular palmerpenguins package that exists for both R and Python. We then use the penguins data frame as the data to compare all functions and methods below.\n\ndplyrpandas\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins &lt;- palmerpenguins::penguins\n\n\n\n\nimport pandas as pd\nfrom palmerpenguins import load_penguins\n\npenguins = load_penguins()"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#work-with-rows",
    "href": "posts/dplyr-vs-pandas/index.html#work-with-rows",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Work with rows",
    "text": "Work with rows\n\nFilter rows\nFiltering rows with dplyr is based on NSE and the dplyr::filter() function. To replicate the same results with pandas, you can use pandas.query() method which accepts a string with the filter conditions as input.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  filter(species == \"Adelie\" & \n           island %in% c(\"Biscoe\", \"Dream\"))\n\n# A tibble: 100 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 90 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .query(\"species == 'Adelie' and island in ['Biscoe', 'Dream']\")\n)\n\n    species  island  bill_length_mm  ...  body_mass_g     sex  year\n20   Adelie  Biscoe            37.8  ...       3400.0  female  2007\n21   Adelie  Biscoe            37.7  ...       3600.0    male  2007\n22   Adelie  Biscoe            35.9  ...       3800.0  female  2007\n23   Adelie  Biscoe            38.2  ...       3950.0    male  2007\n24   Adelie  Biscoe            38.8  ...       3800.0    male  2007\n..      ...     ...             ...  ...          ...     ...   ...\n147  Adelie   Dream            36.6  ...       3475.0  female  2009\n148  Adelie   Dream            36.0  ...       3450.0  female  2009\n149  Adelie   Dream            37.8  ...       3750.0    male  2009\n150  Adelie   Dream            36.0  ...       3700.0  female  2009\n151  Adelie   Dream            41.5  ...       4000.0    male  2009\n\n[100 rows x 8 columns]\n\n\n\n\n\n\n\nSlice rows\ndplyr::slice() takes integers with row numbers as inputs, so you can use ranges and arbitrary vectors of integers. pandas.iloc[] also provides a function for integer-location based indexing (note that indexing starts at 0 in Python, while it starts at 1 in R). Note that pandas.iloc[] requires square brackets instead of parentheses.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  slice(10:20)\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           42            20.2               190        4250\n 2 Adelie  Torgersen           37.8          17.1               186        3300\n 3 Adelie  Torgersen           37.8          17.3               180        3700\n 4 Adelie  Torgersen           41.1          17.6               182        3200\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           36.6          17.8               185        3700\n 8 Adelie  Torgersen           38.7          19                 195        3450\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           34.4          18.4               184        3325\n11 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .iloc[9:20]\n)\n\n   species     island  bill_length_mm  ...  body_mass_g     sex  year\n9   Adelie  Torgersen            42.0  ...       4250.0     NaN  2007\n10  Adelie  Torgersen            37.8  ...       3300.0     NaN  2007\n11  Adelie  Torgersen            37.8  ...       3700.0     NaN  2007\n12  Adelie  Torgersen            41.1  ...       3200.0  female  2007\n13  Adelie  Torgersen            38.6  ...       3800.0    male  2007\n14  Adelie  Torgersen            34.6  ...       4400.0    male  2007\n15  Adelie  Torgersen            36.6  ...       3700.0  female  2007\n16  Adelie  Torgersen            38.7  ...       3450.0  female  2007\n17  Adelie  Torgersen            42.5  ...       4500.0    male  2007\n18  Adelie  Torgersen            34.4  ...       3325.0  female  2007\n19  Adelie  Torgersen            46.0  ...       4200.0    male  2007\n\n[11 rows x 8 columns]\n\n\n\n\n\n\n\nArrange rows\nTo orders the rows of a data frame by the values of selected columns, we have dplyr::arrange() and pandas.sort_values(). Note that both approaches arrange rows in an an ascending order and puts missing values last as defaults.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  arrange(island, desc(bill_length_mm))\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .sort_values(by = [\"island\", \"bill_length_mm\"], \n               ascending = [True, False])\n)\n\n    species     island  bill_length_mm  ...  body_mass_g     sex  year\n185  Gentoo     Biscoe            59.6  ...       6050.0    male  2007\n253  Gentoo     Biscoe            55.9  ...       5600.0    male  2009\n267  Gentoo     Biscoe            55.1  ...       5850.0    male  2009\n215  Gentoo     Biscoe            54.3  ...       5650.0    male  2008\n259  Gentoo     Biscoe            53.4  ...       5500.0    male  2009\n..      ...        ...             ...  ...          ...     ...   ...\n80   Adelie  Torgersen            34.6  ...       3200.0  female  2008\n18   Adelie  Torgersen            34.4  ...       3325.0  female  2007\n8    Adelie  Torgersen            34.1  ...       3475.0     NaN  2007\n70   Adelie  Torgersen            33.5  ...       3600.0  female  2008\n3    Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n\n[344 rows x 8 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#work-with-columns",
    "href": "posts/dplyr-vs-pandas/index.html#work-with-columns",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Work with columns",
    "text": "Work with columns\n\nSelect columns\nSelecting a subset of columns works very similarly withdplyr::select() and pandas.get(). The former accepts column names using NSE (or vectors of charaters), while the latter requires a vector of strings with column names as inputs.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  select(bill_length_mm, sex)\n\n# A tibble: 344 × 2\n   bill_length_mm sex   \n            &lt;dbl&gt; &lt;fct&gt; \n 1           39.1 male  \n 2           39.5 female\n 3           40.3 female\n 4           NA   &lt;NA&gt;  \n 5           36.7 female\n 6           39.3 male  \n 7           38.9 female\n 8           39.2 male  \n 9           34.1 &lt;NA&gt;  \n10           42   &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n\n\n(penguins\n  .get([\"bill_length_mm\", \"sex\"])\n)\n\n     bill_length_mm     sex\n0              39.1    male\n1              39.5  female\n2              40.3  female\n3               NaN     NaN\n4              36.7  female\n..              ...     ...\n339            55.8    male\n340            43.5  female\n341            49.6    male\n342            50.8    male\n343            50.2  female\n\n[344 rows x 2 columns]\n\n\n\n\n\n\n\nRenaming columns\nRenaming columns also works very similarly with the major difference that pandas.rename() takes a dictionary with mappings of old to new names as input, while dplyr::rename() takes variable names via the usual NSE.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  rename(bill_length = bill_length_mm,\n         bill_depth = bill_depth_mm)\n\n# A tibble: 344 × 8\n   species island    bill_length bill_depth flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen        39.1       18.7               181        3750 male  \n 2 Adelie  Torgersen        39.5       17.4               186        3800 female\n 3 Adelie  Torgersen        40.3       18                 195        3250 female\n 4 Adelie  Torgersen        NA         NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen        36.7       19.3               193        3450 female\n 6 Adelie  Torgersen        39.3       20.6               190        3650 male  \n 7 Adelie  Torgersen        38.9       17.8               181        3625 female\n 8 Adelie  Torgersen        39.2       19.6               195        4675 male  \n 9 Adelie  Torgersen        34.1       18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen        42         20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .rename(columns = {\"bill_length_mm\": \"bill_length\",\n                     \"bill_depth_mm\" : \"bill_depth\"})\n)\n\n       species     island  bill_length  ...  body_mass_g     sex  year\n0       Adelie  Torgersen         39.1  ...       3750.0    male  2007\n1       Adelie  Torgersen         39.5  ...       3800.0  female  2007\n2       Adelie  Torgersen         40.3  ...       3250.0  female  2007\n3       Adelie  Torgersen          NaN  ...          NaN     NaN  2007\n4       Adelie  Torgersen         36.7  ...       3450.0  female  2007\n..         ...        ...          ...  ...          ...     ...   ...\n339  Chinstrap      Dream         55.8  ...       4000.0    male  2009\n340  Chinstrap      Dream         43.5  ...       3400.0  female  2009\n341  Chinstrap      Dream         49.6  ...       3775.0    male  2009\n342  Chinstrap      Dream         50.8  ...       4100.0    male  2009\n343  Chinstrap      Dream         50.2  ...       3775.0  female  2009\n\n[344 rows x 8 columns]\n\n\n\n\n\n\n\nMutate columns\nTransforming existing columns or creating new ones is an essential part of data analysis. dplyr::mutate() and pandas.assign() are the work horses for these tasks. While dplyr starts with column names before the expressions that transform columns, pandas uses the lambda function to assign expressions to new columns. Note that you have to split up variable assignments if you want to refer to a newly created variable in pandas, while you can refer to the new variables in the same mutate block in dplyr.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  mutate(ones = 1,\n         bill_length = bill_length_mm / 10,\n         bill_length_squared = bill_length^2) |&gt; \n  select(ones, bill_length_mm, bill_length, bill_length_squared)\n\n# A tibble: 344 × 4\n    ones bill_length_mm bill_length bill_length_squared\n   &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1     1           39.1        3.91                15.3\n 2     1           39.5        3.95                15.6\n 3     1           40.3        4.03                16.2\n 4     1           NA         NA                   NA  \n 5     1           36.7        3.67                13.5\n 6     1           39.3        3.93                15.4\n 7     1           38.9        3.89                15.1\n 8     1           39.2        3.92                15.4\n 9     1           34.1        3.41                11.6\n10     1           42          4.2                 17.6\n# ℹ 334 more rows\n\n\n\n\n\n(penguins \n  .assign(ones = 1,\n          bill_length = lambda x: x[\"bill_length_mm\"] / 10)\n  .assign(bill_length_squared = lambda x: x[\"bill_length\"] ** 2)\n  .get([\"ones\", \"bill_length_mm\", \"bill_length\", \"bill_length_squared\"])\n)\n\n     ones  bill_length_mm  bill_length  bill_length_squared\n0       1            39.1         3.91              15.2881\n1       1            39.5         3.95              15.6025\n2       1            40.3         4.03              16.2409\n3       1             NaN          NaN                  NaN\n4       1            36.7         3.67              13.4689\n..    ...             ...          ...                  ...\n339     1            55.8         5.58              31.1364\n340     1            43.5         4.35              18.9225\n341     1            49.6         4.96              24.6016\n342     1            50.8         5.08              25.8064\n343     1            50.2         5.02              25.2004\n\n[344 rows x 4 columns]\n\n\n\n\n\n\n\nRelocate columns\ndplyr::relocate() provides options to change the positions of columns in a data frame, using the same syntax as dplyr::select(). In addition, there are the options .after and .before to provide users with additional shortcuts.\nThe recommended way to relocate columns in pandas is to use the pandas.get() method, but there are no options as in dplyr::relocate(). In fact, the safest way to consistently get the correct order of columns is to explicitly specify them.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  relocate(c(species, bill_length_mm), .before = sex)\n\n# A tibble: 344 × 8\n   island    bill_depth_mm flipper_length_mm body_mass_g species bill_length_mm\n   &lt;fct&gt;             &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1 Torgersen          18.7               181        3750 Adelie            39.1\n 2 Torgersen          17.4               186        3800 Adelie            39.5\n 3 Torgersen          18                 195        3250 Adelie            40.3\n 4 Torgersen          NA                  NA          NA Adelie            NA  \n 5 Torgersen          19.3               193        3450 Adelie            36.7\n 6 Torgersen          20.6               190        3650 Adelie            39.3\n 7 Torgersen          17.8               181        3625 Adelie            38.9\n 8 Torgersen          19.6               195        4675 Adelie            39.2\n 9 Torgersen          18.1               193        3475 Adelie            34.1\n10 Torgersen          20.2               190        4250 Adelie            42  \n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .get([\"island\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\",\n        \"species\", \"bill_length_mm\", \"sex\"])\n)\n\n        island  bill_depth_mm  ...  bill_length_mm     sex\n0    Torgersen           18.7  ...            39.1    male\n1    Torgersen           17.4  ...            39.5  female\n2    Torgersen           18.0  ...            40.3  female\n3    Torgersen            NaN  ...             NaN     NaN\n4    Torgersen           19.3  ...            36.7  female\n..         ...            ...  ...             ...     ...\n339      Dream           19.8  ...            55.8    male\n340      Dream           18.1  ...            43.5  female\n341      Dream           18.2  ...            49.6    male\n342      Dream           19.0  ...            50.8    male\n343      Dream           18.7  ...            50.2  female\n\n[344 rows x 7 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#work-with-groups-of-rows",
    "href": "posts/dplyr-vs-pandas/index.html#work-with-groups-of-rows",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Work with groups of rows",
    "text": "Work with groups of rows\n\nSimple summaries by group\nLet’s suppose we want to compute summaries by groups such as means or medians. Both packages are very similar again: on the R side you have dplyr::group_by() and dplyr::summarize(), while on the Python side you have pandas.groupby() and pandas.agg().\nNote that dplyr::groupby() also automatically arranges the results by the group, so the reproduce the results of dplyr, we need to add pandas.sort() to the chain.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  island    bill_depth_mean\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Biscoe               15.9\n2 Dream                18.3\n3 Torgersen            18.4\n\n\n\n\n\n(penguins\n  .groupby(\"island\")\n  .agg(bill_depth_mean = (\"bill_depth_mm\", \"mean\"))\n)\n\n           bill_depth_mean\nisland                    \nBiscoe           15.874850\nDream            18.344355\nTorgersen        18.429412\n\n\n\n\n\n\n\nMore complicated summaries by group\nTypically, you want to create multiple different summaries by groups. dplyr provides a lot of flexibility to create new variables on the fly, as does pandas. For instance, we can pass expressions to them mean functions in order to create the share of female penguins per island in the summary statement. Note that you again have to use lambda functions in pandas.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(count = n(),\n            bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE),\n            flipper_length_median = median(flipper_length_mm, na.rm = TRUE),\n            body_mass_sd = sd(body_mass_g, na.rm = TRUE),\n            share_female = mean(sex == \"female\", na.rm = TRUE))\n\n# A tibble: 3 × 6\n  island   count bill_depth_mean flipper_length_median body_mass_sd share_female\n  &lt;fct&gt;    &lt;int&gt;           &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Biscoe     168            15.9                   214         783.        0.491\n2 Dream      124            18.3                   193         417.        0.496\n3 Torgers…    52            18.4                   191         445.        0.511\n\n\n\n\n\n(penguins\n  .groupby(\"island\")\n  .agg(count = (\"island\", \"size\"),\n       bill_depth_mean = (\"bill_depth_mm\", \"mean\"),\n       flipper_length_median = (\"flipper_length_mm\", \"median\"),\n       body_mass_sd = (\"body_mass_g\", \"std\"),\n       share_female = (\"sex\", lambda x: (x == \"female\").mean()))\n)\n\n           count  bill_depth_mean  ...  body_mass_sd  share_female\nisland                             ...                            \nBiscoe       168        15.874850  ...    782.855743      0.476190\nDream        124        18.344355  ...    416.644112      0.491935\nTorgersen     52        18.429412  ...    445.107940      0.461538\n\n[3 rows x 5 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#conclusion",
    "href": "posts/dplyr-vs-pandas/index.html#conclusion",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Conclusion",
    "text": "Conclusion\nThis post highlights syntactic similarities and differences across R’s dplyr and Python’s pandas packages. Two key points emerge: (i) dplyr heavily relies on NSE to enable a syntax that refrains from using strings, something that is not possible in Python; (ii) the structure of inputs to pandas methods is inconsistent compared to dplyr (sometimes inputs are vectors of strings, sometimes just a single string, sometimes dictionaries, etc.). I want to close this post by emphasizing that both languages and packages have their own merits and supporters. I personally find it hard to remember the syntax of each pandas method, so I’m much more prone to on-the-fly coding errors than compared to dplyr."
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#footnotes",
    "href": "posts/dplyr-vs-pandas/index.html#footnotes",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the unifying principles of the tidyverse: https://design.tidyverse.org/unifying.html.↩︎"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html",
    "href": "posts/dplyr-vs-polars/index.html",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "",
    "text": "There are a myriad of options to perform essential data manipulation tasks in R and Python (see, for instance, my other posts on dplyr vs ibis and dplyr vs pandas). However, if we want to do tidy data science in R, there is a clear forerunner: dplyr. In the world of Python, polars is a relatively new kid on the block that shares a lot of semantic with dplyr. In this blog post, I illustrate their syntactic similarities and highlight differences between these two packages that emerge for a few key tasks.\nBefore we dive into the comparison, a short introduction to the packages: the dplyr package in R allows users to refer to columns without quotation marks due to its implementation of non-standard evaluation (NSE). NSE is a programming technique used in R that allows functions to capture the expressions passed to them as arguments, rather than just the values of those arguments. The primary goal of NSE in the context of dplyr is to create a more user-friendly and intuitive syntax. This makes data manipulation tasks more straightforward and aligns with the general philosophy of the tidyverse to make data science faster, easier, and more fun.1\npolars is also designed for data manipulation and heavily optimized for performance, but there are significant differences in their approach, especially in how they handle column referencing and expression evaluation. Python generally relies on standard evaluation, meaning expressions are evaluated to their values before being passed to a function. In polars, column references typically need to be explicitly stated, often using quoted names or through methods attached to data frame objects."
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#loading-packages-and-data",
    "href": "posts/dplyr-vs-polars/index.html#loading-packages-and-data",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Loading packages and data",
    "text": "Loading packages and data\nWe start by loading the main packages of interest and the popular palmerpenguins package that exists for both R and Python. We then use the penguins data frame as the data to compare all functions and methods below. Note that we also limit the print output of polars data frames to 10 rows to prevent this post being flooded by excessively long tables.\n\ndplyrpolars\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins &lt;- palmerpenguins::penguins\n\n\n\n\nimport polars as pl\nfrom palmerpenguins import load_penguins\n\npl.Config(tbl_rows = 10)\n\n&lt;polars.config.Config object at 0x168797f70&gt;\n\n\npenguins = load_penguins().pipe(pl.from_pandas)"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#work-with-rows",
    "href": "posts/dplyr-vs-polars/index.html#work-with-rows",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Work with rows",
    "text": "Work with rows\n\nFilter rows\nFiltering rows works very similarly for both packages, they even have the same function names: dplyr::filter() and polars.filter(). To select columns in polars, you need the polars.col() selector.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  filter(species == \"Adelie\" & \n           island %in% c(\"Biscoe\", \"Dream\"))\n\n# A tibble: 100 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 90 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .filter(\n    (pl.col(\"species\") == \"Adelie\") & \n    (pl.col(\"island\").is_in([\"Biscoe\", \"Dream\"]))) \n)\n\n\nshape: (100, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Adelie\"\n\"Biscoe\"\n37.8\n18.3\n174.0\n3400.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Biscoe\"\n37.7\n18.7\n180.0\n3600.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Biscoe\"\n35.9\n19.2\n189.0\n3800.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Biscoe\"\n38.2\n18.1\n185.0\n3950.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Biscoe\"\n38.8\n17.2\n180.0\n3800.0\n\"male\"\n2007\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Adelie\"\n\"Dream\"\n36.6\n18.4\n184.0\n3475.0\n\"female\"\n2009\n\n\n\"Adelie\"\n\"Dream\"\n36.0\n17.8\n195.0\n3450.0\n\"female\"\n2009\n\n\n\"Adelie\"\n\"Dream\"\n37.8\n18.1\n193.0\n3750.0\n\"male\"\n2009\n\n\n\"Adelie\"\n\"Dream\"\n36.0\n17.1\n187.0\n3700.0\n\"female\"\n2009\n\n\n\"Adelie\"\n\"Dream\"\n41.5\n18.5\n201.0\n4000.0\n\"male\"\n2009\n\n\n\n\n\n\n\n\n\n\n\nSlice rows\ndplyr::slice() takes integers with row numbers as inputs, so you can use ranges and arbitrary vectors of integers. polars.slice() only takes the start index and the length of the slice as inputs. For instance, to the the same result of slicing rows 10 to 20, the code looks as follows (note that indexing starts at 0 in Python, while it starts at 1 in R):\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  slice(10:20)\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           42            20.2               190        4250\n 2 Adelie  Torgersen           37.8          17.1               186        3300\n 3 Adelie  Torgersen           37.8          17.3               180        3700\n 4 Adelie  Torgersen           41.1          17.6               182        3200\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           36.6          17.8               185        3700\n 8 Adelie  Torgersen           38.7          19                 195        3450\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           34.4          18.4               184        3325\n11 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .slice(9, 11)  \n)\n\n\nshape: (11, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Adelie\"\n\"Torgersen\"\n42.0\n20.2\n190.0\n4250.0\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n37.8\n17.1\n186.0\n3300.0\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n37.8\n17.3\n180.0\n3700.0\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n41.1\n17.6\n182.0\n3200.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n38.6\n21.2\n191.0\n3800.0\n\"male\"\n2007\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Adelie\"\n\"Torgersen\"\n36.6\n17.8\n185.0\n3700.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n38.7\n19.0\n195.0\n3450.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n42.5\n20.7\n197.0\n4500.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n34.4\n18.4\n184.0\n3325.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n46.0\n21.5\n194.0\n4200.0\n\"male\"\n2007\n\n\n\n\n\n\n\n\n\n\n\nArrange rows\nTo orders the rows of a data frame by the values of selected columns, we have dplyr::arrange() and polars.sort(). Note that dplyr::arrange() arranges rows in an an ascending order and puts NA values last. polars.sort(), on the other hand, arranges rows in an ascending order and starts with null as default. Note that there are options to control these defaults.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  arrange(island, desc(bill_length_mm))\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .sort([\"island\", \"bill_length_mm\"], \n        descending=[False, True], nulls_last=True)\n)\n\n\nshape: (344, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Gentoo\"\n\"Biscoe\"\n59.6\n17.0\n230.0\n6050.0\n\"male\"\n2007\n\n\n\"Gentoo\"\n\"Biscoe\"\n55.9\n17.0\n228.0\n5600.0\n\"male\"\n2009\n\n\n\"Gentoo\"\n\"Biscoe\"\n55.1\n16.0\n230.0\n5850.0\n\"male\"\n2009\n\n\n\"Gentoo\"\n\"Biscoe\"\n54.3\n15.7\n231.0\n5650.0\n\"male\"\n2008\n\n\n\"Gentoo\"\n\"Biscoe\"\n53.4\n15.8\n219.0\n5500.0\n\"male\"\n2009\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Adelie\"\n\"Torgersen\"\n34.6\n17.2\n189.0\n3200.0\n\"female\"\n2008\n\n\n\"Adelie\"\n\"Torgersen\"\n34.4\n18.4\n184.0\n3325.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n34.1\n18.1\n193.0\n3475.0\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n33.5\n19.0\n190.0\n3600.0\n\"female\"\n2008\n\n\n\"Adelie\"\n\"Torgersen\"\nnull\nnull\nnull\nnull\nnull\n2007"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#work-with-columns",
    "href": "posts/dplyr-vs-polars/index.html#work-with-columns",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Work with columns",
    "text": "Work with columns\n\nSelect columns\nSelecting a subset of columns works essentially the same for both and dplyr::select() and polars.select() even have the same name. Note that you don’t have to use polars.col() but can just pass strings in the polars.select() method.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  select(bill_length_mm, sex)\n\n# A tibble: 344 × 2\n   bill_length_mm sex   \n            &lt;dbl&gt; &lt;fct&gt; \n 1           39.1 male  \n 2           39.5 female\n 3           40.3 female\n 4           NA   &lt;NA&gt;  \n 5           36.7 female\n 6           39.3 male  \n 7           38.9 female\n 8           39.2 male  \n 9           34.1 &lt;NA&gt;  \n10           42   &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n\n\n(penguins\n  .select(pl.col(\"bill_length_mm\"), pl.col(\"sex\"))\n)\n\n\nshape: (344, 2)\n\n\n\nbill_length_mm\nsex\n\n\nf64\nstr\n\n\n\n\n39.1\n\"male\"\n\n\n39.5\n\"female\"\n\n\n40.3\n\"female\"\n\n\nnull\nnull\n\n\n36.7\n\"female\"\n\n\n…\n…\n\n\n55.8\n\"male\"\n\n\n43.5\n\"female\"\n\n\n49.6\n\"male\"\n\n\n50.8\n\"male\"\n\n\n50.2\n\"female\"\n\n\n\n\n\n\n\n\n\n\n\nRename columns\nRenaming columns also works very similarly with the major difference that polars.rename() takes a dictionary with mappings of old to new names as input, while dplyr::rename() takes variable names via the usual NSE.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  rename(bill_length = bill_length_mm,\n         bill_depth = bill_depth_mm)\n\n# A tibble: 344 × 8\n   species island    bill_length bill_depth flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen        39.1       18.7               181        3750 male  \n 2 Adelie  Torgersen        39.5       17.4               186        3800 female\n 3 Adelie  Torgersen        40.3       18                 195        3250 female\n 4 Adelie  Torgersen        NA         NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen        36.7       19.3               193        3450 female\n 6 Adelie  Torgersen        39.3       20.6               190        3650 male  \n 7 Adelie  Torgersen        38.9       17.8               181        3625 female\n 8 Adelie  Torgersen        39.2       19.6               195        4675 male  \n 9 Adelie  Torgersen        34.1       18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen        42         20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .rename({\"bill_length_mm\": \"bill_length\",\n           \"bill_depth_mm\" : \"bill_depth\"})\n)\n\n\nshape: (344, 8)\n\n\n\nspecies\nisland\nbill_length\nbill_depth\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Adelie\"\n\"Torgersen\"\n39.1\n18.7\n181.0\n3750.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n39.5\n17.4\n186.0\n3800.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n40.3\n18.0\n195.0\n3250.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\nnull\nnull\nnull\nnull\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n36.7\n19.3\n193.0\n3450.0\n\"female\"\n2007\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Chinstrap\"\n\"Dream\"\n55.8\n19.8\n207.0\n4000.0\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n43.5\n18.1\n202.0\n3400.0\n\"female\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n49.6\n18.2\n193.0\n3775.0\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.8\n19.0\n210.0\n4100.0\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.2\n18.7\n198.0\n3775.0\n\"female\"\n2009\n\n\n\n\n\n\n\n\n\n\n\nMutate columns\nTransforming existing columns or creating new ones is an essential part of data analysis. dplyr::mutate() and polars.with_columns() are the work horses for these tasks. Both approaches have a very similar syntax. Note that you have to split up variable assignments if you want to refer to a newly created variable in polars, while you can refer to the new variables in the same mutate block in dplyr.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  mutate(ones = 1,\n         bill_length = bill_length_mm / 10,\n         bill_length_squared = bill_length^2) |&gt; \n  select(ones, bill_length_mm, bill_length, bill_length_squared)\n\n# A tibble: 344 × 4\n    ones bill_length_mm bill_length bill_length_squared\n   &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1     1           39.1        3.91                15.3\n 2     1           39.5        3.95                15.6\n 3     1           40.3        4.03                16.2\n 4     1           NA         NA                   NA  \n 5     1           36.7        3.67                13.5\n 6     1           39.3        3.93                15.4\n 7     1           38.9        3.89                15.1\n 8     1           39.2        3.92                15.4\n 9     1           34.1        3.41                11.6\n10     1           42          4.2                 17.6\n# ℹ 334 more rows\n\n\n\n\n\n(penguins \n  .with_columns(ones = pl.lit(1),\n                bill_length = pl.col(\"bill_length_mm\") / 10)\n  .with_columns(bill_length_squared = pl.col(\"bill_length\") ** 2)\n  .select(pl.col(\"ones\"), pl.col(\"bill_length_mm\"),  \n          pl.col(\"bill_length\"), pl.col(\"bill_length_squared\"))\n)\n\n\nshape: (344, 4)\n\n\n\nones\nbill_length_mm\nbill_length\nbill_length_squared\n\n\ni32\nf64\nf64\nf64\n\n\n\n\n1\n39.1\n3.91\n15.2881\n\n\n1\n39.5\n3.95\n15.6025\n\n\n1\n40.3\n4.03\n16.2409\n\n\n1\nnull\nnull\nnull\n\n\n1\n36.7\n3.67\n13.4689\n\n\n…\n…\n…\n…\n\n\n1\n55.8\n5.58\n31.1364\n\n\n1\n43.5\n4.35\n18.9225\n\n\n1\n49.6\n4.96\n24.6016\n\n\n1\n50.8\n5.08\n25.8064\n\n\n1\n50.2\n5.02\n25.2004\n\n\n\n\n\n\n\n\n\n\n\nRelocate columns\ndplyr::relocate() provides options to change the positions of columns in a data frame, using the same syntax as dplyr::select(). In addition, there are the options .after and .before to provide users with additional shortcuts.\nThe recommended way to relocate columns in polars is to use the polars.select() method, but there are no options as in dplyr::relocate(). In fact, the safest way to consistently get the correct order of columns is to explicitly specify them.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  relocate(c(species, bill_length_mm), .before = sex)\n\n# A tibble: 344 × 8\n   island    bill_depth_mm flipper_length_mm body_mass_g species bill_length_mm\n   &lt;fct&gt;             &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1 Torgersen          18.7               181        3750 Adelie            39.1\n 2 Torgersen          17.4               186        3800 Adelie            39.5\n 3 Torgersen          18                 195        3250 Adelie            40.3\n 4 Torgersen          NA                  NA          NA Adelie            NA  \n 5 Torgersen          19.3               193        3450 Adelie            36.7\n 6 Torgersen          20.6               190        3650 Adelie            39.3\n 7 Torgersen          17.8               181        3625 Adelie            38.9\n 8 Torgersen          19.6               195        4675 Adelie            39.2\n 9 Torgersen          18.1               193        3475 Adelie            34.1\n10 Torgersen          20.2               190        4250 Adelie            42  \n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .select(pl.col(\"island\"), pl.col(\"bill_depth_mm\"), \n          pl.col(\"flipper_length_mm\"), pl.col(\"body_mass_g\"), \n          pl.col(\"species\"), pl.col(\"bill_length_mm\"), pl.col(\"sex\"))\n)\n\n\nshape: (344, 7)\n\n\n\nisland\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nspecies\nbill_length_mm\nsex\n\n\nstr\nf64\nf64\nf64\nstr\nf64\nstr\n\n\n\n\n\"Torgersen\"\n18.7\n181.0\n3750.0\n\"Adelie\"\n39.1\n\"male\"\n\n\n\"Torgersen\"\n17.4\n186.0\n3800.0\n\"Adelie\"\n39.5\n\"female\"\n\n\n\"Torgersen\"\n18.0\n195.0\n3250.0\n\"Adelie\"\n40.3\n\"female\"\n\n\n\"Torgersen\"\nnull\nnull\nnull\n\"Adelie\"\nnull\nnull\n\n\n\"Torgersen\"\n19.3\n193.0\n3450.0\n\"Adelie\"\n36.7\n\"female\"\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Dream\"\n19.8\n207.0\n4000.0\n\"Chinstrap\"\n55.8\n\"male\"\n\n\n\"Dream\"\n18.1\n202.0\n3400.0\n\"Chinstrap\"\n43.5\n\"female\"\n\n\n\"Dream\"\n18.2\n193.0\n3775.0\n\"Chinstrap\"\n49.6\n\"male\"\n\n\n\"Dream\"\n19.0\n210.0\n4100.0\n\"Chinstrap\"\n50.8\n\"male\"\n\n\n\"Dream\"\n18.7\n198.0\n3775.0\n\"Chinstrap\"\n50.2\n\"female\""
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#work-with-groups-of-rows",
    "href": "posts/dplyr-vs-polars/index.html#work-with-groups-of-rows",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Work with groups of rows",
    "text": "Work with groups of rows\n\nSimple summaries by group\nLet’s suppose we want to compute summaries by groups such as means or medians. Both packages are very similar again: on the R side you have dplyr::group_by() and dplyr::summarize(), while on the Python side you have polars.group_by() and polars.agg().\nNote that dplyr::group_by() also automatically arranges the results by the group, so the reproduce the results of dplyr, we need to add polars.sort() to the chain.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  island    bill_depth_mean\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Biscoe               15.9\n2 Dream                18.3\n3 Torgersen            18.4\n\n\n\n\n\n(penguins\n  .group_by(\"island\")\n  .agg(bill_depth_mean = pl.mean(\"bill_depth_mm\"))\n  .sort(\"island\")\n)\n\n\nshape: (3, 2)\n\n\n\nisland\nbill_depth_mean\n\n\nstr\nf64\n\n\n\n\n\"Biscoe\"\n15.87485\n\n\n\"Dream\"\n18.344355\n\n\n\"Torgersen\"\n18.429412\n\n\n\n\n\n\n\n\n\n\n\nMore complicated summaries by group\nTypically, you want to create multiple different summaries by groups. dplyr provides a lot of flexibility to create new variables on the fly, while polars seems to be a bit more restrictive. For instance, to compute the share of female penguins by group, it makes more sense to create an ìs_female indicator column using polars because polars.mean() does not accept expressions as inputs.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(count = n(),\n            bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE),\n            flipper_length_median = median(flipper_length_mm, na.rm = TRUE),\n            body_mass_sd = sd(body_mass_g, na.rm = TRUE),\n            share_female = mean(sex == \"female\", na.rm = TRUE))\n\n# A tibble: 3 × 6\n  island   count bill_depth_mean flipper_length_median body_mass_sd share_female\n  &lt;fct&gt;    &lt;int&gt;           &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Biscoe     168            15.9                   214         783.        0.491\n2 Dream      124            18.3                   193         417.        0.496\n3 Torgers…    52            18.4                   191         445.        0.511\n\n\n\n\n\n(penguins\n  .with_columns(is_female = pl.when(pl.col(\"sex\") == \"female\").then(1)\n                  .when(pl.col(\"sex\").is_null()).then(None)\n                  .otherwise(0))\n  .group_by(\"island\")\n  .agg(\n    count = pl.count(),\n    bill_depth_mean = pl.mean(\"bill_depth_mm\"),\n    flipper_length_median = pl.median(\"flipper_length_mm\"),\n    body_mass_sd = pl.std(\"body_mass_g\"),\n    share_female = pl.mean(\"is_female\")\n  )\n  .sort(\"island\")\n)\n\n\nshape: (3, 6)\n\n\n\nisland\ncount\nbill_depth_mean\nflipper_length_median\nbody_mass_sd\nshare_female\n\n\nstr\nu32\nf64\nf64\nf64\nf64\n\n\n\n\n\"Biscoe\"\n168\n15.87485\n214.0\n782.855743\n0.490798\n\n\n\"Dream\"\n124\n18.344355\n193.0\n416.644112\n0.495935\n\n\n\"Torgersen\"\n52\n18.429412\n191.0\n445.10794\n0.510638"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#conclusion",
    "href": "posts/dplyr-vs-polars/index.html#conclusion",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Conclusion",
    "text": "Conclusion\nThis post highlights syntactic similarities and differences across R’s dplyr and Python’s polars packages. One key point emerges: dplyr heavily relies on NSE to enable a syntax that refrains from using strings and column selectors, something that is not possible in Python. I want to close this post by emphasizing that both languages and packages have their own merits and I won’t strictly recommend one over the other - maybe in another post 😄"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#footnotes",
    "href": "posts/dplyr-vs-polars/index.html#footnotes",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the unifying principles of the tidyverse: https://design.tidyverse.org/unifying.html.↩︎"
  },
  {
    "objectID": "posts/fixed-effects-regressions/index.html",
    "href": "posts/fixed-effects-regressions/index.html",
    "title": "Tidy Fixed Effects Regressions: fixest vs pyfixest",
    "section": "",
    "text": "Fixed effects regressions are crucial in econometrics for controlling for unobservable variables that vary across entities but are constant over time. They allow researchers to isolate the impact of independent variables on the dependent variable, which is for instance, essential for educing omitted variable bias in causal inference.\nThe fixest R package is a powerful and efficient tool for econometric analysis, specializing in fixed effects models. It excels in handling large datasets and supports a wide range of regression models including linear, count, and logistic models. Its key strength lies in the fast estimation of models with multiple fixed effects, crucial for controlling unobserved heterogeneity in econometric analysis. With a user-friendly syntax, it integrates well with the tidyverse, facilitating easy data manipulation and visualization. fixest offers robust features for hypothesis testing, clustering standard errors, and instrumental variable estimation, all backed by C++ code for speed.\npyfixest is a Python implementation of the fixest package with the goal to mimic fixest syntax and functionality as closely as Python allows. The great thing is, if you know either package, then you can switch between R and Python (almost) seamlessly!\nTo illustrate both packages, I use real-world data and a meaningful example: we download annual country-level indicators and run various regressions, accounting for different types of unobserved heterogeneity. Don’t take the results to seriously, though, as this post serves an illustrative purpose and is not part of a proper research project."
  },
  {
    "objectID": "posts/fixed-effects-regressions/index.html#loading-packages",
    "href": "posts/fixed-effects-regressions/index.html#loading-packages",
    "title": "Tidy Fixed Effects Regressions: fixest vs pyfixest",
    "section": "Loading packages",
    "text": "Loading packages\nFor data manipulation, we use dplyr in R and pandas in Python. We also load the WDI package and wbdata library to download World Development Indicators (WDI) from the World Bank database.\nThe WDI dataset provides a comprehensive set of economic development data, including social, economic, financial, natural resources, and environmental indicators for over 200 countries over many years. This data is excellent to illustrate country-level fixed effects regressions.\n\nRPython\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(WDI)\nlibrary(fixest)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to load pyfixest in RStudio using reticulate on Mac, then you might run into a very peculiar error: https://github.com/rstudio/rstudio/issues/13967. However, pyfixest should run in Jupyter or Python console in any case.\n\n\n\nimport numpy as np\nimport pandas as pd\n\nfrom wbdata import get_dataframe\nfrom pyfixest.estimation import feols\nfrom pyfixest.summarize import etable"
  },
  {
    "objectID": "posts/fixed-effects-regressions/index.html#downloading-and-preparing-data",
    "href": "posts/fixed-effects-regressions/index.html#downloading-and-preparing-data",
    "title": "Tidy Fixed Effects Regressions: fixest vs pyfixest",
    "section": "Downloading and preparing data",
    "text": "Downloading and preparing data\nWe want to estimate a simple model where we use GDP per capita as the dependent variable and education expenditure, health expenditure, and CO2 emissions as independent variables. We divide the education and health expenditure shares by 100 to get actual percentages and we log GDP per capita and CO2 emissions to reduce the impact of their skewed distributions.\n\nRPython\n\n\n\nindicators &lt;- c(\n  \"gdp_per_capita\" = \"NY.GDP.PCAP.KD\",\n  \"edu_exp_share\" = \"SE.XPD.TOTL.GD.ZS\",\n  \"health_exp_share\" = \"SH.XPD.CHEX.GD.ZS\",\n  \"co2_emissions\" = \"EN.ATM.CO2E.PC\",\n  \"age_dependency_ratio\" = \"SP.POP.DPND\"\n)  \n\nwdi_data &lt;- WDI(\n  indicators, country = \"all\", start = 2002, end = 2022,\n) \n\nwdi_data &lt;- as_tibble(wdi_data) |&gt; \n  mutate(\n    log_gdp_per_capita = log(gdp_per_capita),\n    edu_exp_share = edu_exp_share / 100,\n    health_exp_share = health_exp_share / 100,\n    log_co2_emissions = log(co2_emissions),\n    age_dependency_ratio = age_dependency_ratio / 100\n  ) |&gt; \n  drop_na(log_gdp_per_capita, edu_exp_share,\n          health_exp_share, log_co2_emissions)\n\n\n\n\nindicators = {\n  \"NY.GDP.PCAP.KD\": \"gdp_per_capita\",\n  \"SE.XPD.TOTL.GD.ZS\": \"edu_exp_share\",\n  \"SH.XPD.CHEX.GD.ZS\": \"health_exp_share\",\n  \"EN.ATM.CO2E.PC\": \"co2_emissions\",\n  \"SP.POP.DPND\": \"age_dependency_ratio\"\n}\n\nwdi_data = (get_dataframe(indicators, date = (\"2002\", \"2022\"))\n  .reset_index()\n  .rename(columns = {\"date\": \"year\"})\n)\n\nwdi_data = (pd.DataFrame(wdi_data)\n  .assign(\n    log_gdp_per_capita = lambda x: np.log(x[\"gdp_per_capita\"]),\n    edu_exp_share = lambda x: x[\"edu_exp_share\"] / 100,\n    health_exp_share = lambda x: x[\"health_exp_share\"] / 100,\n    log_co2_emissions = lambda x: np.log(x[\"co2_emissions\"]),\n    age_dependency_ratio = lambda x: x[\"age_dependency_ratio\"] / 100\n  )\n  .dropna(subset = [\"log_gdp_per_capita\", \"edu_exp_share\",\n                    \"health_exp_share\", \"log_co2_emissions\",\n                    \"age_dependency_ratio\"])\n)"
  },
  {
    "objectID": "posts/fixed-effects-regressions/index.html#linear-regressions",
    "href": "posts/fixed-effects-regressions/index.html#linear-regressions",
    "title": "Tidy Fixed Effects Regressions: fixest vs pyfixest",
    "section": "Linear regressions",
    "text": "Linear regressions\nWe focus on linear regressions with fixed effects in the examples below because these models are the most common and simple ones. By using fixed effects, the model controls for all time-invariant differences between the countries, which means that any unobserved factors that do not change over time and that could influence the dependent variable (GDP per capita) are accounted for. This significantly reduces the bias in the estimated coefficients of the independent variables, leading to more reliable and interpretable results.\n\nRPython\n\n\n\nfe_model &lt;- feols(\n  log_gdp_per_capita ~ edu_exp_share + health_exp_share + log_co2_emissions | country + year, \n  data = wdi_data,\n  vcov = \"iid\")\n\nsummary(fe_model)\n\nOLS estimation, Dep. Var.: log_gdp_per_capita\nObservations: 3,387 \nFixed-effects: country: 226,  year: 19\nStandard-errors: IID \n                   Estimate Std. Error  t value   Pr(&gt;|t|)    \nedu_exp_share     -0.784350   0.226210 -3.46736 5.3266e-04 ***\nhealth_exp_share  -1.006972   0.196732 -5.11851 3.2650e-07 ***\nlog_co2_emissions  0.280575   0.008417 33.33364  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.082722     Adj. R2: 0.996154\n                 Within R2: 0.278266\n\n\n\n\n\nfe_model = feols(\n  \"log_gdp_per_capita ~ edu_exp_share + health_exp_share + log_co2_emissions | country + year\", \n  data = wdi_data,\n  vcov = \"iid\")\n  \nfe_model.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: log_gdp_per_capita, Fixed effects: country+year\nInference:  iid\nObservations:  3387\n\n| Coefficient       |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5 % |   97.5 % |\n|:------------------|-----------:|-------------:|----------:|-----------:|--------:|---------:|\n| edu_exp_share     |     -0.784 |        0.218 |    -3.600 |      0.000 |  -1.212 |   -0.357 |\n| health_exp_share  |     -1.007 |        0.190 |    -5.314 |      0.000 |  -1.379 |   -0.635 |\n| log_co2_emissions |      0.281 |        0.008 |    34.605 |      0.000 |   0.265 |    0.296 |\n---\nRMSE: 0.083   R2: 0.996   R2 Within: 0.278"
  },
  {
    "objectID": "posts/fixed-effects-regressions/index.html#clustering-standard-errors",
    "href": "posts/fixed-effects-regressions/index.html#clustering-standard-errors",
    "title": "Tidy Fixed Effects Regressions: fixest vs pyfixest",
    "section": "Clustering standard errors",
    "text": "Clustering standard errors\nStandard errors are a critical part of each FE estimation, in particular when you want to draw inference. Which standard errors to pick depends on your setting and convention in your field. fixest comes with many different implementations of standard errors as you can see in the documentation here.\nIn financial economics (where I come from), you typically use clustered standard errors. So in the application below, I cluster standard errors by country. We already see a significant drop in \\(t\\)-statistics through clustering compared to the iid results from above.\n\nRPython\n\n\n\nfe_model_clustered = feols(\n  log_gdp_per_capita ~ edu_exp_share + health_exp_share + log_co2_emissions | country + year, \n  data = wdi_data,\n  vcov = ~country)\n\nsummary(fe_model_clustered)\n\nOLS estimation, Dep. Var.: log_gdp_per_capita\nObservations: 3,387 \nFixed-effects: country: 226,  year: 19\nStandard-errors: Clustered (country) \n                   Estimate Std. Error  t value  Pr(&gt;|t|)    \nedu_exp_share     -0.784350   0.421681 -1.86005  0.064183 .  \nhealth_exp_share  -1.006972   0.622018 -1.61888  0.106874    \nlog_co2_emissions  0.280575   0.028154  9.96560 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.082722     Adj. R2: 0.996154\n                 Within R2: 0.278266\n\n\n\n\n\nfe_model_clustered = feols(\n  \"log_gdp_per_capita ~ edu_exp_share + health_exp_share + log_co2_emissions | country + year\", \n  data = wdi_data,\n  vcov = {\"CRV1\": \"country\"})\n  \nfe_model_clustered.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: log_gdp_per_capita, Fixed effects: country+year\nInference:  CRV1\nObservations:  3387\n\n| Coefficient       |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5 % |   97.5 % |\n|:------------------|-----------:|-------------:|----------:|-----------:|--------:|---------:|\n| edu_exp_share     |     -0.784 |        0.420 |    -1.865 |      0.063 |  -1.613 |    0.044 |\n| health_exp_share  |     -1.007 |        0.620 |    -1.623 |      0.106 |  -2.229 |    0.215 |\n| log_co2_emissions |      0.281 |        0.028 |     9.994 |      0.000 |   0.225 |    0.336 |\n---\nRMSE: 0.083   R2: 0.996   R2 Within: 0.278"
  },
  {
    "objectID": "posts/fixed-effects-regressions/index.html#multiple-models",
    "href": "posts/fixed-effects-regressions/index.html#multiple-models",
    "title": "Tidy Fixed Effects Regressions: fixest vs pyfixest",
    "section": "Multiple models",
    "text": "Multiple models\nIt usually makes sense to run multiple models with various specifications to better understand the impact of fixed effects on coefficient estimates. fixest and pyfixest come with an amazing set of tools to support multiple estimations with minimal syntax (see the documentation here).\nIn the example below, we run the regression from above but iteratively add fixed effects using the csw0() helper function. Check out the documentation for more helpers in this direction. Note that I generally prefer looking at \\(t\\)-stats rather than standard errors because the latter are typically very hard to interpret across coefficients that vary in size. The \\(t\\)-statistics provide a consistent way to interpret changes in estimation uncertainty across different model specifications. Unfortunately, pyfixest does not yet support \\(t\\)-stats in etable(), but the feature will come soon (see this issue).\n\nRPython\n\n\n\nfe_models = feols(\n  log_gdp_per_capita ~ edu_exp_share + health_exp_share + log_co2_emissions | csw0(country, year), \n  data = wdi_data,\n  vcov = ~country)\netable(fe_models, coefstat = \"tstat\", digits = 3, digits.stats = 3)\n\n                         fe_models.1        fe_models.2        fe_models.3\nDependent Var.:   log_gdp_per_capita log_gdp_per_capita log_gdp_per_capita\n                                                                          \nConstant              7.39*** (60.4)                                      \nedu_exp_share          -2.67 (-1.04)    -0.215 (-0.333)    -0.784. (-1.86)\nhealth_exp_share      11.5*** (7.26)      3.01** (3.14)      -1.01 (-1.62)\nlog_co2_emissions    0.803*** (29.7)    0.441*** (11.1)    0.281*** (9.96)\nFixed-Effects:    ------------------ ------------------ ------------------\ncountry                           No                Yes                Yes\nyear                              No                 No                Yes\n_________________ __________________ __________________ __________________\nVCOV: Clustered          by: country        by: country        by: country\nObservations                   3,387              3,387              3,387\nR2                             0.828              0.992              0.996\nWithin R2                         --              0.315              0.278\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nfe_models = feols(\n  \"log_gdp_per_capita ~ edu_exp_share + health_exp_share + log_co2_emissions | csw0(country, year)\", \n  data = wdi_data,\n  vcov = {\"CRV1\": \"country\"})\n  \netable([fe_models.fetch_model(j) for j in range(len(fe_models.all_fitted_models))],\n       digits = 3)\n\nModel:  log_gdp_per_capita~edu_exp_share+health_exp_share+log_co2_emissions\nModel:  log_gdp_per_capita~edu_exp_share+health_exp_share+log_co2_emissions|country\nModel:  log_gdp_per_capita~edu_exp_share+health_exp_share+log_co2_emissions|country+year\n                                 est1                est2                est3\n-----------------  ------------------  ------------------  ------------------\ndepvar             log_gdp_per_capita  log_gdp_per_capita  log_gdp_per_capita\n-----------------------------------------------------------------------------\nIntercept            7.388*** (0.122)\nedu_exp_share          -2.673 (2.558)      -0.215 (0.647)       -0.784 (0.42)\nhealth_exp_share     11.543*** (1.59)     3.005** (0.956)       -1.007 (0.62)\nlog_co2_emissions    0.803*** (0.027)     0.441*** (0.04)    0.281*** (0.028)\n-----------------------------------------------------------------------------\ncountry                             -                   x                   x\nyear                                -                   -                   x\n-----------------------------------------------------------------------------\nR2                              0.828               0.992               0.996\nS.E. type                 by: country         by: country         by: country\nObservations                     3387                3387                3387\n-----------------------------------------------------------------------------\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "posts/fixed-effects-regressions/index.html#instrumental-variables",
    "href": "posts/fixed-effects-regressions/index.html#instrumental-variables",
    "title": "Tidy Fixed Effects Regressions: fixest vs pyfixest",
    "section": "Instrumental variables",
    "text": "Instrumental variables\nLet’s consider an economic model where we are interested in the impact of education expenditure (as a percentage of GDP) on GDP per capita growth. The challenge is that education expenditure might be endogenous; countries might allocate their spending based on expected future GDP growth, or there might be omitted variables affecting both.\nA good instrument in this context could be something that influences education expenditure but does not directly affect GDP per capita growth except through its impact on education. An example might be the age dependency ratio outside the working age population (% of working-age population), under the assumption that countries with a higher dependency ratio might prioritize education differently, affecting their education expenditures without directly influencing GDP growth except through this channel.\nThe examples below show to implement IV regressions. Unfortunately, our model does not indicate any statistically meaningful relationship for our channel, but that’s fine as its purpose is illustrative anyway.\n\nRPython\n\n\n\niv_model &lt;- feols(\n  log_gdp_per_capita ~ health_exp_share + log_co2_emissions | country + year | edu_exp_share ~ age_dependency_ratio,\n  data = wdi_data,\n  vcov = ~country)\n\nsummary(iv_model)\n\nTSLS estimation, Dep. Var.: log_gdp_per_capita, Endo.: edu_exp_share, Instr.: age_dependency_ratio\nSecond stage: Dep. Var.: log_gdp_per_capita\nObservations: 3,387 \nFixed-effects: country: 226,  year: 19\nStandard-errors: Clustered (country) \n                   Estimate Std. Error   t value   Pr(&gt;|t|)    \nfit_edu_exp_share 17.987284  26.101642  0.689125 0.49145486    \nhealth_exp_share  -1.527878   1.466760 -1.041668 0.29868341    \nlog_co2_emissions  0.238962   0.068493  3.488869 0.00058341 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.147817     Adj. R2: 0.987721\n                 Within R2: -1.30454\nF-test (1st stage), edu_exp_share: stat =  7.13703, p = 0.007587, on 1 and 3,365 DoF.\n                       Wu-Hausman: stat = 14.7    , p = 1.306e-4, on 1 and 3,139 DoF.\n\n\n\n\n\niv_model = feols(\n  \"log_gdp_per_capita ~ health_exp_share + log_co2_emissions | country + year | edu_exp_share ~ age_dependency_ratio\",\n  data = wdi_data,\n  vcov = {\"CRV1\": \"country\"})\n  \niv_model.summary()\n\n###\n\nEstimation:  IV\nDep. var.: log_gdp_per_capita, Fixed effects: country+year\nInference:  CRV1\nObservations:  3387\n\n| Coefficient       |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5 % |   97.5 % |\n|:------------------|-----------:|-------------:|----------:|-----------:|--------:|---------:|\n| edu_exp_share     |     17.987 |       26.028 |     0.691 |      0.490 | -33.303 |   69.278 |\n| health_exp_share  |     -1.528 |        1.463 |    -1.045 |      0.297 |  -4.410 |    1.354 |\n| log_co2_emissions |      0.239 |        0.068 |     3.499 |      0.001 |   0.104 |    0.374 |\n---"
  },
  {
    "objectID": "posts/fixed-effects-regressions/index.html#conclusion",
    "href": "posts/fixed-effects-regressions/index.html#conclusion",
    "title": "Tidy Fixed Effects Regressions: fixest vs pyfixest",
    "section": "Conclusion",
    "text": "Conclusion\nI am a big fan of fixest and its sibling pyfixest, in particular the simple and powerful syntax are winning features. The summary tables are also concise and focus on the most relevant information. I think these packages are ideal for economists and data scientists who require a reliable and efficient solution for complex econometric models."
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html",
    "href": "posts/ggplot2-vs-seaborn/index.html",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "",
    "text": "ggplot2 is based on Leland Wilkinson”s Grammar of Graphics, a set of principles for creating consistent and effective statistical graphics, and was developed by Hadley Wickham. The package is a cornerstone of the R community and integrates seamlessly with other tidyverse packages. One of the key strengths of ggplot2 is its use of a consistent syntax, making it relatively easy to learn and enabling users to create a wide range of graphics with a common set of functions. The package is also highly customizable, allowing detailed adjustments to almost every element of a plot.\nseaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Key features of seaborn include its ability to work well with pandas data frames, built-in themes for styling matplotlib graphics, and functions for visualizing distributions.\nThe types of plots that I chose for the comparison heavily draw on the examples given in R for Data Science - an amazing resource if you want to get started with data visualization."
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#loading-packages-and-data",
    "href": "posts/ggplot2-vs-seaborn/index.html#loading-packages-and-data",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "Loading packages and data",
    "text": "Loading packages and data\nWe start by loading the main packages of interest and the popular penguins data that comes with seaborn and exists as an R package. We then use the penguins data frame as the data to compare all functions and methods below. Note that I drop all rows with missing values because I don’t want to get into related messages in this post.\n\nggplot2seaborn\n\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\npenguins &lt;- na.omit(palmerpenguins::penguins)\n\n\n\n\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\n\nsns.set_theme(style = \"whitegrid\")"
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#a-full-blown-example",
    "href": "posts/ggplot2-vs-seaborn/index.html#a-full-blown-example",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "A full-blown example",
    "text": "A full-blown example\nLet”s start with an advancved example that combines many different aesthetics at the same time: we plot two columns against each other, use color and shape aesthetics do differentiate species, include separate regression lines for each species, manually set nice labels, and use a theme. As you can see in this example already, ggplot2 and seaborn have a fundamentally different syntactic approach. While ggplot2 works with layers, seaborn uses a specific function with a few parameter and additional methods.\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = bill_depth_mm, \n           color = species, shape = species)) + \n  geom_point(size = 2) + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n  labs(x = \"Bill length (mm)\", y = \"Bill width (mm)\", \n       title = \"Bill length vs. bill width\", \n       subtitle = \"Using geom_point and geom_smooth of the ggplot2 package\",\n       color = \"Species\", shape = \"Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n(sns.lmplot(\n    data = penguins,\n    x = \"bill_length_mm\", y = \"bill_depth_mm\", \n    hue = \"species\", markers = [\"o\", \"s\", \"^\"], fit_reg = True, \n    scatter_kws = {\"s\": 50}, legend = False\n  )\n  .set_axis_labels(\"Bill length (mm)\", \"Bill width (mm)\")\n  .add_legend(title = \"Species\")\n  .fig.suptitle(\"Bill length vs. bill width\", y = 1)\n)"
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#visualizing-distributions",
    "href": "posts/ggplot2-vs-seaborn/index.html#visualizing-distributions",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\n\nA categorical variable\nLet’s break down the differences in smaller steps by focusing on simpler examples. If you have a categorical variable and want to compare its relevance in your data, then ggplot2::geom_bar() and seaborn.countplot() are your friends. I manually specify the order in the seaborn figure to mimic the automatic behavior of ggplot2.\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = island)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\nsns.countplot(\n  data = penguins, \n  x = \"island\",\n  order = sorted(penguins[\"island\"].unique())\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA numerical variable\nIf you have a numerical variable, usually histograms are a good starting point to get a better feeling for the distribution of your data. ggplot2::geom_histogram() and seaborn.histplot with options to control bin widths or number of bins are the functions for this task.\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\n\n\nsns.histplot(\n  data = penguins, \n  x = \"bill_length_mm\", \n  binwidth = 2\n)\n\n\n\n\n\n\n\n\n\n\n\nBoth packages also support density curves, but I personally wouldn”t recommend to start with densities because they are estimated curves that might obscure underlying data features. However, we look at densities in the next section."
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#visualizing-relationships",
    "href": "posts/ggplot2-vs-seaborn/index.html#visualizing-relationships",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\n\nA numerical and a categorical variable\nTo visualize relationships, you need to have at least two columns. If you have a numerical and a categorical variable, then histograms or densities with groups are a good starting point. The next example illustrates the use of density curves via ggplot2::geom_density() and seaborn.kdeplot() with similar options to control the appearance.\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(linewidth = 0.75, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(\n  data = penguins, \n  x = \"body_mass_g\", \n  hue = \"species\", \n  fill = True, common_norm = False, alpha = 0.5, linewidth = 0.75\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo categorical columns\nStacked bar plots are a good way to display the relationship between two categorical columns. geom_bar() with the position argument and seaborn.histplot() with stat are your aesthetics of choice for this task. Note that you can easily switch to counts by using position = \"identity\" and stat = \"count\", respectively, instead of relative frequencies as in the example below. Note that I use shrink = 0.8 to get some spacing between columns in the seaborn plot.\n\nggplot2seaborn\n\n\n\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\n\nsns.histplot(\n  data = penguins, \n  x = \"species\", \n  hue = \"island\", multiple = \"fill\", stat = \"percent\", shrink = 0.8\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo numerical columns\nScatter plots and regression lines are definitely the most common approach for visualizing the relationship between two numerical columns and we focus on scatter plots for this example (see the first visualization example if you want to see again how to add a regression line). Here, the size parameter controls the size of the shapes that you use for the data points in ggplot2::geom_point() relative to the base size (i.e., it is not tied to any unit of measurement like pixels). For seaborn.scatterplot() you have the s parameter to control point sizes manually, where size is typically given in squared points (where a point is a unit of measure in typography, equal to 1/72 of an inch).\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(size = 2)\n\n\n\n\n\n\n\n\n\n\n\nsns.scatterplot(\n  data = penguins, \n  x = \"bill_length_mm\", y = \"flipper_length_mm\", \n  s = 50\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree or more columns\nYou can include more information by mapping columns to additional aesthetics. For instance, we can map colors and shapes to species and create separate plots for each island by using facets. Facets are actually a great way to extend your figures, so I highly recommend playing around with them using your own data.\nIn ggplot2 you add the facet layer at the end, whereas in seaborn you have to start with the facet grid at the beginning and map scatter plots across facets. Note that I use variable assignment to penguins_facet in order to prevent seaborn from printing the figure twice while rendering this post (no idea why though).\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\n\n\n\n\n\n\n\n\n\npenguins_facet = (sns.FacetGrid(\n    data = penguins, col=\"island\", col_order = sorted(penguins[\"island\"].unique()),\n    hue=\"species\", margin_titles = True\n  )\n  .map(sns.scatterplot, \"bill_length_mm\", \"flipper_length_mm\", alpha = 0.7)\n  .add_legend()\n)"
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#saving-plots",
    "href": "posts/ggplot2-vs-seaborn/index.html#saving-plots",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "Saving plots",
    "text": "Saving plots\nAs a final comparison, let us look at saving plots. ggplot2::ggsave() provides the most important options as function paramters. In seaborn, you have to, for instance, tweak the figure size before can save the figure.\n\nggplot2seaborn\n\n\n\npenguins_figure &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) + \n  geom_point()\n\nggsave(penguins_figure, filename = \"penguins-figure.png\",\n       width = 7, height = 5, dpi = 300)\n\n\n\n\npenguins_figure = sns.scatterplot(\n  data = penguins, \n  x = \"bill_length_mm\", y = \"flipper_length_mm\"\n)\n\npenguins_figure.figure.set_size_inches(7, 5)\npenguins_figure.figure.savefig(\"penguins-figure.png\", dpi = 300)"
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#conclusion",
    "href": "posts/ggplot2-vs-seaborn/index.html#conclusion",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "Conclusion",
    "text": "Conclusion\nIn terms of syntax, ggplot2 and seaborn are considerably different. ggplot2 follows the basic syntax of\nggplot(data = &lt;DATA&gt;, aes(x = &lt;X-VAR&gt;, y = &lt;Y-VAR&gt;)) +\n  geom_&lt;PLOT-TYPE&gt;() +\n  other_layers\nand seaborn follows the syntax\nsns.&lt;PLOT-FUNCTION&gt;(data = &lt;DATA&gt;, x = \"&lt;X-VAR&gt;\", y = \"&lt;Y-VAR&gt;\", other_parameters).\nI think this difference comes from different philosophies. ggplot2 focuses on layering and mapping data to aesthetics, whereas seaborn is more about providing a variety of complex plot types easily. Moreover, ggplot2 is designed to work considerably well with tidy data (see my blog post on tidy data), while seaborn is more flexible with input data formats.\nBoth offer extensive customization options, but they are implemented differently. ggplot2 uses additional layers and scales and hence allows for a large ecosystem (see, for instance, this repo for a collection of links), while seaborn relies on the customization options of matplotlib and its own parameters. I think both approaches are powerful and have their unique advantages, and the choice between them often depends on your programming language preference and specific requirements of the data visualization task."
  },
  {
    "objectID": "posts/clustering-binary-data/index.html",
    "href": "posts/clustering-binary-data/index.html",
    "title": "Clustering Binary Data",
    "section": "",
    "text": "In this post, I tackle the challenge to extract a small number of typical respondent profiles from a large scale survey with multiple yes-no questions. This type of setting corresponds to a classification problem without knowing the true labels of the observations – also known as unsupervised learning.\nTechnically speaking, we have a set of \\(N\\) observations \\((x_1, x_2, ... , x_N)\\) of a random \\(p\\)-vector \\(X\\) with joint density \\(\\text{Pr}(X)\\). The goal of classification is to directly infer the properties of this probability density without the help of the correct answers (or degree-of-error) for each observation. In this note, we focus on cluster analysis that attempts to find convex regions of the \\(X\\)-space that contain modes of \\(\\text{Pr}(X)\\). This approach aims to tell whether \\(\\text{Pr}(X)\\) can be represented by a mixture of simpler densities representing distinct classes of observations.\nIntuitively, we want to find clusters of the survey responses such that respondents within each cluster are more closely related to one another than respondents assigned to different clusters. There are many possible ways to achieve that, but we focus on the most popular and most approachable ones: \\(K\\)-means, \\(K\\)-modes, as well as agglomerative and divisive hierarchical clustering. As we see below, the 4 models yield quite different results for clustering binary data.\nWe use the following packages throughout this post. In particular, we use klaR and cluster for clustering algorithms that go beyond the stats package that is included with your R installation.1\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(klaR)\nlibrary(cluster)\nNote that there will be an annoying namespace conflict between MASS::select() and dplyr::select()). We use the conflicted package to explicitly resolve these conflicts.\nlibrary(conflicted)\n\nconflicts_prefer(\n  dplyr::filter,\n  dplyr::lag,\n  dplyr::select\n)"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#creating-sample-data",
    "href": "posts/clustering-binary-data/index.html#creating-sample-data",
    "title": "Clustering Binary Data",
    "section": "Creating sample data",
    "text": "Creating sample data\nLet us start by creating some sample data where we basically exactly know which kind of answer profiles are out there. Later, we evaluate the cluster models according to how well they are doing in uncovering the clusters and assigning respondents to clusters. We assume that there are 4 yes/no questions labeled q1, q2, q3 and q4. In addition, there are 3 different answer profiles where cluster 1 answers positively to the first question only, cluster 2 answers positively to question 2 and 3 and cluster 3 answers all questions positively. We also define the the number of respondents for each cluster.\n\ncenters &lt;- tibble(\n  cluster = factor(1:3), \n  respondents = c(250, 500, 200),\n  q1 = c(1, 0, 1),\n  q2 = c(0, 1, 1),             \n  q3 = c(0, 1, 1),\n  q4 = c(0, 0, 1)\n)\n\nAlternatively, we could think of the yes/no questions as medical records that indicate whether the subject has a certain pre-condition or not.\nSince it should be a bit tricky for the clustering models to find the actual response profiles, let us add some noise in the form of respondents that deviate from their assigned cluster profile and shuffle all rows. We find out below how the cluster algorithms are able to deal with this noise.\n\nset.seed(42)\nlabelled_respondents &lt;- centers |&gt; \n  mutate(\n    across(\n      starts_with(\"q\"),\n      ~map2(respondents, .x, function(x, y) {\n        rbinom(x, 1, max((y - 0.1), 0.1))\n      }),\n      .names = \"{col}\"\n    )\n  ) |&gt; \n  select(-respondents) |&gt; \n  unnest(cols = c(q1, q2, q3, q4)) |&gt; \n  sample_n(n())\n\nThe figure below visualizes the distribution of simulated question responses by cluster.\n\nlabelled_respondents |&gt;\n  pivot_longer(cols = -cluster, \n               names_to = \"question\", values_to = \"response\") |&gt;\n  mutate(response = response == 1) |&gt;\n  ggplot(aes(x = response, y = question, color = cluster)) +\n  geom_jitter() +\n  theme_bw() +\n  labs(x = \"Response\", y = \"Question\", color = \"Cluster\",\n       title = \"Visualization of simulated question responses by cluster\")"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#k-means-clustering",
    "href": "posts/clustering-binary-data/index.html#k-means-clustering",
    "title": "Clustering Binary Data",
    "section": "\\(K\\)-means clustering",
    "text": "\\(K\\)-means clustering\nThe \\(K\\)-means algorithm is one of the most popular clustering methods (see also this tidymodels example). It is intended for situations in which all variables are of the quantitative type since it partitions all respondents into \\(k\\) groups such that the sum of squares from respondents to the assigned cluster centers are minimized. For binary data, the Euclidean distance reduces to counting the number of variables on which two cases disagree.\nThis leads to a problem (which is also described here) because of an arbitrary cluster assignment after cluster initialization. The first chosen clusters are still binary data and hence observations have integer distances from each of the centers. The corresponding ties are hard to overcome in any meaningful way. Afterwards, the algorithm computes means in clusters and revisits assignments. Nonetheless, \\(K\\)-means might produce informative results in a fast and easy to interpret way. We hence include it in this post for comparison.\nTo run the \\(K\\)-means algorithm, we first drop the cluster column.\n\nrespondents &lt;- labelled_respondents |&gt;\n  select(-cluster)\n\nIt is very straight-forward to run the built-in stats::kmeans clustering algorithm. We choose the parameter of maximum iterations to be 1000 to increase the likeliness of getting the best fitting clusters. Since the data is fairly small and the algorithm is also quite fast, we see no harm in using a high number of iterations.\n\niter_max &lt;- 1000\nkmeans_example &lt;- stats::kmeans(respondents, centers = 3, iter.max = iter_max)\n\nThe output of the algorithm is a list with different types of information including the assigned clusters for each respondent.\nAs we want to compare cluster assignment across different models and we repeatedly assign different clusters to respondents, we write up a helper function that adds assignments to the respondent data from above. The function shows that \\(K\\)-means and \\(K\\)-modes contain a field with cluster information. The two hierarchical cluster models, however, need to be cut a the desired number of clusters (more on that later).\n\nassign_clusters &lt;- function(model, k = NULL) {\n  if (class(model)[1] %in% c(\"kmeans\", \"kmodes\")) {\n    cluster_assignment &lt;- model$cluster\n  }\n  if (class(model)[1] %in% c(\"agnes\", \"diana\")) {\n    if (is.null(k)) {\n      stop(\"k required for hierarchical models!\")\n    }\n    cluster_assignment &lt;- stats::cutree(model, k = k)\n  }\n  \n  clusters &lt;- respondents |&gt;\n    mutate(cluster = cluster_assignment)\n  \n  return(clusters)\n}\n\nIn addition, we introduce a helper function that summarizes information by cluster. In particular, the function computes average survey responses (which correspond to proportion of yes answers in the current setting) and sorts the clusters according to the total number of positive answers. The latter helps us later to compare clusters across different models.\n\nsummarize_clusters &lt;- function(model, k = NULL) {\n\n  clusters &lt;- assign_clusters(model = model, k = k)\n  \n  summary_statistics &lt;- clusters |&gt;\n    group_by(cluster) |&gt;\n    summarize(across(matches(\"q\"), \\(x) mean(x, na.rm = TRUE)),\n              assigned_respondents = n()) |&gt;\n    select(-cluster) |&gt;\n    mutate(total = rowSums(across(matches(\"q\")))) |&gt;\n    arrange(-total) |&gt;\n    mutate(k = row_number(),\n           model = class(model)[1])\n  \n  return(summary_statistics)\n}\n\nWe could easily introduce other summary statistics into the function, but the current specification is sufficient for the purpose of this note.\n\nkmeans_example &lt;- summarize_clusters(kmeans_example)\n\nSince we do not know the true number of clusters in real-world settings, we want to compare the performance of clustering models for different numbers of clusters. Since we know that the true number of clusters is 3 in the current setting, let us stick to a maximum of 7 clusters. In practice, you might of course choose an arbitrary maximum number of clusters.\n\nk_min &lt;- 1\nk_max &lt;- 7\n\nkmeans_results &lt;- tibble(k = k_min:k_max) |&gt;\n  mutate(\n    kclust = map(k, ~kmeans(respondents, centers = .x, iter.max = iter_max)),\n  )\n\nA common heuristic to determine the optimal number of clusters is the elbow method where we plot the within-cluster sum of squared errors of an algorithm for increasing number of clusters. The optimal number of clusters corresponds to the point where adding another cluster does lead to much of an improvement anymore. In economic terms, we look for the point where the diminishing returns to an additional cluster are not worth the additional cost (assuming that we want the minimum number of clusters with optimal predictive power).\nThe function below computes the within-cluster sum of squares for any cluster assignments.\n\ncompute_withinss &lt;- function(model, k = NULL) {\n  \n  clusters &lt;- assign_clusters(model = model, k = k)\n  \n  centers &lt;- clusters |&gt;\n    group_by(cluster) |&gt;\n    summarize_all(mean) |&gt;\n    pivot_longer(cols = -cluster, names_to = \"question\", values_to = \"cluster_mean\")\n  \n  withinss &lt;- clusters |&gt;\n    pivot_longer(cols = -cluster, names_to = \"question\", values_to = \"response\") |&gt;\n    left_join(centers, by = c(\"cluster\", \"question\")) |&gt;\n    summarize(k = max(cluster),\n              withinss = sum((response - cluster_mean)^2)) |&gt;\n    mutate(model = class(model)[1])\n  \n  return(withinss)\n}\n\nWe can simply map the function across our list of \\(K\\)-means models. For better comparability, we normalize the within-cluster sum of squares for any number of cluster by the benchmark case of only having a single cluster. Moreover, we consider log-differences to because we care more about the percentage decrease in sum of squares rather than the absolute number.\n\nkmeans_logwithindiss &lt;- kmeans_results$kclust |&gt;\n  map(compute_withinss) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#k-modes-clustering",
    "href": "posts/clustering-binary-data/index.html#k-modes-clustering",
    "title": "Clustering Binary Data",
    "section": "\\(K\\)-modes clustering",
    "text": "\\(K\\)-modes clustering\nSince \\(K\\)-means is actually not ideal for binary (or hierarchical data in general), Huang (1997) came up with the \\(K\\)-modes algorithm. This clustering approach aims to partition respondents into \\(K\\) groups such that the distance from respondents to the assigned cluster modes is minimized. A mode is a vector of elements that minimize the dissimilarities between the vector and each object of the data. Rather than using the Euclidean distance, \\(K\\)-modes uses simple matching distance between respondents to quantify dissimilarity which translates into counting the number of mismatches in all question responses in the current setting.\nFortunately, the klaR package provides an implementation of the \\(K\\)-modes algorithm that we can apply just like the \\(K\\)-means above.\n\nkmodes_example &lt;- klaR::kmodes(respondents, iter.max = iter_max, modes = 3) |&gt;\n  summarize_clusters()\n\nSimilarly, we just map the model across different numbers of target cluster modes and compute the within-cluster sum of squares.\n\nkmodes_results &lt;- tibble(k = k_min:k_max) |&gt;\n  mutate(\n    kclust = map(k, ~klaR::kmodes(respondents, modes = ., iter.max = iter_max))\n  )\n\nkmodes_logwithindiss &lt;- kmodes_results$kclust |&gt;\n  map(compute_withinss) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))\n\nNote that we computed the within-cluster sum of squared errors rather than using the within-cluster simple-matching distance provided by the function itself. The latter counts the number of differences from assigned respondents to their cluster modes."
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#hierarchical-clustering",
    "href": "posts/clustering-binary-data/index.html#hierarchical-clustering",
    "title": "Clustering Binary Data",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nAs an alternative to computing optimal assignments for a given number of clusters, we might sometimes prefer to arrange the clusters into a natural hierarchy. This involves successively grouping the clusters themselves such that at each level of the hierarchy, clusters within the same group are more similar to each other than those in different groups. There are two fundamentally different approaches to hierarchical clustering that are fortunately implemented in the great cluster package.\nBoth hierarchical clustering approaches require a dissimilarity or distance matrix. Since we have binary data, we choose the asymmetric binary distance matrix based on the Jaccard distance. Intuitively, the Jaccard distance measures how far the overlap of responses between two groups is from perfect overlap.\n\ndissimilarity_matrix &lt;- stats::dist(respondents, method = \"binary\")\n\nAgglomerative clustering start at the bottom and at each level recursively merge a selected pair of clusters into a single cluster. This produces a clustering at the next higher level with one less cluster. The pair chosen for merging consist of the two clusters with the smallest within-cluster dissimilarity. On an intuitive level, agglomerative clustering is hence better in discovering small clusters.\nThe cluster package provides the agnes algorithm (AGglomerative NESting) that can easily applied to the dissimilarity matrix.\n\nagnes_results &lt;- cluster::agnes(\n  dissimilarity_matrix, diss = TRUE, keep.diss = TRUE, method = \"complete\"\n)\n\nThe function returns a clustering tree that we could plot (which actually is rarely really helpful) or cut into different partitions using the stats::cutree function. This is why the helper functions from above need a number of target clusters as an input for hierarchical clustering models. However, the logic of the summary statistics are just as above.\n\nagnes_example &lt;- summarize_clusters(agnes_results, k = 3)\n\nagnes_logwithindiss &lt;- k_min:k_max |&gt;\n  map(~compute_withinss(agnes_results, .)) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))\n\nDivisive methods start at the top and at each level recursively split one of the existing clusters at that level into two new clusters. The split is chosen such that two new groups with the largest between-group dissimilarity emerge. Intuitively speaking, divisive clustering is thus better in discovering large clusters.\nThe cluster package provides the diana algorithm (DIvise ANAlysis) for this clustering approach where the logic is basically the same as for the agnes model.\n\ndiana_results &lt;- cluster::diana(\n  dissimilarity_matrix, diss = TRUE, keep.diss = TRUE\n) \n\ndiana_example &lt;- diana_results |&gt;\n  summarize_clusters(k = 3)\n\ndiana_logwithindiss &lt;-  k_min:k_max |&gt;\n  map(~compute_withinss(diana_results, .)) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#model-comparison",
    "href": "posts/clustering-binary-data/index.html#model-comparison",
    "title": "Clustering Binary Data",
    "section": "Model comparison",
    "text": "Model comparison\nLet us start the model comparison by looking at the within cluster sum of squares for different numbers of clusters. The figure shows that the \\(K\\)-modes algorithm improves the fastest towards the true number of 3 clusters. The elbow method would suggest in this case to stick with 3 clusters for this algorithm. Similarly, for the \\(K\\)-means model. The hierarchical clustering models do not seem to support 3 clusters.\n\nbind_rows(kmeans_logwithindiss, kmodes_logwithindiss,\n          agnes_logwithindiss, diana_logwithindiss) |&gt;\n  ggplot(aes(x = k, y = logwithindiss, color = model, linetype = model)) +\n  geom_line() +\n  scale_x_continuous(breaks = k_min:k_max) + \n  theme_minimal() +\n  labs(x = \"Number of Clusters\", y = bquote(log(W[k])-log(W[1])), \n       color = \"Model\", linetype = \"Model\",\n       title = \"Within cluster sum of squares relative to benchmark case of one cluster\")\n\n\n\n\n\n\n\n\nNow, let us compare the proportion of positive responses within assigned clusters across models. Recall that we ranked clusters according to the total share of positive answers to ensure comparability. This approach is only possible in this type of setting where we can easily introduce such a ranking. The figure suggests that \\(K\\)-modes performs best for the current setting as it identifies the correct responses for each cluster.\n\nbind_rows(\n  kmeans_example, kmodes_example,\n  agnes_example, diana_example) |&gt;\n  select(-c(total, assigned_respondents)) |&gt;\n  pivot_longer(cols = -c(k, model), \n               names_to = \"question\", values_to = \"response\") |&gt;\n  mutate(cluster = paste0(\"Cluster \", k)) |&gt;\n  ggplot(aes(x = response, y = question, fill = model)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~cluster) +\n  theme_bw() +\n  scale_x_continuous(labels = scales::percent) + \n  geom_hline(yintercept = seq(1.5, length(unique(colnames(respondents))) - 0.5, 1),\n             colour = 'black') +\n  labs(x = \"Proportion of responses\", y = \"Question\", fill = \"Model\",\n       title = \"Proportion of positive responses within assigned clusters\")\n\n\n\n\n\n\n\n\nFinally, let us check how well each model assigns respondents to the true cluster which is obviously not possible in real unsupervised applications. The figure below shows the true number of respondents by cluster as a dashed box and the assigned respondents as bars. The figure shows that \\(K\\)-modes is the only model that is able to consistently assign respondents to their correct cluster.\n\nbind_rows(\n  kmeans_example, kmodes_example,\n  agnes_example, diana_example) |&gt;\n  mutate(cluster = paste0(\"Cluster \", k)) |&gt;\n  select(model, cluster, assigned_respondents) |&gt;\n  ggplot() +\n  geom_col(position = \"dodge\", \n           aes(y = assigned_respondents, x = cluster, fill = model)) +\n  geom_col(data = labelled_respondents |&gt;\n             group_by(cluster = paste0(\"Cluster \", cluster)) |&gt;\n             summarize(assigned_respondents = n(),\n                       model = \"actual\"),\n           aes(y = assigned_respondents, x = cluster), \n           fill = \"white\", color = \"black\", alpha = 0, linetype = \"dashed\") +\n  theme_bw() +\n  labs(x = NULL, y = \"Number of assigned respondents\", fill = \"Model\",\n       title = \"Number of assigned respondents by cluster\",\n       subtitle = \"Dashed box indicates true number of respondents by cluster\")\n\n\n\n\n\n\n\n\nLet me end this post with a few words of caution: first, the ultimate outcome heavily depends on the seed chosen at the beginning of the post. The results might be quite different for other draws of respondents or initial conditions for clustering algorithms. Second, there are many more models out there that can be applied to the current setting. However, with this post I want to emphasize that it is important to consider different models at the same time and to compare them through a consistent set of measures. Ultimately, choosing the optimal number of clusters in practice requires a judgment call, but at least it can be informed as much as possible."
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#footnotes",
    "href": "posts/clustering-binary-data/index.html#footnotes",
    "title": "Clustering Binary Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs of writing, the tidyclust package only has limited support for hierarchical clustering, so I decided to abstain from using it for this post.↩︎"
  },
  {
    "objectID": "posts/dplyr-vs-siuba/index.html",
    "href": "posts/dplyr-vs-siuba/index.html",
    "title": "Tidy Data Manipulation: dplyr vs siuba",
    "section": "",
    "text": "There are a myriad of options to perform essential data manipulation tasks in R and Python (see, for instance, my other posts on dplyr vs ibis and dplyr vs pandas). However, if we want to do tidy data science in R, there is a clear forerunner: dplyr. In the world of Python, siuba is around since 2019 and a dedicated port of dplyr and other R libraries. In this blog post, I illustrate their syntactic similarities and highlight differences between these two packages that emerge for a few key tasks.\nBefore we dive into the comparison, a short introduction to the packages: the dplyr package in R allows users to refer to columns without quotation marks due to its implementation of non-standard evaluation (NSE). NSE is a programming technique used in R that allows functions to capture the expressions passed to them as arguments, rather than just the values of those arguments. The primary goal of NSE in the context of dplyr is to create a more user-friendly and intuitive syntax. This makes data manipulation tasks more straightforward and aligns with the general philosophy of the tidyverse to make data science faster, easier, and more fun.1\nThe siuba package in Python offers a similar user-friendly experience for data manipulation by allowing users to work with data frames in a way that mimics dplyr’s intuitive syntax. siuba leverages Python’s syntax and capabilities, enabling operations like filtering, selecting, and mutating without the need for extensive boilerplate code. siuba tries to capture the spirit of concise and expressive data manipulation via NSE by introducing siu expressions and a pipe (which we will both use below). This approach aligns with the broader goals of making data science more accessible and efficient, providing Python users with a powerful tool that enhances productivity and readability in their data analysis workflow."
  },
  {
    "objectID": "posts/dplyr-vs-siuba/index.html#loading-packages-and-data",
    "href": "posts/dplyr-vs-siuba/index.html#loading-packages-and-data",
    "title": "Tidy Data Manipulation: dplyr vs siuba",
    "section": "Loading packages and data",
    "text": "Loading packages and data\nWe start by loading the main packages of interest and the popular palmerpenguins package that exists for both R and Python. We then use the penguins data frame as the data to compare all functions and methods below.\n\ndplyrsiuba\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins &lt;- palmerpenguins::penguins\n\n\n\n\nfrom siuba import _, filter, arrange, select, rename, mutate, group_by, summarize\nfrom palmerpenguins import load_penguins\n\npenguins = load_penguins()"
  },
  {
    "objectID": "posts/dplyr-vs-siuba/index.html#work-with-rows",
    "href": "posts/dplyr-vs-siuba/index.html#work-with-rows",
    "title": "Tidy Data Manipulation: dplyr vs siuba",
    "section": "Work with rows",
    "text": "Work with rows\n\nFilter rows\nFiltering rows works very similarly for both packages, they even have the same function names: dplyr::filter() and siuba.filter(). To select columns in siuba, you need the siuba._ expression that allows you to specify what action you want to perform on a column and that is later evaluated by functions such as siuba.filter()\n\ndplyrsiuba\n\n\n\npenguins |&gt; \n  filter(species == \"Adelie\" & \n           island %in% c(\"Biscoe\", \"Dream\"))\n\n# A tibble: 100 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 90 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins \n  &gt;&gt; filter((_.species == \"Adelie\") &\n              (_.island.isin([\"Biscoe\", \"Dream\"])))\n)\n\n    species  island  bill_length_mm  ...  body_mass_g     sex  year\n20   Adelie  Biscoe            37.8  ...       3400.0  female  2007\n21   Adelie  Biscoe            37.7  ...       3600.0    male  2007\n22   Adelie  Biscoe            35.9  ...       3800.0  female  2007\n23   Adelie  Biscoe            38.2  ...       3950.0    male  2007\n24   Adelie  Biscoe            38.8  ...       3800.0    male  2007\n..      ...     ...             ...  ...          ...     ...   ...\n147  Adelie   Dream            36.6  ...       3475.0  female  2009\n148  Adelie   Dream            36.0  ...       3450.0  female  2009\n149  Adelie   Dream            37.8  ...       3750.0    male  2009\n150  Adelie   Dream            36.0  ...       3700.0  female  2009\n151  Adelie   Dream            41.5  ...       4000.0    male  2009\n\n[100 rows x 8 columns]\n\n\n\n\n\n\n\nSlice rows\ndplyr::slice() takes integers with row numbers as inputs, so you can use ranges and arbitrary vectors of integers. There is no direct equivalent in siuba, but we can just use the iloc method to replicate the results. For instance, to the the same result of slicing rows 10 to 20, the code looks as follows (note that indexing starts at 0 in Python, while it starts at 1 in R):\n\ndplyrsiuba\n\n\n\npenguins |&gt; \n  slice(10:20)\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           42            20.2               190        4250\n 2 Adelie  Torgersen           37.8          17.1               186        3300\n 3 Adelie  Torgersen           37.8          17.3               180        3700\n 4 Adelie  Torgersen           41.1          17.6               182        3200\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           36.6          17.8               185        3700\n 8 Adelie  Torgersen           38.7          19                 195        3450\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           34.4          18.4               184        3325\n11 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\npenguins.iloc[9:19]\n\n   species     island  bill_length_mm  ...  body_mass_g     sex  year\n9   Adelie  Torgersen            42.0  ...       4250.0     NaN  2007\n10  Adelie  Torgersen            37.8  ...       3300.0     NaN  2007\n11  Adelie  Torgersen            37.8  ...       3700.0     NaN  2007\n12  Adelie  Torgersen            41.1  ...       3200.0  female  2007\n13  Adelie  Torgersen            38.6  ...       3800.0    male  2007\n14  Adelie  Torgersen            34.6  ...       4400.0    male  2007\n15  Adelie  Torgersen            36.6  ...       3700.0  female  2007\n16  Adelie  Torgersen            38.7  ...       3450.0  female  2007\n17  Adelie  Torgersen            42.5  ...       4500.0    male  2007\n18  Adelie  Torgersen            34.4  ...       3325.0  female  2007\n\n[10 rows x 8 columns]\n\n\n\n\n\n\n\nArrange rows\nTo orders the rows of a data frame by the values of selected columns, we have dplyr::arrange() and siuba.arrange().\n\ndplyrsiuba\n\n\n\npenguins |&gt; \n  arrange(island, desc(bill_length_mm))\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins \n  &gt;&gt; arrange(_.island, -_.bill_length_mm)\n)\n\n    species     island  bill_length_mm  ...  body_mass_g     sex  year\n185  Gentoo     Biscoe            59.6  ...       6050.0    male  2007\n253  Gentoo     Biscoe            55.9  ...       5600.0    male  2009\n267  Gentoo     Biscoe            55.1  ...       5850.0    male  2009\n215  Gentoo     Biscoe            54.3  ...       5650.0    male  2008\n259  Gentoo     Biscoe            53.4  ...       5500.0    male  2009\n..      ...        ...             ...  ...          ...     ...   ...\n80   Adelie  Torgersen            34.6  ...       3200.0  female  2008\n18   Adelie  Torgersen            34.4  ...       3325.0  female  2007\n8    Adelie  Torgersen            34.1  ...       3475.0     NaN  2007\n70   Adelie  Torgersen            33.5  ...       3600.0  female  2008\n3    Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n\n[344 rows x 8 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-siuba/index.html#work-with-columns",
    "href": "posts/dplyr-vs-siuba/index.html#work-with-columns",
    "title": "Tidy Data Manipulation: dplyr vs siuba",
    "section": "Work with columns",
    "text": "Work with columns\n\nSelect columns\nSelecting a subset of columns works essentially the same for both dplyr::select() and siuba.select().\n\ndplyrsiuba\n\n\n\npenguins |&gt; \n  select(bill_length_mm, sex)\n\n# A tibble: 344 × 2\n   bill_length_mm sex   \n            &lt;dbl&gt; &lt;fct&gt; \n 1           39.1 male  \n 2           39.5 female\n 3           40.3 female\n 4           NA   &lt;NA&gt;  \n 5           36.7 female\n 6           39.3 male  \n 7           38.9 female\n 8           39.2 male  \n 9           34.1 &lt;NA&gt;  \n10           42   &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n\n\n(penguins\n  &gt;&gt; select(_.bill_length_mm, _.sex)\n)\n\n     bill_length_mm     sex\n0              39.1    male\n1              39.5  female\n2              40.3  female\n3               NaN     NaN\n4              36.7  female\n..              ...     ...\n339            55.8    male\n340            43.5  female\n341            49.6    male\n342            50.8    male\n343            50.2  female\n\n[344 rows x 2 columns]\n\n\n\n\n\n\n\nRename columns\nRenaming columns also works the same in dplyr::rename() and siuba.rename().\n\ndplyrsiuba\n\n\n\npenguins |&gt; \n  rename(bill_length = bill_length_mm,\n         bill_depth = bill_depth_mm)\n\n# A tibble: 344 × 8\n   species island    bill_length bill_depth flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen        39.1       18.7               181        3750 male  \n 2 Adelie  Torgersen        39.5       17.4               186        3800 female\n 3 Adelie  Torgersen        40.3       18                 195        3250 female\n 4 Adelie  Torgersen        NA         NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen        36.7       19.3               193        3450 female\n 6 Adelie  Torgersen        39.3       20.6               190        3650 male  \n 7 Adelie  Torgersen        38.9       17.8               181        3625 female\n 8 Adelie  Torgersen        39.2       19.6               195        4675 male  \n 9 Adelie  Torgersen        34.1       18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen        42         20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\n\n\n(penguins\n  &gt;&gt; rename(bill_length = _.bill_length_mm, \n            bill_depth = _.bill_depth_mm)\n)\n\n       species     island  bill_length  ...  body_mass_g     sex  year\n0       Adelie  Torgersen         39.1  ...       3750.0    male  2007\n1       Adelie  Torgersen         39.5  ...       3800.0  female  2007\n2       Adelie  Torgersen         40.3  ...       3250.0  female  2007\n3       Adelie  Torgersen          NaN  ...          NaN     NaN  2007\n4       Adelie  Torgersen         36.7  ...       3450.0  female  2007\n..         ...        ...          ...  ...          ...     ...   ...\n339  Chinstrap      Dream         55.8  ...       4000.0    male  2009\n340  Chinstrap      Dream         43.5  ...       3400.0  female  2009\n341  Chinstrap      Dream         49.6  ...       3775.0    male  2009\n342  Chinstrap      Dream         50.8  ...       4100.0    male  2009\n343  Chinstrap      Dream         50.2  ...       3775.0  female  2009\n\n[344 rows x 8 columns]\n\n\n\n\n\n\n\nMutate columns\nTransforming existing columns or creating new ones is an essential part of data analysis. dplyr::mutate() and siuba.mutate() are the work horses for these tasks. Both approaches have a very similar syntax and capabilities. Compared to other Python libraries, you don’t have to split up variable assignments across mutate blocks if you want to refer to a newly created variable in siuba.\n\ndplyrsiuba\n\n\n\npenguins |&gt; \n  mutate(ones = 1,\n         bill_length = bill_length_mm / 10,\n         bill_length_squared = bill_length^2) |&gt; \n  select(ones, bill_length_mm, bill_length, bill_length_squared)\n\n# A tibble: 344 × 4\n    ones bill_length_mm bill_length bill_length_squared\n   &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1     1           39.1        3.91                15.3\n 2     1           39.5        3.95                15.6\n 3     1           40.3        4.03                16.2\n 4     1           NA         NA                   NA  \n 5     1           36.7        3.67                13.5\n 6     1           39.3        3.93                15.4\n 7     1           38.9        3.89                15.1\n 8     1           39.2        3.92                15.4\n 9     1           34.1        3.41                11.6\n10     1           42          4.2                 17.6\n# ℹ 334 more rows\n\n\n\n\n\n(penguins\n  &gt;&gt; mutate(ones = 1,\n            bill_length = _.bill_length_mm / 10,\n            bill_length_squared = _.bill_length ** 2)\n  &gt;&gt; select(_.ones, _.bill_length_mm, _.bill_length, _.bill_length_squared)\n)\n\n     ones  bill_length_mm  bill_length  bill_length_squared\n0       1            39.1         3.91              15.2881\n1       1            39.5         3.95              15.6025\n2       1            40.3         4.03              16.2409\n3       1             NaN          NaN                  NaN\n4       1            36.7         3.67              13.4689\n..    ...             ...          ...                  ...\n339     1            55.8         5.58              31.1364\n340     1            43.5         4.35              18.9225\n341     1            49.6         4.96              24.6016\n342     1            50.8         5.08              25.8064\n343     1            50.2         5.02              25.2004\n\n[344 rows x 4 columns]\n\n\n\n\n\n\n\nRelocate columns\ndplyr::relocate() provides options to change the positions of columns in a data frame, using the same syntax as dplyr::select(). In addition, there are the options .after and .before to provide users with additional shortcuts.\nThe recommended way to relocate columns in siuba is to use the siuba.select() method, but there are no options as in dplyr::relocate(). In fact, the safest way to consistently get the correct order of columns is to explicitly specify them.\n\ndplyrsiuba\n\n\n\npenguins |&gt; \n  relocate(c(species, bill_length_mm), .before = sex)\n\n# A tibble: 344 × 8\n   island    bill_depth_mm flipper_length_mm body_mass_g species bill_length_mm\n   &lt;fct&gt;             &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1 Torgersen          18.7               181        3750 Adelie            39.1\n 2 Torgersen          17.4               186        3800 Adelie            39.5\n 3 Torgersen          18                 195        3250 Adelie            40.3\n 4 Torgersen          NA                  NA          NA Adelie            NA  \n 5 Torgersen          19.3               193        3450 Adelie            36.7\n 6 Torgersen          20.6               190        3650 Adelie            39.3\n 7 Torgersen          17.8               181        3625 Adelie            38.9\n 8 Torgersen          19.6               195        4675 Adelie            39.2\n 9 Torgersen          18.1               193        3475 Adelie            34.1\n10 Torgersen          20.2               190        4250 Adelie            42  \n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins \n  &gt;&gt; select(_.island, _.bill_depth_mm, _.flipper_length_mm, _.body_mass_g, \n            _.species, _.bill_length_mm, _.sex)\n)\n\n        island  bill_depth_mm  ...  bill_length_mm     sex\n0    Torgersen           18.7  ...            39.1    male\n1    Torgersen           17.4  ...            39.5  female\n2    Torgersen           18.0  ...            40.3  female\n3    Torgersen            NaN  ...             NaN     NaN\n4    Torgersen           19.3  ...            36.7  female\n..         ...            ...  ...             ...     ...\n339      Dream           19.8  ...            55.8    male\n340      Dream           18.1  ...            43.5  female\n341      Dream           18.2  ...            49.6    male\n342      Dream           19.0  ...            50.8    male\n343      Dream           18.7  ...            50.2  female\n\n[344 rows x 7 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-siuba/index.html#work-with-groups-of-rows",
    "href": "posts/dplyr-vs-siuba/index.html#work-with-groups-of-rows",
    "title": "Tidy Data Manipulation: dplyr vs siuba",
    "section": "Work with groups of rows",
    "text": "Work with groups of rows\n\nSimple summaries by group\nLet’s suppose we want to compute summaries by groups such as means or medians. Both packages are very similar again: on the R side you have dplyr::group_by() and dplyr::summarize(), while on the Python side you have siuba.group_by() and siuba.summarize().\n\ndplyrsiuba\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  island    bill_depth_mean\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Biscoe               15.9\n2 Dream                18.3\n3 Torgersen            18.4\n\n\n\n\n\n(penguins\n  &gt;&gt; group_by(_.island)\n  &gt;&gt; summarize(bill_depth_mean = _.bill_depth_mm.mean())\n)\n\n      island  bill_depth_mean\n0     Biscoe        15.874850\n1      Dream        18.344355\n2  Torgersen        18.429412\n\n\n\n\n\n\n\nMore complicated summaries by group\nTypically, you want to create multiple different summaries by groups. dplyr provides a lot of flexibility to create new variables on the fly and siuba is able to replicate these capabilities perfectly!\n\ndplyrsiuba\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(count = n(),\n            bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE),\n            flipper_length_median = median(flipper_length_mm, na.rm = TRUE),\n            body_mass_sd = sd(body_mass_g, na.rm = TRUE),\n            share_female = mean(sex == \"female\", na.rm = TRUE))\n\n# A tibble: 3 × 6\n  island   count bill_depth_mean flipper_length_median body_mass_sd share_female\n  &lt;fct&gt;    &lt;int&gt;           &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Biscoe     168            15.9                   214         783.        0.491\n2 Dream      124            18.3                   193         417.        0.496\n3 Torgers…    52            18.4                   191         445.        0.511\n\n\n\n\n\n(penguins\n  &gt;&gt; group_by(_.island)\n  &gt;&gt; summarize(count = _.island.count(),\n               bill_depth_mean = _.bill_depth_mm.mean(),\n               flipper_length_median = _.flipper_length_mm.median(),\n               body_mass_sd = _.body_mass_g.std(),\n               share_female = (_.sex == \"female\").mean())\n)\n\n      island  count  ...  body_mass_sd  share_female\n0     Biscoe    168  ...    782.855743      0.476190\n1      Dream    124  ...    416.644112      0.491935\n2  Torgersen     52  ...    445.107940      0.461538\n\n[3 rows x 6 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-siuba/index.html#conclusion",
    "href": "posts/dplyr-vs-siuba/index.html#conclusion",
    "title": "Tidy Data Manipulation: dplyr vs siuba",
    "section": "Conclusion",
    "text": "Conclusion\nThis post highlights syntactic similarities and differences across R’s dplyr package and Python’s siuba library. One key point emerges: dplyr heavily relies on NSE to enable a syntax that refrains from using strings and column selectors, something that is strictly speaking not possible in Python. However, siuba’s approach using siu expressions and the pipe provide a very similar syntax to dplyr. I want to close this post by emphasizing that both languages and packages have their own merits and I won’t strictly recommend one over the other - maybe in another post 😄"
  },
  {
    "objectID": "posts/dplyr-vs-siuba/index.html#footnotes",
    "href": "posts/dplyr-vs-siuba/index.html#footnotes",
    "title": "Tidy Data Manipulation: dplyr vs siuba",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the unifying principles of the tidyverse: https://design.tidyverse.org/unifying.html.↩︎"
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html",
    "href": "posts/ggplot2-vs-plotnine/index.html",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "",
    "text": "Both ggplot2 and plotnine are based on Leland Wilkinson’s Grammar of Graphics, a set of principles for creating consistent and effective statistical graphics. This means they both use similar syntax and logic for constructing plots, making it relatively easy for users to transition between them. ggplot2, developed by Hadley Wickham, is a cornerstone of the R community and integrates seamlessly with other tidyverse packages. plotnine, on the other hand, is a Python package that attempts to bring ggplot2 functionality and philosophy to Python users, but it is not part of a larger ecosystem (although it works well with pandas, Python’s most popular data manipulation package).\nBoth packages use a layer-based approach, where a plot is built up by adding components like axes, geoms, stats, and scales. However, ggplot2 benefits from R’s native support for data frames and its formula notation, which can make its syntax more concise. plotnine has to adhere to Python’s syntax rules, in particular referring to columns via strings, which can occasionally lead to more verbose code. As you can see in the examples below, the syntactic differences are miniscule.\nThe types of plots that I chose for the comparison heavily draw on the examples given in R for Data Science - an amazing resource if you want to get started with data visualization."
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#loading-packages-and-data",
    "href": "posts/ggplot2-vs-plotnine/index.html#loading-packages-and-data",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "Loading packages and data",
    "text": "Loading packages and data\nWe start by loading the main packages of interest and the popular palmerpenguins package that exists for both R and Python. We then use the penguins data frame as the data to compare all functions and methods below. Note that I drop all rows with missing values because I don’t want to get into related messages in this post.\n\nggplot2plotnine\n\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\npenguins &lt;- na.omit(palmerpenguins::penguins)\n\n\n\n\nfrom plotnine import *\nfrom palmerpenguins import load_penguins\n\npenguins = load_penguins().dropna()"
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#a-full-blown-example",
    "href": "posts/ggplot2-vs-plotnine/index.html#a-full-blown-example",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "A full-blown example",
    "text": "A full-blown example\nLet’s start with an advancved example that combines many different aesthetics at the same time: we plot two columns against each other, use color and shape aesthetics do differentiate species, include separate regression lines for each species, manually set nice labels, and use a theme. Except for the quotation of column names, plotnine has exactly the same syntax as ggplot2 - this is remarkable!\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = bill_depth_mm, \n           color = species, shape = species)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n  labs(x = \"Bill length (mm)\", y = \"Bill Width (mm)\", \n       title = \"Bill length vs. bill width\", \n       subtitle = \"Using geom_point and geom_smooth of the ggplot2 package\",\n       color = \"Species\", shape = \"Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n        aes(x = \"bill_length_mm\", y = \"bill_depth_mm\", \n            color = \"species\", shape = \"species\"))\n  + geom_point()\n  + geom_smooth(method = \"lm\", formula = \"y ~ x\")\n  + labs(x = \"Bill length (mm)\", y = \"Bill width (mm)\", \n         title = \"Bill length vs. bill width\", \n         subtitle = \"Using geom_point and geom_smooth of the plotnine package\",\n         color = \"Species\", shape = \"Species\")\n  + theme_minimal()\n)\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#visualizing-distributions",
    "href": "posts/ggplot2-vs-plotnine/index.html#visualizing-distributions",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\n\nA categorical variable\nLet’s break down the similarity in smaller steps by focusing on simpler examples. If you have a categorical variable and want to compare its relevance in your data, then geom_bar() is your friend.\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = island)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n        aes(x = \"island\"))\n  + geom_bar()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA numerical variable\nIf you have a numerical variable, usually histograms are a good starting point to get a better feeling for the distribution of your data. geom_histogram() with options to control bin widths or number of bins is the aesthetic for this task.\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n       aes(x = \"bill_length_mm\"))\n  + geom_histogram(binwidth = 2)\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nBoth packages also support the geom_density() geom to plot density curves, but I personally wouldn’t recommend to start with densities because they are estimated curves that might obscure underlying data features. However, we look at densities in the next section."
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#visualizing-relationships",
    "href": "posts/ggplot2-vs-plotnine/index.html#visualizing-relationships",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\n\nA numerical and a categorical variable\nTo visualize relationships, you need to have at least two columns. If you have a numerical and a categorical variable, then histograms or densities with groups are a good starting point. The next example illustrates the use of geom_density().\nNote that plotnine still uses the historical size option and not the new linewidth wording (see this blog post here). Maybe this will change in the future, so keep an eye on this issue to stay up to date.\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(linewidth = 0.75, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n       aes(x = \"body_mass_g\", color = \"species\", fill = \"species\"))\n  + geom_density(size = 0.75, alpha = 0.5)\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo categorical columns\nStacked bar plots are a good way to display the relationship between two categorical columns. geom_bar() with the position argument is your aesthetic of choice for this task. Note that you can easily switch to counts by using position = \"identity\" instead of relative frequencies as in the example below.\n\nggplot2plotnine\n\n\n\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, aes(x = \"species\", fill = \"island\"))\n  + geom_bar(position = \"fill\")\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo numerical columns\nScatter plots and regression lines are definitely the most common approach for visualizing the relationship between two numerical columns. Here, the size parameter controls the size of the shapes that you use for the data points. See the first visualization example if you want to see again how to add a regression line.\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(size = 2)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n        aes(x = \"bill_length_mm\", y = \"flipper_length_mm\"))\n  + geom_point(size = 2)\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree or more columns\nYou can include more information by mapping columns to additional aesthetics. For instance, we can map colors and shapes to species and create separate plots for each island by using facets. Facets are actually a great way to extend your figures, so I highly recommend playing around with them using your own data.\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n       aes(x = \"bill_length_mm\", y = \"flipper_length_mm\"))\n  + geom_point(aes(color = \"species\", shape = \"species\"))\n  + facet_wrap(\"~island\")\n)\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#saving-plots",
    "href": "posts/ggplot2-vs-plotnine/index.html#saving-plots",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "Saving plots",
    "text": "Saving plots\nAs a final comparison, let us look at saving plots. Again, the implementations are virtually the same across both packages with the same function name and corresponding options.\n\nggplot2plotnine\n\n\n\npenguins_figure &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) + \n  geom_point()\n\nggsave(penguins_figure, filename = \"penguins-figure.png\",\n       width = 7, height = 5, dpi = 300)\n\n\n\n\npenguins_figure = (\n  ggplot(penguins, \n         aes(x = \"bill_length_mm\", y = \"flipper_length_mm\"))\n  + geom_point()\n)\n\nggsave(penguins_figure, filename = \"penguins-figure.png\",\n       width = 7, height = 5, dpi = 300)"
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#conclusion",
    "href": "posts/ggplot2-vs-plotnine/index.html#conclusion",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "Conclusion",
    "text": "Conclusion\nIn terms of syntax, ggplot2 and plotnine are remarkably similar, with minor differences primarily due to the differences between R and Python:\n\nColumn references are implemented via strings in Python, while you can use unquoted column names in R due to its support of non-standard evaluation.\nThe layer connector + has to come at the end of the line R, not at the start. In Python, it makes more sense to have it at the start because you can comment out code better, but in principle also at the line end is possible.\n\nThe strongest argument in favor of ggplot2, however, is its large ecosystem of extension packages (see, for instance, this repo for a collection of links).\nIf you want to learn more about the power of the grammar of graphics, follow Cédric Scherer and check out his content. For instance, you can find his data visualization workshop notes from posit::conf(2022) here. Thomas Lin Pedersen also does fantastic things with ggplot2 among them creating generative art with code."
  },
  {
    "objectID": "posts/sakura-visualizations/index.html",
    "href": "posts/sakura-visualizations/index.html",
    "title": "Sakura Visualizations",
    "section": "",
    "text": "While I was traveling Japan earlier this year, I had the pleasure to experience sakura, the famous Japanese cherry blossoms. These blossoms are highly significant in Japanese culture, symbolizing the transient nature of life due to their brief blooming period. Sakura typically bloom in spring, from late March to early May, depending on the region. The blossoming of cherry trees is celebrated with hanami, a traditional custom of enjoying the beauty of flowers, often involving picnics under the blooming trees.\nClimate change is causing sakura to bloom earlier and for shorter periods due to rising temperatures and shifting phenological patterns. This disrupts traditional hanami festivals, affects pollination success, and leads to regional variations in bloom timing, with urban areas often experiencing more pronounced shifts.\nIn this blog post, I visualize data with respect to cherry blossom blooming periods over regions and time. I provide a comprehensive guide on loading and preparing the corresponding data for analysis using both R and Python. The analysis spans from 1953 to 2023, leveraging the tidyverse family of packages for R and the ibis-framework for Python, complemented by plotnine for visualizations. I draw some inspiration from this blog post by Yuriko Schuhmacher. My primary focus is to transform and merge datasets detailing the first and full bloom dates of cherry blossoms and visualizing them by region."
  },
  {
    "objectID": "posts/sakura-visualizations/index.html#loading-packages-and-data",
    "href": "posts/sakura-visualizations/index.html#loading-packages-and-data",
    "title": "Sakura Visualizations",
    "section": "Loading packages and data",
    "text": "Loading packages and data\nFor the R version, I rely on the tidyverse family of packages and list the required packages explicitly. For the Python version, I use ibis-framework for data manipulation (see my post on dplyr-vs-ibis for more information) and plotnine for visualizations.\n\nRPython\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(ggplot2)\n\n\n\n\nimport ibis\nfrom ibis import _\nimport ibis.selectors as s\nfrom plotnine import *"
  },
  {
    "objectID": "posts/sakura-visualizations/index.html#download-and-prepare-data",
    "href": "posts/sakura-visualizations/index.html#download-and-prepare-data",
    "title": "Sakura Visualizations",
    "section": "Download and prepare data",
    "text": "Download and prepare data\nI use regional cherry blossom data between 1953 and 2023 from the Japan Meteorological Agency provided on Kaggle. There are two data sets: one table provides the dates when the cherry blossoms start blooming and the second table contains the dates when cherry blossoms reach full bloom. In the code below, I combine both to a long format table with additional columns that indicate the day of the year with full bloom and the days between first and full bloom, respectively.\n\nRPython\n\n\n\nsakura_first_bloom_dates &lt;- read_csv(\"data/sakura_first_bloom_dates.csv\") \nsakura_full_bloom_dates &lt;- read_csv(\"data/sakura_full_bloom_dates.csv\")\n  \nfirst_bloom_long &lt;- sakura_first_bloom_dates |&gt;\n  select(-`30 Year Average 1981-2010`, -Notes) |&gt;\n  rename(location = `Site Name`, is_currently_observed = `Currently Being Observed`) |&gt;\n  pivot_longer(`1953`:`2023`, names_to = \"year\", values_to = \"first_bloom\")\n\nfull_bloom_long &lt;- sakura_full_bloom_dates |&gt;\n  select(-`30 Year Average 1981-2010`, -Notes) |&gt;\n  rename(location = `Site Name`, is_currently_observed = `Currently Being Observed`) |&gt;\n  pivot_longer(`1953`:`2023`, names_to = \"year\", values_to = \"full_bloom\")\n\nsakura_dates &lt;- first_bloom_long |&gt;\n  full_join(full_bloom_long, c(\"location\", \"year\", \"is_currently_observed\")) |&gt;\n  mutate(year = as.integer(year),\n         days_to_full_bloom = as.integer(full_bloom - as.Date(paste(year, \"-01-01\", sep = \"\"))),\n         days_from_first_to_full_bloom = as.integer(full_bloom - first_bloom))\n\n\n\n\nsakura_first_bloom_dates = ibis.read_csv(\"data/sakura_first_bloom_dates.csv\")\nsakura_full_bloom_dates = ibis.read_csv(\"data/sakura_full_bloom_dates.csv\")\n\nfirst_bloom_long = (sakura_first_bloom_dates\n  .drop(\"30 Year Average 1981-2010\", \"Notes\")\n  .rename(location = \"Site Name\", is_currently_observed = \"Currently Being Observed\")\n  .pivot_longer(s.r[\"1953\":\"2023\"], names_to = \"year\", values_to = \"first_bloom\")\n)\n\nfull_bloom_long = (sakura_full_bloom_dates\n  .drop(\"30 Year Average 1981-2010\", \"Notes\")\n  .rename(location = \"Site Name\", is_currently_observed = \"Currently Being Observed\")\n  .pivot_longer(s.r[\"1953\":\"2023\"], names_to = \"year\", values_to = \"full_bloom\")\n)\n\nsakura_dates = (first_bloom_long.outer_join(full_bloom_long, [\"location\", \"year\", \"is_currently_observed\"])\n  .select(~s.contains(\"_right\"))\n  .mutate(year = _.year.cast(\"int32\"))\n  .mutate(days_to_full_bloom = (_.full_bloom - ibis.date(_.year.cast('string') + '-01-01')).cast('interval(\"D\")').cast(\"int32\"),\n          days_from_first_to_full_bloom=(_.full_bloom - _.first_bloom).cast('interval(\"D\")').cast(\"int32\")\n    )\n)\n\n\n\n\nI next add regions that I later use to group the locations. I used a list of regions from Wikipedia as input and let ChatGPT label the locations accordingly. You can find the resulting mapping on GitHub.\n\nRPython\n\n\n\nlocations_regions &lt;- read_csv(\"data/locations_region.csv\") \n\nsouthern_islands &lt;- c(\"Naze\", \"Ishigaki Island\", \"Miyakojima\", \"Naha\", \"Minami Daito Island\")\n\nlocations_regions &lt;- locations_regions |&gt; \n  mutate(region = if_else(location %in% southern_islands, \"Ryukyu Islands\", region))\n\n\n\n\nlocations_regions = ibis.read_csv(\"data/locations_region.csv\")\n\nsouthern_islands = [\"Naze\", \"Ishigaki Island\", \"Miyakojima\", \"Naha\", \"Minami Daito Island\"]\n\nlocations_regions = (locations_regions\n  .mutate(\n    region = ibis.case().when(_.location.isin(southern_islands), \"Ryukyu Islands\").else_(_.region).end()\n  )\n)\n\n\n\n\nBefore we move on to visualizations, let’s add the regions to the sakura dates and keep only regions that are still currently observed and with valid full bloom days and days between first and full bloom. In addition, I drop year 1953 to have exactly 70 years of data. Finally, I want to order the regions from north to south.\n\nRPython\n\n\n\nsakura_data &lt;- sakura_dates |&gt; \n  left_join(locations_regions, join_by(location)) |&gt; \n  filter(is_currently_observed == TRUE &\n          year &gt;= 1954 &\n          !is.na(days_to_full_bloom) & \n          !is.na(days_from_first_to_full_bloom))\n\nsakura_data &lt;- sakura_data |&gt; \n  mutate(region = factor(region, levels = c(\"Hokkaidō\", \"Honshū\", \"Kyūshū\", \"Shikoku\", \"Ryukyu Islands\")))\n\n\n\nSince ibis-framework does not support factor variables, we have to use pandas here.\n\nsakura_data = (sakura_dates\n  .left_join(locations_regions, \"location\")\n  .filter([_.is_currently_observed == True, \n           _.year &gt;= 1954, \n           _.days_to_full_bloom.notnull(),\n           _.days_from_first_to_full_bloom.notnull()])\n)\n\nimport pandas as pd\nsakura_data = sakura_data.to_pandas()\nsakura_data[\"region\"] = pd.Categorical(sakura_data[\"region\"], categories = [\"Hokkaidō\", \"Honshū\", \"Kyūshū\", \"Shikoku\", \"Ryukyu Islands\"], ordered = True)"
  },
  {
    "objectID": "posts/sakura-visualizations/index.html#set-theme-and-color-scheme",
    "href": "posts/sakura-visualizations/index.html#set-theme-and-color-scheme",
    "title": "Sakura Visualizations",
    "section": "Set theme and color scheme",
    "text": "Set theme and color scheme\nI set the theme and colors for the figures below on a global level. Note that setting the figure size in the theme works best for plotnine, while ggplot2 uses the paramters defined in the YAML header of the .qmd file underlying this post.\n\nRPython\n\n\n\ntheme_set(theme_classic(base_size = 16, base_family = \"SF Pro\")) \n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  panel.grid.major = element_blank(),\n  strip.text = element_text(size = 16),\n  strip.background = element_blank(),\n  axis.title.x = element_blank(), \n  axis.title.y = element_blank(),\n  axis.ticks = element_blank(),\n  axis.line = element_blank()\n)\n\ncolors &lt;- c(\"#ffb7c5\", \"#A0522D\")\nbreaks_year &lt;- seq(1950, 2030, by = 20)\n\n\n\nSince plotnine is built on matplotlib, setting the font family can be done more flexibly and consistently using the latter’s font manager.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\n\nfpath = \"/Library/Fonts/SF-Pro.ttf\"\n\nfm.fontManager.addfont(fpath)\nprop = fm.FontProperties(fname = fpath)\nfont_name = prop.get_name()\nplt.rcParams[\"font.family\"] = font_name\n\ntheme_set(theme_classic(base_size = 16) + theme(figure_size = (12, 8)))\n\ntheme_update(\n  panel_grid_minor = element_blank(),\n  panel_grid_major = element_blank(),\n  strip_text = element_text(size = 16),\n  strip_background = element_blank(),\n  axis_title_x = element_blank(),\n  axis_title_y = element_blank(),\n  axis_ticks = element_blank(),\n  axis_line = element_blank()\n)\n\ncolors = [\"#ffb7c5\", \"#A0522D\"]\nbreaks_year = range(1950, 2031, 20)"
  },
  {
    "objectID": "posts/sakura-visualizations/index.html#days-to-full-bloom",
    "href": "posts/sakura-visualizations/index.html#days-to-full-bloom",
    "title": "Sakura Visualizations",
    "section": "Days to full bloom",
    "text": "Days to full bloom\nThe first figure visually represents the full bloom days of cherry blossoms across various regions in Japan over time. Each point on the plot shows the bloom day for a specific year. The line indicates the general trend over time. The regions are separated into individual panels arranged in a single row, allowing for easy comparison. The subtitle highlights a key finding: cities in the northern regions, such as Hokkaidō and Honshū, tend to have earlier bloom dates compared to those in the Ryukyu Islands, which experience later blooms.\n\nRPython\n\n\n\nsakura_data |&gt; \n  ggplot(aes(x = year, y = days_to_full_bloom)) +\n  geom_point(color = colors[1], alpha = 0.9, size = 4, shape = 21, fill = \"white\") +\n  geom_smooth(method = \"loess\", se = FALSE,\n              color = colors[2], linewidth = 2) +\n  facet_wrap(~region, nrow = 1) + \n  labs(title = \"Day of the year with peak cherry tree blossom for regions in Japan since 1954\",\n       subtitle = \"Cities in northern regions Hokkaidō and Honshū exhibit earlier full blooms, while Ryukyu Islands even later\",\n       x = NULL, y = NULL) +\n  scale_x_continuous(breaks = breaks_year) +\n  scale_y_continuous(breaks = seq(30, 150, by = 30))\n\n\n\n\n\n\n\n\n\n\nNote that linewidth is still size in plotnine.\n\n(ggplot(sakura_data, \n         aes(x = \"year\", y = \"days_to_full_bloom\"))\n  + geom_point(color = colors[0], alpha = 0.9, size = 4, shape = \"o\", fill = \"white\")\n  + geom_smooth(method = \"loess\", se = False,\n                color = colors[1], size = 2)\n  + facet_wrap(\"~region\", nrow = 1)\n  + labs(title = \"Day of the year with peak cherry tree blossom for regions in Japan since 1954\",\n         subtitle = \"Cities in northern regions Hokkaidō and Honshū exhibit earlier full blooms, while Ryukyu Islands even later\",\n         x = None, y = None)\n  + scale_x_continuous(breaks = breaks_year)\n  + scale_y_continuous(breaks = range(30, 151, 30))\n)\n\n&lt;Figure Size: (1200 x 800)&gt;"
  },
  {
    "objectID": "posts/sakura-visualizations/index.html#days-from-first-to-full-bloom",
    "href": "posts/sakura-visualizations/index.html#days-from-first-to-full-bloom",
    "title": "Sakura Visualizations",
    "section": "Days from first to full bloom",
    "text": "Days from first to full bloom\nThe next figure illustrates the duration between the initial opening of cherry blossoms and their peak bloom across different regions in Japan over time. Each point represents the number of days for a given year. The linear line highlights the overall trends in blooming duration. The regions are individually faceted in a single row, enabling side-by-side comparisons. The figure shows that Hokkaidō has the shortest and decreasing periods between blossom opening and peak bloom, while regions like Kyūshū and Ryukyu exhibit increasing durations.\n\nRPython\n\n\n\nsakura_data |&gt;\n  ggplot(aes(x = year, y = days_from_first_to_full_bloom)) +\n  geom_point(color = colors[1], alpha = 0.9, size = 4, shape = 21, fill = \"white\") +\n  geom_smooth(method = \"loess\", se = FALSE,\n              color = colors[2], linewidth = 2) +\n  facet_wrap(~region, nrow = 1) +\n  labs(title = \"Days from blossoms opening to peak bloom for regions in Japan since 1954\",\n       subtitle = \"Hokkaidō exhibits the shortest and decreasing blooming periods, while Kyūshū's and Ryukyu's have lengthened\",\n       x = NULL, y = NULL) +\n  scale_x_continuous(breaks = breaks_year)\n\n\n\n\n\n\n\n\n\n\nNote that point shape in plotnine are according to matplotlib.markers, see the list here.\n\n(ggplot(sakura_data, \n        aes(x = \"year\", y = \"days_from_first_to_full_bloom\"))\n  + geom_point(color = colors[0], alpha = 0.9, size = 4, shape = \"o\", fill = \"white\")\n  + geom_smooth(method = \"loess\", se = False, \n                color = colors[1], size = 2)\n  + facet_wrap(\"~region\", nrow = 1)\n  + labs(title = \"Days from blossoms opening to peak bloom for regions in Japan since 1954\",\n         subtitle = \"Hokkaidō exhibits the shortest and decreasing blooming periods, while Kyūshū's and Ryukyu's have lengthened\",\n          x = None, y = None)\n  + scale_x_continuous(breaks = breaks_year)\n)\n\n&lt;Figure Size: (1200 x 800)&gt;"
  },
  {
    "objectID": "posts/sakura-visualizations/index.html#concluding-remarks",
    "href": "posts/sakura-visualizations/index.html#concluding-remarks",
    "title": "Sakura Visualizations",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nThe post includes detailed code snippets for both R and Python, ensuring that readers can follow along regardless of their preferred programming language. I think it is remarkable how similar R and Python syntax can be these days.\nIt would be great to include the daily average temperature in each year as an additional variable for the visualizations (e.g. color intensity of point color) to tackle the question of missing variables that explain the changes. However, for the purpose of this post, it seemed to much work to download the regional temperatures using the unintuitive and restrictive interface of the Japanese Meterological Agency (JMA). If anybody can point me to an alternative data source, I’m happy to include the temperature changes as well!\n\n   Related articles\n    \n      \n        \n            \n            \n               Tidy Data Manipulation: dplyr vs ibis\n               A comparison of R's dplyr and Python's ibis data manipulation packages\n            \n        \n        \n        \n        \n        \n            \n            \n                Tidy Data Visualization: ggplot2 vs plotnine\n                A comparison of implementations of the grammar of graphics in R and Python.\n            \n        \n        \n        \n        \n        \n            \n            \n                Interactive Data Visualization with R\n                A comparison of the dynamic visualization packages ggiraph, plotly and highcharter for the programming language R"
  },
  {
    "objectID": "posts/dax-seasonality/index.html",
    "href": "posts/dax-seasonality/index.html",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "",
    "text": "Seasonalcharts.de claims that stock indices exhibit persistent seasonality that may be exploited through an appropriate trading strategy. As part of a job application, I had to replicate the seasonal pattern for the DAX and then test whether this pattern entails a profitable trading strategy. To sum up, I indeed find that a trading strategy that holds the index only over a specific season outperforms the market significantly, but these results might be driven by a few outliers. Note that the post below references an opinion and is for information purposes only. I do not intend to provide any investment advice.\nThe code is structured in a way that allows for a straight-forward replication of the methodology for other indices. The post uses the following packages:\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(frenchdata)\nlibrary(scales)\nlibrary(fixest)"
  },
  {
    "objectID": "posts/dax-seasonality/index.html#data-preparation",
    "href": "posts/dax-seasonality/index.html#data-preparation",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, download data from yahoo finance using the tidyquant package. Note that the DAX was officially launched in July 1988, so this is where our sample starts.\n\ndax_raw &lt;- tq_get(\n  \"^GDAXI\", get = \"stock.prices\", \n  from = \"1988-07-01\", to = \"2023-10-30\"\n) \n\nThen, select only date and the adjusted price (i.e., closing price after adjustments for all applicable splits and dividend distributions) as the relevant variables and compute summary statistics to check for missing or weird values. The results are virtually the same if we use unadjusted closing prices.\n\ndax &lt;- dax_raw |&gt;\n  select(date, price = adjusted)\n\nWe replace the missing values by the last available index value.\n\ndax &lt;- dax |&gt;\n  arrange(date) |&gt;\n  fill(price, .direction = \"down\")\n\nAs a immediate plausibility check, we plot the DAX over the whole sample period.\n\ndax |&gt;\n  ggplot(aes(x = date, y = price)) +\n  geom_line() + \n  labs(x = NULL, y = \"Adjusted Price\",\n       title = \"Adjusted DAX index price between 1988 and 2023\") +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\nThe main idea of Seasonalcharts is to implement the strategy proposed by Jacobsen and Bouman (2002)1 and Jacobsen and Zhan (2018)2 which they label ‘The Halloween Indicator’ (or ‘Sell in May Effect’). The main finding of these papers is that stock indices returns seem significantly lower during the May-October period than during the remainder of the year. The corresponding trading strategy holds an index during the months November-April, but holds the risk-free asset in the May-October period.\nTo replicate their approach (and avoid noise in the daily data), we focus on monthly returns from now on.\n\ndax_monthly &lt;- dax |&gt;\n  mutate(year = year(date),\n         month = factor(month(date))) |&gt;\n  group_by(year, month) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  arrange(date) |&gt;\n  mutate(ret = price / lag(price) - 1) |&gt;\n  drop_na()\n\nAnd as usual in empirical asset pricing, we do not care about raw returns, but returns in excess of the risk-free asset. We simply add the European risk free rate from the Fama-French data library as the corresponding reference point. Of course, one could use other measures for the risk-free rate, but the impact on the results won’t be substantial.\n\nfactors_ff3_monthly_raw &lt;- download_french_data(\"Fama/French 3 Factors\")\n\nrisk_free_monthly &lt;- factors_ff3_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    year = year(ymd(str_c(date, \"01\"))),\n    month = factor(month(ymd(str_c(date, \"01\")))),\n    rf = as.numeric(RF) / 100,\n    .keep = \"none\"\n  )\n\ndax_monthly &lt;- dax_monthly |&gt; \n  left_join(risk_free_monthly, join_by(year, month)) |&gt; \n  mutate(ret_excess = ret - rf) |&gt; \n  drop_na()"
  },
  {
    "objectID": "posts/dax-seasonality/index.html#graphical-evidence-for-seasonality",
    "href": "posts/dax-seasonality/index.html#graphical-evidence-for-seasonality",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Graphical Evidence for Seasonality",
    "text": "Graphical Evidence for Seasonality\nWe start by first plotting the average returns for each month.\n\ndax_monthly |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret)) |&gt; \n  ggplot(aes(x = month, y = ret, fill = ret &gt; 0)) +\n  geom_col() +\n  scale_y_continuous(labels = percent) + \n  labs(\n    x = \"Month\", y = \"Average DAX Return\", \n    title = \"Average monthly DAX returns between 1988 and 2023\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe figure shows negative returns for June, August, and September, while all other months exhibit positive returns. However, it makes more sense to look at distributions instead of simple means, which might be heavily influenced by outliers. To illustrate distributions, I follow Cedric Scherer and use raincloud plots. which combine halved violin plot, a box plot, and the raw data as some kind of scatter. These plots hence provide detailed visualizations of the distributions.\n\ndax_monthly |&gt; \n  ggplot(aes(x = month, y = ret, group = month)) + \n  ggdist::stat_halfeye(\n    adjust = .5, width = .6, .width = 0, justification = -.3, point_colour = NA\n    ) + \n  geom_boxplot(\n    width = .25, outlier.shape = NA\n    ) +\n  stat_summary(\n    fun = mean, geom=\"point\", color = \"red\", fill = \"red\"\n    ) +\n  geom_point(\n    size = 1.5, alpha = .2, position = position_jitter(seed = 42, width = .1)\n    ) +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n  labs(\n    x = \"Month\", y = \"DAX Return\", \n    title = \"Distributions of monthly DAX returns between 1988 and 2023\",\n    subtitle = \"Red dots indicate means, solid lines indicate medians\"\n  ) \n\n\n\n\n\n\n\n\nThe figure suggests that August and September exhibit considerable negative outliers."
  },
  {
    "objectID": "posts/dax-seasonality/index.html#evaluating-trading-strategies",
    "href": "posts/dax-seasonality/index.html#evaluating-trading-strategies",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Evaluating Trading Strategies",
    "text": "Evaluating Trading Strategies\nLet us now take a look at the average excess returns per month. We also add the standard deviation, 5% and 95% quantiles, and t-statistic of a t-test of the null hypothesis that average returns are zero in a given month.\n\ndax_monthly |&gt;\n  drop_na(ret_excess) |&gt; \n  group_by(Month = month) |&gt;\n  summarize(\n    Mean = mean(ret_excess),\n    SD = sd(ret_excess),\n    Q05 = quantile(ret_excess, 0.05),\n    Q95 = quantile(ret_excess, 0.95),\n    `t-Statistic` = sqrt(n()) * mean(ret_excess) / sd(ret_excess)\n  )\n\n# A tibble: 12 × 6\n   Month     Mean     SD     Q05    Q95 `t-Statistic`\n   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n 1 1      0.00621 0.0552 -0.0911 0.0868         0.666\n 2 2      0.00200 0.0541 -0.0868 0.0775         0.219\n 3 3      0.00486 0.0539 -0.0732 0.0842         0.533\n 4 4      0.0260  0.0599 -0.0502 0.119          2.57 \n 5 5      0.00470 0.0401 -0.0573 0.0610         0.693\n 6 6     -0.00353 0.0476 -0.0934 0.0587        -0.439\n 7 7      0.0131  0.0598 -0.0658 0.0902         1.29 \n 8 8     -0.0235  0.0622 -0.168  0.0347        -2.27 \n 9 9     -0.0244  0.0730 -0.176  0.0613        -2.01 \n10 10     0.0186  0.0660 -0.0960 0.122          1.70 \n11 11     0.0228  0.0483 -0.0455 0.0862         2.79 \n12 12     0.0195  0.0554 -0.0586 0.108          2.08 \n\n\nAugust and September seem to usually exhibit negative excess returns with an average of about -2.4% (statistically significant) over all years, while April is the only months that tend to exhibit statistically significant positive excess returns.\nLet us proceed to test for the presence of statistically significant excess returns due to seasonal patterns. In the above table, I only test for significance for each month separately. To test for positive returns in a joint model, I regress the monthly excess returns on month indicators. Note that I always adjust the standard errors to be heteroskedasticity robust.\n\nmodel_monthly &lt;- feols(\n  ret_excess ~ month, \n  data = dax_monthly,\n  vcov = \"hetero\"\n)\n\nsummary(model_monthly)\n\nOLS estimation, Dep. Var.: ret_excess\nObservations: 423 \nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)  0.006214   0.009331  0.665936 0.505825    \nmonth2      -0.004210   0.013064 -0.322298 0.747391    \nmonth3      -0.001355   0.013045 -0.103901 0.917298    \nmonth4       0.019820   0.013762  1.440170 0.150581    \nmonth5      -0.001516   0.011533 -0.131456 0.895479    \nmonth6      -0.009744   0.012318 -0.791049 0.429372    \nmonth7       0.006857   0.013752  0.498605 0.618325    \nmonth8      -0.029730   0.013951 -2.131118 0.033672 *  \nmonth9      -0.030625   0.015334 -1.997223 0.046460 *  \nmonth10      0.012424   0.014422  0.861410 0.389514    \nmonth11      0.016551   0.012399  1.334888 0.182652    \nmonth12      0.013261   0.013224  1.002802 0.316547    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.056177   Adj. R2: 0.048833\n\n\nSeems like August and September have on average indeed lower returns than January (which is the omitted reference point in this regression). Note that the size of the coefficients from the regression are the same as in the table above (i.e., constant plus coefficient), but the t-statistics are different because we are estimating a joint model now.\nAs the raincloud plots indicated that outliers might drive any statistical significant differences, we estimate the model again after trimming the data. In particular, we drop the top and bottom 1% of observations. This trimming step only drops 10 observations.\n\nret_excess_q01 &lt;- quantile(dax_monthly$ret_excess, 0.01)\nret_excess_q99 &lt;- quantile(dax_monthly$ret_excess, 0.99)\n\nmodel_monthly_trimmed &lt;- feols(\n  ret_excess ~ month, \n  data = dax_monthly |&gt; \n     filter(ret_excess &gt;= ret_excess_q01 & ret_excess &lt;= ret_excess_q99),\n  vcov = \"hetero\"\n)\n   \netable(model_monthly, model_monthly_trimmed, coefstat = \"tstat\")\n\n                    model_monthly model_monthly_t..\nDependent Var.:        ret_excess        ret_excess\n                                                   \nConstant          0.0062 (0.6659)   0.0062 (0.6657)\nmonth2          -0.0042 (-0.3223) -0.0042 (-0.3222)\nmonth3          -0.0014 (-0.1039) -0.0014 (-0.1039)\nmonth4             0.0198 (1.440)   0.0099 (0.8135)\nmonth5          -0.0015 (-0.1315) -0.0015 (-0.1314)\nmonth6          -0.0097 (-0.7910) -0.0097 (-0.7908)\nmonth7            0.0069 (0.4986)   0.0024 (0.1805)\nmonth8          -0.0297* (-2.131)  -0.0201 (-1.602)\nmonth9          -0.0306* (-1.997)  -0.0142 (-1.127)\nmonth10           0.0124 (0.8614)   0.0124 (0.8611)\nmonth11            0.0165 (1.335)    0.0128 (1.071)\nmonth12            0.0133 (1.003)   0.0087 (0.6897)\n_______________ _________________ _________________\nVCOV type       Heteroskeda.-rob. Heteroskeda.-rob.\nObservations                  423               413\nR2                        0.07363           0.03824\nAdj. R2                   0.04883           0.01186\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIndeed, now no month exhibits a statistically significant outperformance compared to January.\nNext, I follow Jacobsen and Bouman (2002) and simply regress excess returns on dummies that indicate specific seasons, i.e., I estimate the model\n\\[ y_t=\\alpha + \\beta D_t + \\epsilon_t,\\]\nwhere \\(D_t\\) is a dummy variable equal to one for the months in a specific season and zero otherwise. We consider both the ‘Halloween’ season (where the dummy is one for November-April). If \\(D_t\\) is statistically significant and positive for the corresponding season, then we take this as evidence for the presence of seasonality effects.\n\nhalloween_months &lt;- c(11, 12, 1, 2, 3, 4)\n\ndax_monthly &lt;- dax_monthly |&gt;\n  mutate(halloween = if_else(month %in% halloween_months, 1L, 0L))\n\nWe again estimate two models to analyze the ‘Halloween’ effect:\n\nmodel_halloween &lt;- feols(\n  ret_excess ~ halloween, \n  data = dax_monthly,\n  vcov = \"hetero\"\n)\n\nmodel_halloween_trimmed &lt;- feols(\n  ret_excess ~ halloween, \n  data = dax_monthly |&gt; \n     filter(ret_excess &gt;= ret_excess_q01 & ret_excess &lt;= ret_excess_q99),\n  vcov = \"hetero\"\n)\n\netable(model_halloween, model_halloween_trimmed, coefstat = \"tstat\")\n\n                  model_halloween model_hallowe..\nDependent Var.:        ret_excess      ret_excess\n                                                 \nConstant        -0.0026 (-0.6254) 0.0013 (0.3592)\nhalloween        0.0162** (2.872) 0.0091. (1.806)\n_______________ _________________ _______________\nVCOV type       Heteroskeda.-rob. Heteroske.-rob.\nObservations                  423             413\nR2                        0.01919         0.00787\nAdj. R2                   0.01685         0.00546\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe indeed find evidence that excess returns are higher during the months November-April relative to the remaining months in the full sample. However, if we remove the top and bottom 1% of observations, then the statistical significant outperformance disappears again.\nAs a last step, let us compare three different strategies: (i) buy and hold the index over the full year, (ii) go long in the index outside of the Halloween season and otherwise hold the risk-free asset, and (iii) go long in the index outside of the Halloween season and otherwise short the index. Below I compare the returns of the three different strategies on an annual basis:\n\ndax_monthly &lt;- dax_monthly |&gt;\n  mutate(\n    ret_excess_halloween = if_else(halloween == 1, ret_excess, 0),\n    ret_excess_halloween_short = if_else(halloween == 1, ret_excess, -ret_excess)\n  )\n\nWhich of these strategies might constitute a better investment opportunity? For a very simple assessment, let us compute the corresponding Sharpe ratios. Note that I annualize Sharpe ratios by multiplying them with \\(\\sqrt{12}\\) which strictly speaking only works under IID distributed returns (which is typically unlikely to be the case), but which suffices for the purpose of this post.\n\nsharpe_ratio &lt;- function(x) {\n  sqrt(12) *  mean(x) / sd(x)\n}\n\nbind_rows(\n  dax_monthly |&gt;\n    summarize(\n      `Buy and Hold` = sharpe_ratio(ret),\n      `Halloween` = sharpe_ratio(ret_excess_halloween),\n      `Halloween-Short` = sharpe_ratio(ret_excess_halloween_short)\n    ) |&gt; \n    mutate(Data = \"Full\"),\n  dax_monthly |&gt; \n    filter(ret_excess &gt;= ret_excess_q01 & ret_excess &lt;= ret_excess_q99) |&gt;\n    summarize(\n      `Buy and Hold` = sharpe_ratio(ret_excess),\n      `Halloween` = sharpe_ratio(ret_excess_halloween),\n      `Halloween-Short` = sharpe_ratio(ret_excess_halloween_short)\n    ) |&gt; \n    mutate(Data = \"Trimmed\")\n) |&gt; \n  select(Data, everything())\n\n# A tibble: 2 × 4\n  Data    `Buy and Hold` Halloween `Halloween-Short`\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n1 Full             0.458     0.596             0.479\n2 Trimmed          0.394     0.502             0.305\n\n\nThe Sharpe ratio suggests that the Halloween strategy is a better investment opportunity than the other strategies for both the full and the trimmed sample. Shorting the market in the Halloween season even leads to worse performance than just staying invested in the market the whole time once we drop the outliers.\nTo sum up, this post showed some simple data-related issues that we should consider when we analyze return data. Overall, we could find strong support for this seasonality effect from a statistical perspective once we get rid of a few extreme observations."
  },
  {
    "objectID": "posts/dax-seasonality/index.html#footnotes",
    "href": "posts/dax-seasonality/index.html#footnotes",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBouman, Sven and Jacobsen, Ben (2002). “The Halloween Indicator, ‘Sell in May and Go Away’: Another Puzzle”, The American Economic Review, Vol. 92, No. 5, pp. 1618-1635, https://www.jstor.org/stable/3083268↩︎\nJacobsen, Ben and Zhang, Cherry Yi (2018), “The Halloween Indicator, ‘Sell in May and Go Away’: Everywhere and All the Time”, https://ssrn.com/abstract=2154873↩︎"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "",
    "text": "In this post, I provide a simple approach to scrape Environmental, Social and Governance (ESG) information from Yahoo Finance (e.g., Apple) using the programming language R. Yahoo Finance provides total ESG scores, environment, social and governance risk scores, as well as controversy levels, all compiled by Sustainalytics (which is by now owned by Morningstar). My code builds on the walk-through by Kyle Ruden, which I adapted to the current page structure of Yahoo Finance and my own coding style. In addition, I added a few steps that I, as web scraping newbie, had to look up while going through his guide.\nTo begin with, I want to urge you to read at least the legal and ethical considerations put forward by Kyle. Most importantly, I want to mention that, when performing web scraping tasks, it is both good practice and often required to set a custom user agent request header to identify yourself, as well as sending requests at a modest rate to ‘smell like a human’. I consider both of these key aspects in my code below.\nThroughout this post, I rely on the following packages:\nlibrary(tidyverse) # overall grammar\nlibrary(tidytext)  # only for reorder_within & scale_y_reordered functions\nlibrary(scales)    # only for percent function\nlibrary(httr2)     # for making http requests\nlibrary(rvest)     # for web scraping function\nlibrary(robotstxt) # only for paths_allowed function"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#get-symbols",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#get-symbols",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Get Symbols",
    "text": "Get Symbols\nFirst, we want to get some companies for which we want to scrap ESG information from Yahoo Finance. Let us get a table of symbols and industry information of the S&P 500 constituents from Wikipedia. The function read_html normalizes the page to a valid XML document. html_nodes then allows us to point exactly to the table we can find on the website using the name of the CSS node. html_table then parses the HTML table into a data frame. Note that, as one of the last steps, we need to replace all dots in the symbols with dashes to get the symbols used by Yahoo Finance.\n\nwikipedia_link &lt;- \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n\nsymbols &lt;- read_html(wikipedia_link) |&gt; \n  html_nodes(css = \"table[id='constituents']\") |&gt; \n  html_table() \n\nsymbols &lt;- symbols[[1]] |&gt; \n  select(symbol = Symbol, \n         company = Security, \n         sector = `GICS Sector`, \n         industry = `GICS Sub-Industry`) |&gt; \n  mutate(symbol = str_replace(symbol, \"[.]\", \"-\")) |&gt; \n  arrange(symbol)\n\nThe following chunk prints what we got from Wikipedia. We will use the sector information in the last section of this post where we take a quick look at the scraped data.\n\nsymbols\n\n# A tibble: 503 × 4\n   symbol company                 sector                 industry               \n   &lt;chr&gt;  &lt;chr&gt;                   &lt;chr&gt;                  &lt;chr&gt;                  \n 1 A      Agilent Technologies    Health Care            Life Sciences Tools & …\n 2 AAL    American Airlines Group Industrials            Passenger Airlines     \n 3 AAPL   Apple Inc.              Information Technology Technology Hardware, S…\n 4 ABBV   AbbVie                  Health Care            Biotechnology          \n 5 ABNB   Airbnb                  Consumer Discretionary Hotels, Resorts & Crui…\n 6 ABT    Abbott                  Health Care            Health Care Equipment  \n 7 ACGL   Arch Capital Group      Financials             Property & Casualty In…\n 8 ACN    Accenture               Information Technology IT Consulting & Other …\n 9 ADBE   Adobe Inc.              Information Technology Application Software   \n10 ADI    Analog Devices          Information Technology Semiconductors         \n# ℹ 493 more rows"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#locate-esg-information",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#locate-esg-information",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Locate ESG Information",
    "text": "Locate ESG Information\nThe point where I struggled when I tried to replicate other guides was the search for the exact location of the information that I want to scrape (and the fact that the old locations seemed to have changed). After some trial and error, it turns out that it is really easy. Once you download a web page, you can in principle either use CSS nodes or XML paths to extract information using html_nodes() as above. However, the CSS nodes on Yahoo Finance have a weird structure that is apparently not straight-forward to use in this function. Fortunately, XML paths work perfectly! Google will explain to you what these terms mean, I only demonstrate how you find the relevant paths which we use in the scraping function below.\nLet us stick to Apple as our main example and go to the sustainability tab on Yahoo Finance. If we right-click on the ESG score (e.g., using Google Chrome), we can see the the option to ‘Inspect’.\n\nOnce you click on it, a tab to the right opens where you see the underlying code. What is even more useful is the fact that the browser highlights the corresponding elements on the website as you hover over the code. This way, it is really easy to locate the information we are after. So we click on the relevant element and we copy the XML path.\n\nSo the location of the total ESG score on the page is: '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[1]/div/div[2]/div[1]'\nWe can just point-and-click on all the items we want to scrap and collect the relevant XML paths. Once we downloaded a page, we just tell html_node() where to look for the information we want and afterwards how to parse it."
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#define-functions-for-scraping",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#define-functions-for-scraping",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Define Functions for Scraping",
    "text": "Define Functions for Scraping\nMy function to scrap ESG data takes two main inputs: the stock symbol and your user agent. We got the symbols from Wikipedia, but we need to define our own user agent. For instance, I use an agent that looks like this:\n\nagent &lt;- \"Your Name (your@email.com). Doing personal research.\"\n\nThe main functions then proceeds as follows:\n\nConstruct the link of the page we want to download.\nCheck if scraping is allowed.\nDownload the page.\nExtract individual information using the XML paths we manually extracted following the point-and-click procedure from above.\nCollect all information in a table.\n\nLet us start with a function that scrapes a page for a specific symbol:\n\nscrape_sustainability_page &lt;- function(symbol, agent, max_tries = 10) {\n  link &lt;- paste0(\n    \"https://finance.yahoo.com/quote/\", symbol, \"/sustainability?p=\", symbol\n  )\n  \n  check &lt;- suppressMessages(robotstxt::paths_allowed(link))\n  \n  if (check == TRUE) {\n    resp &lt;- request(link) |&gt; \n      req_user_agent(agent) |&gt; \n      req_retry(max_tries = max_tries) |&gt; \n      req_perform()\n    \n    page &lt;- resp$body |&gt; \n      read_html()\n    \n    return(page)\n  } else {\n    stop(paste0(\"No bots allowed on page '\", link ,\"'!\"))\n  }\n}\n\nThe second function extracts the relevant information from the scraped pages and returns it as a table.\n\nextract_esg_data &lt;- function(symbol, page) {\n  scrape_date &lt;- Sys.time()\n  \n  total_esg_score &lt;- page|&gt; \n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[1]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  total_esg_percentile &lt;- page|&gt; \n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[1]/div/div[2]/div[2]/span')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  environment_risk_score &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[2]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  social_risk_score &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[3]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number() \n  \n  governance_risk_score &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[4]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  controversy_level &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[2]/div[2]/div/div/div/div[1]/div')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  last_update_date &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[3]/span[2]/span')|&gt;\n    html_text()\n  \n  last_update_date &lt;- str_remove(last_update_date, \"Last updated on \")\n  \n  tibble(\n    symbol,\n    scrape_date,\n    total_esg_score,\n    environment_risk_score,\n    social_risk_score,\n    governance_risk_score,\n    controversy_level,\n    last_update_date\n  )\n}"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#scrape-esg-data",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#scrape-esg-data",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Scrape ESG Data",
    "text": "Scrape ESG Data\nNow, let us put everything together: we loop over all symbols to download the relevant pages and extract the relevant ESG data. I store each instance of esg_data because the scraping process is very likely to be interrupted by Yahoo Finance as it starts to block requests after some time. By using a loop, I can interrupt the execution any time and continue with the last index.\n\nfor (j in 1:nrow(symbols)) {\n  page &lt;- scrape_sustainability_page(symbols$symbol[j], agent)\n  esg_data &lt;- extract_esg_data(symbols$symbol[j], page)\n  write_rds(esg_data, paste0(\"data/esg_data_\", symbols$symbol[j], \".rds\"))\n}\n\nesg_data &lt;- list.files(\"data/\", full.names = TRUE) |&gt; \n  map_df(read_rds)\n\nThe code chunk from above takes a couple of hours in the current specification because of the increasing waiting times. The whole table then looks like this and also includes our initial example Apple:\n\nesg_data \n\n# A tibble: 503 × 8\n   symbol scrape_date         total_esg_score environment_risk_score\n   &lt;chr&gt;  &lt;dttm&gt;                        &lt;dbl&gt;                  &lt;dbl&gt;\n 1 A      2023-11-27 00:29:33              15                    0.3\n 2 AAL    2023-11-27 00:29:33              29                   11.5\n 3 AAPL   2023-11-27 00:29:35              17                    0.6\n 4 ABBV   2023-11-27 00:29:37              28                    1.1\n 5 ABNB   2023-11-27 00:29:38              NA                   NA  \n 6 ABT    2023-11-27 00:29:39              25                    3  \n 7 ACGL   2023-11-27 00:29:39              21                    1.5\n 8 ACN    2023-11-27 00:29:40              10                    0.3\n 9 ADBE   2023-11-27 00:29:41              12                    1.9\n10 ADI    2023-11-27 00:29:41              24                   10.1\n# ℹ 493 more rows\n# ℹ 4 more variables: social_risk_score &lt;dbl&gt;, governance_risk_score &lt;dbl&gt;,\n#   controversy_level &lt;dbl&gt;, last_update_date &lt;chr&gt;"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#quick-evaluation-of-esg-scores",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#quick-evaluation-of-esg-scores",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Quick Evaluation of ESG Scores",
    "text": "Quick Evaluation of ESG Scores\nLet us take a quick look at the data we collected. First, let us check the overall coverage of our sample:\n\nscales::percent(nrow(na.omit(esg_data)) / nrow(esg_data))\n\n[1] \"86%\"\n\n\nThis is not too bad. I believe that for most of the companies without ESG scores in my sample, Yahoo Finance does not provide any data. Admittedly, I should check manually at some point, but for the purpose of this post, this is definitely a success. To analyze sector-level breakdowns, I construct a summary table which I use as the main source for the following figures.\n\nesg_scores_sector &lt;- symbols |&gt;\n  left_join(esg_data, join_by(symbol)) |&gt; \n  group_by(sector)|&gt;\n  summarize(companies = n(),\n            coverage = sum(!is.na(total_esg_score)) / n(),\n            across(c(contains(\"score\"), controversy_level), \n                   ~mean(., na.rm = TRUE)))|&gt;\n  arrange(-coverage)\n\nThe first figure gives us the coverage per sector. All real estate companies have ESG scores, while only a bit more than three quarters of communication services feature this information.\n\nesg_scores_sector|&gt;\n  mutate(labels = paste0(companies * coverage, \" out of \", companies))|&gt;\n  ggplot(aes(y = reorder(sector, coverage), \n             x = coverage, fill = factor(round(coverage, 0)))) +\n  geom_col(show.legend = FALSE) + \n  theme_minimal() + \n  geom_text(aes(label = labels), hjust = 1.1, color = \"white\") +\n  coord_cartesian(xlim = c(0, 1)) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = NULL, y = NULL,\n       title = \"Number of companies with ESG scores per sector\",\n       subtitle = \"Based on Yahoo Finance and S&P 500 data as of November 2023\")\n\n\n\n\n\n\n\n\nNext, I want to look at average ESG scores by sector. For instance, the real estate sector has the lowest total ESG score, indicating the lowest degree to which a sector’s business value is at risk driven by environmental, social and governance risks. Financials exhibit the the lowest environmental risk, while the energy sector (at least the part included in the S&P 500) has the highest exposure to environmental risks.\n\nesg_scores_sector|&gt;\n  pivot_longer(cols = contains(\"score\"))|&gt;\n  mutate(name = str_to_title(str_replace_all(name, \"_\", \" \")),\n         name = factor(name),\n         sector = tidytext::reorder_within(sector, -value, name))|&gt;\n  ggplot(aes(y = sector, x = value, fill = name)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~name, scales = \"free_y\") +\n  theme_minimal() + \n  tidytext::scale_y_reordered() +\n  geom_text(aes(label = round(value, 0)), hjust = 1.1, color = \"white\") +\n  labs(y = NULL, x = NULL,\n       title = \"Average ESG scores per sector\",\n       subtitle = \"Based on Yahoo Finance and S&P 500 data as of November 2023\")\n\n\n\n\n\n\n\n\nFinally, I am also interested in the average controversy level which measures to which degree companies are involved in incidents and events that may negatively impact stakeholders, the environment or their operations. I decided to plot the controversy of each sector relative to the average overall controversy. Real estate and information technology seem to be far less controverse than consumer staples and communication services.\n\nesg_scores_sector|&gt;\n  mutate(controversy_relative = controversy_level - mean(controversy_level)) |&gt; \n  ggplot(aes(y = reorder(sector, -controversy_relative), \n             x = controversy_relative, fill = (controversy_relative &lt; 0))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_minimal() + theme(legend.position = \"none\") + \n  coord_cartesian(xlim = c(-1.5, 1.5)) +\n  labs(y = NULL, x = NULL,\n       title = \"Average sector-level controversy relative to overall controversy\",\n       subtitle = \"Based on Yahoo Finance and S&P 500 data as of November 2023\")\n\n\n\n\n\n\n\n\nI think there is a lot more interesting stuff to uncover using the ESG scores, but for now I’ll leave it at that. I am nonetheless surprised, how easy scraping information from websites is using these amazing packages."
  },
  {
    "objectID": "posts/data-storage-comparison/index.html",
    "href": "posts/data-storage-comparison/index.html",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "",
    "text": "Sharing data between different collaborators, machines, or programming languages can be cumbersome for many reasons. In this post, I look into the issue of column types and how different storage technologies handle them. I focus on self-contained technologies that are easy to install and run on your machine without setting up a separate backend server. This requirement typically arises in academic contexts, educational settings, or when you quickly want to prototype something without spending time on setting up a data backend.\nI start with simple CSV, then move on to the popular SQLite database before I look at the rising star DuckDB. We close the comparison with a look at the Parquet and Feather file formats. I always check how the column type depends on the language that is used to store the data in the corresponding storage technology."
  },
  {
    "objectID": "posts/data-storage-comparison/index.html#create-example-data",
    "href": "posts/data-storage-comparison/index.html#create-example-data",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "Create example data",
    "text": "Create example data\nLet us start this blog post by creating some example data frame with the most important column types that one typically encounters in data analysis. The code chunks create the example data. Note that I create a little helper function that allows us to quickly extract the column types for a data frame, while we can use the .dtypes method in pandas to achieve the same.\n\nRPython\n\n\n\nlibrary(dplyr)\n\ndata_r &lt;- tibble(\n  character_column = c(\"A\", \"B\", \"C\", \"D\"), \n  date_column = as.Date(c(\"2023-01-01\", \"2023-02-01\", \"2023-03-01\", \"2023-04-01\")),\n  datetime_column = as.POSIXct(c(\"2023-01-01 10:00:00\", \"2023-02-01 11:00:00\", \"2023-03-01 12:00:00\", \"2023-04-01 13:00:00\")),\n  numeric_column = c(1.5, 2.5, 3.5, 4.5),\n  integer_column = as.integer(c(1, 2, 3, 4)),\n  logical_column = c(TRUE, FALSE, FALSE, TRUE)\n)\n\nextract_column_classes &lt;- function(df) {\n  sapply(sapply(df, class), function(x) paste(x, collapse = \", \"))\n}\n\ntibble(\"data_r\" = extract_column_classes(data_r))\n\n# A tibble: 6 × 1\n  data_r         \n  &lt;chr&gt;          \n1 character      \n2 Date           \n3 POSIXct, POSIXt\n4 numeric        \n5 integer        \n6 logical        \n\n\n\n\n\nimport pandas as pd\n\ndata_python = pd.DataFrame({\n  \"character_column\": [\"A\", \"B\", \"C\", \"D\"],\n  \"date_column\": pd.to_datetime([\"2023-01-01\", \"2023-02-01\", \"2023-03-01\", \"2023-04-01\"]),\n  \"datetime_column\": pd.to_datetime([\"2023-01-01 10:00:00\", \"2023-02-01 11:00:00\", \"2023-03-01 12:00:00\", \"2023-04-01 13:00:00\"]),\n  \"numeric_column\": [1.5, 2.5, 3.5, 4.5],\n  \"integer_column\": [1, 2, 3, 4],\n  \"logical_column\": [True, False, False, True]\n})\n\ndata_python.dtypes\n\ncharacter_column            object\ndate_column         datetime64[ns]\ndatetime_column     datetime64[ns]\nnumeric_column             float64\ninteger_column               int64\nlogical_column                bool\ndtype: object\n\n\n\n\n\nThe following table shows the mapping between the different column types in the example above.\n\n\n\nR\nPython\n\n\n\n\ncharacter\nobject\n\n\ndate\ndatetime\n\n\ndatetime\ndatetime\n\n\nnumeric\nfloat\n\n\ninteger\ninteger\n\n\nlogical\nbool\n\n\n\nA few differences immediately arise:\n\npandas uses the object type to denote character columns.1\npandas does not have a date type and rather reuses datetime with a different format.\n\nThe other column types might have different names, but they are essentially equivalent."
  },
  {
    "objectID": "posts/data-storage-comparison/index.html#csv",
    "href": "posts/data-storage-comparison/index.html#csv",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "CSV",
    "text": "CSV\nCSV (Comma-Separated Values) files are a popular data format, renowned for their simplicity, widespread support, and ease of use. Their simple structure, consisting of plain text with values separated by commas, makes them an accessible choice for data storage and exchange.\nHowever, CSV files have their limitations. They are not the most efficient format for handling large datasets, as they can become cumbersome and slow to process. Additionally, CSVs are limited in their ability to handle complex data structures or to store metadata. They lack capabilities for data typing, hierarchy, and relationships, which are essential for more complex data management needs. This limitation makes them less suitable for applications requiring advanced data storage and retrieval functionalities.\n\nRPython\n\n\nI use the readr package to write and read CSV files in this post. The nice thing is that readr actually preserves the column types through its powerful built-in parser.\n\nlibrary(readr)\n\nwrite_csv(data_r, file = \"data_r.csv\")\ndata_r_csv &lt;- read_csv(\"data_r.csv\")\n\nglimpse(data_r_csv)\n\nRows: 4\nColumns: 6\n$ character_column &lt;chr&gt; \"A\", \"B\", \"C\", \"D\"\n$ date_column      &lt;date&gt; 2023-01-01, 2023-02-01, 2023-03-01, 2023-04-01\n$ datetime_column  &lt;dttm&gt; 2023-01-01 09:00:00, 2023-02-01 10:00:00, 2023-03-01 …\n$ numeric_column   &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5\n$ integer_column   &lt;dbl&gt; 1, 2, 3, 4\n$ logical_column   &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE\n\n\nThanks to the smart parsing capabilities of read_csv(), the column types are even aligned between R and Python when using the CSV file format.\n\ndata_python_csv &lt;- read_csv(\"data_python.csv\")\n\ntibble(\n  \"data_r_csv\" = extract_column_classes(data_r_csv),\n  \"data_python_csv\" = extract_column_classes(data_python_csv)\n)\n\n# A tibble: 6 × 2\n  data_r_csv      data_python_csv\n  &lt;chr&gt;           &lt;chr&gt;          \n1 character       character      \n2 Date            Date           \n3 POSIXct, POSIXt POSIXct, POSIXt\n4 numeric         numeric        \n5 numeric         numeric        \n6 logical         logical        \n\n\n\n\nTo write and read CSV files, I use the corresponding pandas methods. To parse the dates correctly when reading in data, you actually need to add parse_dates = [\"date_column\", \"datetime_column\"] to pandas.read_csv(). If you don’t specify this, then pandas automatically reads dates as strings.\n\ndata_python.to_csv(\"data_python.csv\", index = False)\ndata_python_csv = pd.read_csv(\"data_python.csv\")\n\ndata_python_csv.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4 entries, 0 to 3\nData columns (total 6 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   character_column  4 non-null      object \n 1   date_column       4 non-null      object \n 2   datetime_column   4 non-null      object \n 3   numeric_column    4 non-null      float64\n 4   integer_column    4 non-null      int64  \n 5   logical_column    4 non-null      bool   \ndtypes: bool(1), float64(1), int64(1), object(3)\nmemory usage: 292.0+ bytes\n\n\nThe missing date parsing specifications actually leads to an alignment with data that was written with R because dates are just handled as strings.\n\ndata_r_csv = pd.read_csv(\"data_r.csv\")\n\npd.DataFrame({\n  \"data_r_csv\": data_r_csv.dtypes, \n  \"data_python_csv\": data_python_csv.dtypes\n})\n\n                 data_r_csv data_python_csv\ncharacter_column     object          object\ndate_column          object          object\ndatetime_column      object          object\nnumeric_column      float64         float64\ninteger_column        int64           int64\nlogical_column         bool            bool\n\n\n\n\n\nIf you are willing to explicitly specify the date columns in Python, then CSV is actually a valid option to transfer data between R and Python. We could even open CSV files in Excel and manipulate them there, so the simplicity is quite striking.\nIf you work with large amounts of data or if you want to parallelize read or write operations on the same file, then CSV is definitely not for you. On the one hand, it is hard to read only parts of a CSV file (e.g., only rows that fulfill certain criteria). On the other hand, you will most likely run into all kinds of concurrency issues due to file locking.\nWith these issues in mind, let us move on to more advanced storage technologies."
  },
  {
    "objectID": "posts/data-storage-comparison/index.html#sqlite",
    "href": "posts/data-storage-comparison/index.html#sqlite",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "SQLite",
    "text": "SQLite\nSQLite is a highly regarded, lightweight, file-based SQL database that stands out for its simplicity and ease of use. As a self-contained, serverless database engine, it provides a robust platform for storing and managing data without the need for a separate server setup. This feature makes SQLite exceptionally suitable for applications that require an embedded database system or for development purposes.\nHowever, SQLite has its limitations. It is not typically recommended for very large-scale applications or those requiring high levels of concurrency. SQLite handles concurrency at a basic level but is not optimized for situations where numerous processes need to write to the database simultaneously. For such applications, more robust database systems like PostgreSQL or MySQL are generally preferred. Nonetheless, for smaller applications, educational purposes, and situations where a lightweight and easy-to-deploy database is needed, SQLite is an excellent choice.\n\nRPython\n\n\nThe RSQLite package provides a seamless integration of SQLite databases into R projects. The typical workflow starts with initializing a connection to a database (which also creates an SQLite database if it does not exist). To check how SQLite treats our column types, I just write the data to the local database and then read it again into a different data frame. Note that it is also good practice to close database connections again after you are done with whatever you need from it (in particular when multiple processes target the same database).\nWe can see that the column types are not automatically preserved, since dates, datetime and logical columns are converted to integers.2 Note that you can use the extended_types = TRUE option in dbConnect() if you want your SQLite database to handle dates correctly in R (the other programming languages don’t care about this option).\n\nlibrary(RSQLite)\n\ncon_sqlite_r &lt;- dbConnect(SQLite(), \"data_r.sqlite\")\ndbWriteTable(con_sqlite_r, \"data\", data_r, overwrite = TRUE)\ndata_r_sqlite &lt;- dbReadTable(con_sqlite_r, \"data\")\ndbDisconnect(con_sqlite_r)\n\nglimpse(data_r_sqlite)\n\nRows: 4\nColumns: 6\n$ character_column &lt;chr&gt; \"A\", \"B\", \"C\", \"D\"\n$ date_column      &lt;dbl&gt; 19358, 19389, 19417, 19448\n$ datetime_column  &lt;dbl&gt; 1672563600, 1675245600, 1677668400, 1680346800\n$ numeric_column   &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5\n$ integer_column   &lt;int&gt; 1, 2, 3, 4\n$ logical_column   &lt;int&gt; 1, 0, 0, 1\n\n\nIf we now read from the SQLite databases created in Python, we can see that things get even more complicated: pandas stores dates as characters, so you need to convert them to dates or datetime.\n\ncon_sqlite_python &lt;- dbConnect(SQLite(), \"data_python.sqlite\")\ndata_python_sqlite &lt;- dbReadTable(con_sqlite_python, \"data\")\ndbDisconnect(con_sqlite_python)\n\ntibble(\n  \"data_r_sqlite\" = extract_column_classes(data_r_sqlite),\n  \"data_python_sqlite\" = extract_column_classes(data_python_sqlite)\n)\n\n# A tibble: 6 × 2\n  data_r_sqlite data_python_sqlite\n  &lt;chr&gt;         &lt;chr&gt;             \n1 character     character         \n2 numeric       character         \n3 numeric       character         \n4 numeric       numeric           \n5 integer       integer           \n6 integer       integer           \n\n\n\n\nI use the sqlite3 package to perform the same tasks as in R: create a database (or connect to an existing one), write the data to the database, and read the data again to\nSimilar to CSV, you have to add the parse_dates = [\"date_column\", \"datetime_column\"] option to correctly read back in dates using pandas.read_sql_query(). Otherwise dates are simply string columns.\n\nimport sqlite3\n\ncon_sqlite_python = sqlite3.connect(\"data_python.sqlite\")\nres = data_python.to_sql(\"data\", con_sqlite_python, if_exists = \"replace\", index = False)\ndata_python_sqlite = pd.read_sql_query(\"SELECT * FROM data\", con_sqlite_python)\ncon_sqlite_python.close()\n\ndata_python_sqlite.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4 entries, 0 to 3\nData columns (total 6 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   character_column  4 non-null      object \n 1   date_column       4 non-null      object \n 2   datetime_column   4 non-null      object \n 3   numeric_column    4 non-null      float64\n 4   integer_column    4 non-null      int64  \n 5   logical_column    4 non-null      int64  \ndtypes: float64(1), int64(2), object(3)\nmemory usage: 320.0+ bytes\n\n\nWhen I compare the SQLite data to the database that I have created in R, we can see that dates are handled differently because R uses integers to store date and datetime, which are actually interpreted as numeric by Python.\n\ncon_sqlite_r = sqlite3.connect(\"data_r.sqlite\")\ndata_r_sqlite = pd.read_sql_query(\"SELECT * FROM data\", con_sqlite_r)\ncon_sqlite_r.close()\n\npd.DataFrame({\n  \"data_r_sqlite\": data_r_sqlite.dtypes, \n  \"data_python_sqlite\": data_python_sqlite.dtypes\n})\n\n                 data_r_sqlite data_python_sqlite\ncharacter_column        object             object\ndate_column            float64             object\ndatetime_column        float64             object\nnumeric_column         float64            float64\ninteger_column           int64              int64\nlogical_column           int64              int64\n\n\n\n\n\nIf you stay in the same programming language and only work with up to a couple of giga bytes of data, then SQLite is a great database. However, if you want to pull data from the same data base in different programming languages, then you have to be really careful with respect to column types."
  },
  {
    "objectID": "posts/data-storage-comparison/index.html#duckdb",
    "href": "posts/data-storage-comparison/index.html#duckdb",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "DuckDB",
    "text": "DuckDB\nDuckDB is an emerging database management system that is gaining attention for its remarkable efficiency and user-friendliness. It is specifically designed to be an OLAP (Online Analytical Processing) database, making it well-suited for executing analytical queries on large datasets. This focus positions DuckDB as a highly efficient tool for data analytics. One of the standout features of DuckDB is its ability to run directly within data analysis environments. It integrates seamlessly with popular data science languages and tools, allowing data scientists to perform analysis within their familiar programming environment.\nHowever, being a relatively new addition to the world of database management systems, DuckDB might not yet offer the same level of community support, breadth of tools, and integrations as more established databases (such as SQLite).\n\nRPython\n\n\nThe duckdb package contains everything we need to set up the database and execute the same steps as with SQLite. If I write the data to and then read it back in, I get the same column types again - this is great!\n\nlibrary(duckdb)\n\ncon_duckdb_r &lt;- dbConnect(duckdb(), \"data_r.duckdb\")\ndbWriteTable(con_duckdb_r, \"data\", data_r, overwrite = TRUE)\ndata_r_duckdb &lt;- dbReadTable(con_duckdb_r, \"data\")\ndbDisconnect(con_duckdb_r, shutdown = TRUE)\n\nglimpse(data_r_duckdb)\n\nRows: 4\nColumns: 6\n$ character_column &lt;chr&gt; \"A\", \"B\", \"C\", \"D\"\n$ date_column      &lt;date&gt; 2023-01-01, 2023-02-01, 2023-03-01, 2023-04-01\n$ datetime_column  &lt;dttm&gt; 2023-01-01 09:00:00, 2023-02-01 10:00:00, 2023-03-01 …\n$ numeric_column   &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5\n$ integer_column   &lt;int&gt; 1, 2, 3, 4\n$ logical_column   &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE\n\n\nIf I read data from the database create in Python, we only see a small difference: while we have Date in R, we get a POSIXct column from the Python database. For many applications, this difference should have no impact. However, if you work with time series data, make sure to have the correct date format before you proceed!\n\ncon_duckdb_python &lt;- dbConnect(duckdb(), \"data_python.duckdb\")\ndata_python_duckdb &lt;- dbReadTable(con_duckdb_python, \"data\")\n\ntibble(\n  \"data_r_duckdb\" = extract_column_classes(data_r_duckdb),\n  \"data_python_duckdb\" = extract_column_classes(data_python_duckdb)\n)\n\n# A tibble: 6 × 2\n  data_r_duckdb   data_python_duckdb\n  &lt;chr&gt;           &lt;chr&gt;             \n1 character       character         \n2 Date            POSIXct, POSIXt   \n3 POSIXct, POSIXt POSIXct, POSIXt   \n4 numeric         numeric           \n5 integer         integer           \n6 logical         integer           \n\n\n\n\nI use the duckdb package to create the database and the sqlalchemy package to handle the connection. Note that I don’t have to disconnect manually because the engine is taking care of this automatically. Again, the great thing is that we have the same column types for the data that I put into and the data that I have pulled out of the database!\n\nimport duckdb\nfrom sqlalchemy import create_engine\n\ncon_duckdb_python = create_engine(\"duckdb:///data_python.duckdb\")\nres = data_python.to_sql(\"data\", con_duckdb_python, if_exists = \"replace\", index = False)\ndata_python_duckdb = pd.read_sql_query(\"SELECT * FROM data\", con_duckdb_python)\n\ndata_python_duckdb.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4 entries, 0 to 3\nData columns (total 6 columns):\n #   Column            Non-Null Count  Dtype         \n---  ------            --------------  -----         \n 0   character_column  4 non-null      object        \n 1   date_column       4 non-null      datetime64[ns]\n 2   datetime_column   4 non-null      datetime64[ns]\n 3   numeric_column    4 non-null      float64       \n 4   integer_column    4 non-null      int64         \n 5   logical_column    4 non-null      bool          \ndtypes: bool(1), datetime64[ns](2), float64(1), int64(1), object(1)\nmemory usage: 292.0+ bytes\n\n\nIf I now read the data from the database generated in R, then we see that the only difference again arises in the date column, but at least the datetime columns are aligned.\n\ncon_duckdb_r = create_engine(\"duckdb:///data_r.duckdb\")\ndata_r_duckdb = pd.read_sql_query(\"SELECT * FROM data\", con_duckdb_r)\n\npd.DataFrame({\n  \"data_r_duckdb\": data_r_duckdb.dtypes, \n  \"data_python_duckdb\": data_python_duckdb.dtypes\n})\n\n                   data_r_duckdb data_python_duckdb\ncharacter_column          object             object\ndate_column               object     datetime64[ns]\ndatetime_column   datetime64[ns]     datetime64[ns]\nnumeric_column           float64            float64\ninteger_column             int64              int64\nlogical_column              bool               bool\n\n\n\n\n\nDuckDB is a great alternative if you want to play around with new data storage technologies and if you work with multiple programming languages. The examples above show a high overlap between column types with the only major issue that the Date type from R is interpreted as a string in Python."
  },
  {
    "objectID": "posts/data-storage-comparison/index.html#parquet",
    "href": "posts/data-storage-comparison/index.html#parquet",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "Parquet",
    "text": "Parquet\nParquet is a highly efficient columnar storage format, increasingly popular in the field of data analytics, particularly when dealing with large datasets. This format is structured in a way that stores data by columns rather than by rows, which is typical in traditional database formats. This columnar approach allows for more efficient data retrieval and scanning, making it an excellent choice for analytical querying where operations are often performed on specific columns of data (similar to DuckDB).\nOne of the key strengths of Parquet is its ability to compress data effectively. It supports various compression and encoding schemes, which can significantly reduce the storage footprint of large data sets without sacrificing read performance. This efficiency in data storage and compression makes Parquet an attractive option for systems where storage cost and I/O performance are critical considerations, such as in big data applications and cloud environments.\n\nRPython\n\n\nThe arrow package provides all we need to write and read Parquet files. Similar to DuckDB, all column types are preserved exactly as to before we wrote the data frame to a parquet file.\n\nlibrary(arrow)\n\nwrite_parquet(data_r, \"data_r.parquet\")\ndata_r_parquet &lt;- read_parquet(\"data_r.parquet\")\n\nglimpse(data_r_parquet)\n\nRows: 4\nColumns: 6\n$ character_column &lt;chr&gt; \"A\", \"B\", \"C\", \"D\"\n$ date_column      &lt;date&gt; 2023-01-01, 2023-02-01, 2023-03-01, 2023-04-01\n$ datetime_column  &lt;dttm&gt; 2023-01-01 10:00:00, 2023-02-01 11:00:00, 2023-03-01 …\n$ numeric_column   &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5\n$ integer_column   &lt;int&gt; 1, 2, 3, 4\n$ logical_column   &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE\n\n\nIf I now read the data from the database generated in R, then we see that the only difference again arises in the date column (just as with DuckDB).\n\ndata_python_parquet &lt;- read_parquet(\"data_python.parquet\")\n\ntibble(\n  \"data_r_parquet\" = extract_column_classes(data_r_parquet),\n  \"data_python_parquet\" = extract_column_classes(data_python_parquet)\n)\n\n# A tibble: 6 × 2\n  data_r_parquet  data_python_parquet\n  &lt;chr&gt;           &lt;chr&gt;              \n1 character       character          \n2 Date            POSIXct, POSIXt    \n3 POSIXct, POSIXt POSIXct, POSIXt    \n4 numeric         numeric            \n5 integer         integer            \n6 logical         logical            \n\n\n\n\nI use the pyarrow.parquet to perform the same write and read operations as in R. As expected, all column types are preserved.\n\nimport pyarrow.parquet as pq\n\ndata_python.to_parquet(\"data_python.parquet\")\ndata_python_parquet = pd.read_parquet(\"data_python.parquet\")\n\ndata_python_parquet.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4 entries, 0 to 3\nData columns (total 6 columns):\n #   Column            Non-Null Count  Dtype         \n---  ------            --------------  -----         \n 0   character_column  4 non-null      object        \n 1   date_column       4 non-null      datetime64[ns]\n 2   datetime_column   4 non-null      datetime64[ns]\n 3   numeric_column    4 non-null      float64       \n 4   integer_column    4 non-null      int64         \n 5   logical_column    4 non-null      bool          \ndtypes: bool(1), datetime64[ns](2), float64(1), int64(1), object(1)\nmemory usage: 292.0+ bytes\n\n\nIf I now read the data from the database generated in R, then we see that the only difference again arises in the date column (just as with DuckDB).\n\ndata_r_parquet = pd.read_parquet(\"data_r.parquet\")\n\npd.DataFrame({\n  \"data_r_parquet\": data_r_parquet.dtypes, \n  \"data_python_parquet\": data_python_parquet.dtypes\n})\n\n                  data_r_parquet data_python_parquet\ncharacter_column          object              object\ndate_column               object      datetime64[ns]\ndatetime_column   datetime64[us]      datetime64[ns]\nnumeric_column           float64             float64\ninteger_column             int32               int64\nlogical_column              bool                bool"
  },
  {
    "objectID": "posts/data-storage-comparison/index.html#feather",
    "href": "posts/data-storage-comparison/index.html#feather",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "Feather",
    "text": "Feather\nFeather primarily designed for efficient data interchange between R and Python. Developed jointly by R and Python teams, Feather provides a fast, lightweight, and easy-to-use binary file format for storing data frames. However, Feather is primarily designed as an intermediate file format for data interchange rather than for long-term data storage. It does not offer the same level of compression as formats like Parquet, nor is it optimized for queries and data analysis tasks directly on the stored files.\n\nRPython\n\n\nThe arrow package also provides all we need to write and read Feather files. Similar to DuckDB and Parquet, all column types are preserved exactly as to before we wrote the data frame to a parquet file.\n\nwrite_feather(data_r, \"data_r.feather\")\ndata_r_feather &lt;- read_feather(\"data_r.feather\")\n\nglimpse(data_r_feather)\n\nRows: 4\nColumns: 6\n$ character_column &lt;chr&gt; \"A\", \"B\", \"C\", \"D\"\n$ date_column      &lt;date&gt; 2023-01-01, 2023-02-01, 2023-03-01, 2023-04-01\n$ datetime_column  &lt;dttm&gt; 2023-01-01 10:00:00, 2023-02-01 11:00:00, 2023-03-01 …\n$ numeric_column   &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5\n$ integer_column   &lt;int&gt; 1, 2, 3, 4\n$ logical_column   &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE\n\n\n\ndata_python_feather &lt;- read_feather(\"data_python.feather\")\n\ntibble(\n  \"data_r_feather\" = extract_column_classes(data_r_feather),\n  \"data_python_feather\" = extract_column_classes(data_python_feather)\n)\n\n# A tibble: 6 × 2\n  data_r_feather  data_python_feather\n  &lt;chr&gt;           &lt;chr&gt;              \n1 character       character          \n2 Date            POSIXct, POSIXt    \n3 POSIXct, POSIXt POSIXct, POSIXt    \n4 numeric         numeric            \n5 integer         integer            \n6 logical         logical            \n\n\n\n\nI use pyarrow.feather to perform the same write and read operations as in R. As expected, all column types are preserved.\n\nimport pyarrow.feather as feather\n\nfeather.write_feather(data_python, \"data_python.feather\")\ndata_python_feather = feather.read_feather(\"data_python.feather\")\n\ndata_python_feather.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4 entries, 0 to 3\nData columns (total 6 columns):\n #   Column            Non-Null Count  Dtype         \n---  ------            --------------  -----         \n 0   character_column  4 non-null      object        \n 1   date_column       4 non-null      datetime64[ns]\n 2   datetime_column   4 non-null      datetime64[ns]\n 3   numeric_column    4 non-null      float64       \n 4   integer_column    4 non-null      int64         \n 5   logical_column    4 non-null      bool          \ndtypes: bool(1), datetime64[ns](2), float64(1), int64(1), object(1)\nmemory usage: 292.0+ bytes\n\n\nIf I now read the data from the database generated in R, then we see that the only difference again arises in the date column (just as with DuckDB and Parquet).\n\ndata_r_feather = feather.read_feather(\"data_r.feather\")\n\npd.DataFrame({\n  \"data_r_feather\": data_r_feather.dtypes, \n  \"data_python_feather\": data_python_feather.dtypes\n})\n\n                  data_r_feather data_python_feather\ncharacter_column          object              object\ndate_column               object      datetime64[ns]\ndatetime_column   datetime64[us]      datetime64[ns]\nnumeric_column           float64             float64\ninteger_column             int32               int64\nlogical_column              bool                bool"
  },
  {
    "objectID": "posts/data-storage-comparison/index.html#conclusion",
    "href": "posts/data-storage-comparison/index.html#conclusion",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "Conclusion",
    "text": "Conclusion\nThere are two main takeaways: first, R’s handling of dates is peculiar and cannot be smoothly aligned with Python (at with pandas). I think I need to ask myself the question how often I actually need the Date type if I want to avoid issues with other languages. Second, Parquet is an exciting format for storing data and I’ll definitely use it more in future applications."
  },
  {
    "objectID": "posts/data-storage-comparison/index.html#footnotes",
    "href": "posts/data-storage-comparison/index.html#footnotes",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is also a dedicated StringDtype for string data, introduced in more recent versions of pandas (version 1.0.0 and later), but it seems that it is not used often.↩︎\nIn R, dates are typically represented as the number of days since January 1, 1970, known as the Unix epoch. This is a standard date reference used in many programming languages. The integer is positive for dates after January 1, 1970, and negative for dates before it. For example, a Date object representing January 2, 1970, would have a value of 1, as it is one day after the epoch start. Similarly, POSIXct is a count of the number of seconds since the Unix epoch (January 1, 1970, 00:00:00 GMT).↩︎"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-r/index.html",
    "href": "posts/interactive-data-visualization-with-r/index.html",
    "title": "Interactive Data Visualization with R",
    "section": "",
    "text": "Interactive figures are an essential tool for communicating data insights, in particular in reports or dashboards. In this blog post, I compare different packages for dynamic data visualization in R. Before we dive into the comparison, here is a quick introduction to each contestant.\nggiraph is a package designed to enhance the interactivity of ggplot2 visualizations. ggiraph allows users to create dynamic and interactive graphics that can include features such as tooltips, clickable elements, and JavaScript actions. This is particularly useful for web-based data visualizations and interactive reporting.\nplotly is a powerful framework for creating interactive, web-based data visualizations directly from R. It serves as an interface to the Plotly javascript library, enabling R users to create a wide range of highly interactive and dynamic plots that can be viewed in any web browser. One of the key features of plotly is its ability to add interactivity to plots with minimal effort. Interactive features include tooltips, zooming, panning, and selection capabilities, allowing users to explore and interact with the data in depth. Furthermore, plotly integrates seamlessly with the ggplot2 package, allowing users to convert ggplot2 figures into interactive plotly charts using the ggplotly() function.\nThe highcharter package is a wrapper for the Highcharts javascript library and its modules. Highcharts is very flexible and customizable javascript charting library and it has a powerful API. highcharter stands out for its emphasis on creating visually appealing, interactive charts.\nI compare code to generate ggiraph, plotly, ggplotly, and highcharter output in the post below. The types of plots that I chose for the comparison heavily draw on the examples given in R for Data Science - an amazing resource if you want to get started with data visualization. Spoiler alert: I’m not always able to replicate the same figure with all approaches (yet)."
  },
  {
    "objectID": "posts/interactive-data-visualization-with-r/index.html#loading-packages-and-data",
    "href": "posts/interactive-data-visualization-with-r/index.html#loading-packages-and-data",
    "title": "Interactive Data Visualization with R",
    "section": "Loading packages and data",
    "text": "Loading packages and data\nWe start by loading the main packages of interest (ggiraph, plotly, highcharter), dplyr and purr for data manipulation tools, and the popular palmerpenguins data. We then use the penguins data frame as the data to compare all functions and methods below. Note that I drop all rows with missing values because I don’t want to get into related messages in this post.\n\nlibrary(ggiraph)\nlibrary(plotly)\nlibrary(highcharter)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(palmerpenguins)\n\npenguins &lt;- na.omit(palmerpenguins::penguins)"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-r/index.html#a-full-blown-example",
    "href": "posts/interactive-data-visualization-with-r/index.html#a-full-blown-example",
    "title": "Interactive Data Visualization with R",
    "section": "A full-blown example",
    "text": "A full-blown example\nLet”s start with an advanced example that combines many different aesthetics at the same time: we plot two columns against each other, use color and shape aesthetics do differentiate species, include separate regression lines for each species, manually set nice labels, and use a theme. You can click through the results in the tabs below.\nUnfortunately, I wasn’t able to add species-specific regression lines to the plotly output - do you have any idea? Feel free to drop a comment below. You can also see that adding regression lines to highcharter plots requires a lot of manual tinkering compared to ggplot2. Moreoever, plotly does not support subtitles, while, for some reason, plotly::ggplotly() and highcharter don’t display the subtitles.\n\nggiraphplotlyggplotlyhighcharter\n\n\n\nfig_full &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, \n             color = species, shape = species)) + \n  geom_point_interactive(\n    aes(tooltip = paste(\"bill_length_mm:\", bill_length_mm, \"&lt;br&gt;\",\n                        \"bill_depth_mm:\", bill_depth_mm)),\n    size = 2\n  ) + \n  geom_smooth_interactive(method = \"lm\", formula = \"y ~ x\") +\n  labs(x = \"Bill length (mm)\", y = \"Bill width (mm)\", \n       title = \"Bill length vs. bill width\", \n       subtitle = \"Using the ggiraph package\",\n       color = \"Species\", shape = \"Species\") +\n  theme_minimal()\ngirafe(ggobj = fig_full)\n\n\n\n\n\n\n\n\npenguins |&gt; \n  plot_ly(x = ~bill_length_mm, y = ~flipper_length_mm, \n          color = ~species, symbol = ~species,\n          type = \"scatter\", mode = \"markers\",  marker = list(size = 10)) |&gt; \n  layout(\n    plot_bgcolor = 'white',\n    xaxis = list(title = \"Bill Length (mm)\", zeroline = FALSE, ticklen = 5),\n    yaxis = list(title = \"Flipper Length (mm)\", zeroline = FALSE, ticklen = 5),\n    title = \"Bill length vs. bill width\"\n  )\n\n\n\n\n\n\n\n\nfig_full &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, \n             color = species, shape = species)) + \n  geom_point(size = 2) + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n  labs(x = \"Bill length (mm)\", y = \"Bill width (mm)\", \n       title = \"Bill length vs. bill width\", \n       subtitle = \"Using ggplot2 and ggplotly() from plotly\",\n       color = \"Species\", shape = \"Species\") +\n  theme_minimal()\nggplotly(fig_full)\n\n\n\n\n\n\n\n\nfig_full &lt;- penguins |&gt; \n  hchart(type = \"scatter\", \n         hcaes(x = bill_length_mm, y = flipper_length_mm,\n               color = species), \n         marker = list(radius = 5)) |&gt; \n  hc_xAxis(title = list(text = \"Bill length (mm)\")) |&gt; \n  hc_yAxis(title = list(text = \"Bill width (mm)\")) |&gt; \n  hc_title(text = \"Bill length vs. bill width\") |&gt;\n  hc_subtitle(\"Using highcharter\") |&gt; \n  hc_add_theme(hc_theme_ggplot2())\n\nspecies_unique &lt;- levels(penguins$species)\ncolors &lt;- c(\"#440154\", \"#21908C\", \"#FDE725\")\nfor(j in seq_along(species_unique)) {\n  \n  penguins_subset &lt;- penguins |&gt; \n    filter(species == species_unique[j])\n  \n  regression &lt;- lm(flipper_length_mm ~ bill_length_mm, data = penguins_subset)\n  intercept &lt;- regression$coefficients[1]\n  slope &lt;- regression$coefficients[2]\n  values_range &lt;- range(penguins_subset$bill_length_mm, na.rm = TRUE)\n  \n  line_data &lt;- tibble(\n    bill_length_mm = values_range,\n    flipper_length_mm = intercept + slope * values_range\n  )\n  \n  fig_full &lt;- fig_full |&gt;  \n    hc_add_series(data = list_parse2(line_data), \n                  type = \"line\", marker = \"none\",\n                  color = colors[j])\n}\n\nfig_full"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-r/index.html#visualizing-distributions",
    "href": "posts/interactive-data-visualization-with-r/index.html#visualizing-distributions",
    "title": "Interactive Data Visualization with R",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\n\nA categorical variable\nLet’s break down the differences in smaller steps by focusing on simpler examples. If you have a categorical variable and want to compare its relevance in your data, then ggiraph::geom_bar_interactive(), plotly::plot_ly(type = \"bar\") and highcharter::hchart(type = \"column\") are your friends. However, to show the counts, you have to manually prepare the data for plot_ly() and hchart() (as far as I know).\nNotice how you have to manually specify the tooltip to show the counts on hover in geom_bar_interactive() and that data_id determines which bar is highlighted on hover.\n\nggiraphplotlyggplotlyhighcharter\n\n\n\nfig_categorical &lt;- penguins |&gt; \n  ggplot(aes(x = island)) +\n  geom_bar_interactive(aes(tooltip = paste(\"count:\", after_stat(count)),\n                           data_id = island))\ngirafe(ggobj = fig_categorical)\n\n\n\n\n\n\n\n\npenguins |&gt; \n  count(island) |&gt; \n  plot_ly(data = _, x = ~island, y = ~n, type = \"bar\") |&gt; \n  layout(barmode = 'stack')\n\n\n\n\n\n\n\n\nfig_categorical &lt;- penguins |&gt; \n  ggplot(aes(x = island)) +\n  geom_bar()\nggplotly(fig_categorical)\n\n\n\n\n\n\n\n\npenguins |&gt; \n  count(island) |&gt; \n  hchart(type = \"column\", \n         hcaes(x = island, y = n))\n\n\n\n\n\n\n\n\n\n\nA numerical variable\nIf you have a numerical variable, usually histograms are a good starting point to get a better feeling for the distribution of your data. ggiraph::geom_histogram_interactive(), plotly::plot_ly(type = \"histogram\"), highcharter::hchart() with options to control bin widths or number of bins are the functions for this task.\nNote that the binning algorithms are different across the approaches: while ggpplot2 creates bins around a midpoint (e.g. 34), plotly and highcharter create bins across a range (e.g. between 34-35.9). This leads to seemingly different histograms, but none of them is wrong.\nMoreover, note that the data property is not available for histograms in highcharter, unlike most other Highcharts series,1, so we need to pass penguins$bill_length_mm. This is tidy anti-pattern and cost me quite some time to figure out.\n\nggiraphplotlyggplotlyhighcharter\n\n\n\nfig_numerical &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram_interactive(\n    aes(tooltip = paste(\"bill_length_mm:\", after_stat(count))),\n    binwidth = 2\n  )\ngirafe(ggobj = fig_numerical)\n\n\n\n\n\n\n\n\nplot_ly(penguins, x = ~bill_length_mm, type = \"histogram\",\n        xbins = list(size = 2))\n\n\n\n\n\n\n\n\nfig_numerical &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram(binwidth = 2)\nggplotly(fig_numerical)\n\n\n\n\n\n\n\n\nhchart(penguins$bill_length_mm) |&gt; \n  hc_plotOptions(series = list(binWidth = 2))"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-r/index.html#visualizing-relationships",
    "href": "posts/interactive-data-visualization-with-r/index.html#visualizing-relationships",
    "title": "Interactive Data Visualization with R",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\n\nA numerical and a categorical variable\nTo visualize relationships, you need to have at least two columns. If you have a numerical and a categorical variable, then histograms or densities with groups are a good starting point. The next example illustrates the use of densities via ggiraph::geom_density_interactive() and plotly::plot_ly(histnorm = \"probability density). For highcharter, you need to comute the density estimates yourself and then add them as lines to a plot.\nNote that plotly offers no out-of-the-box support for density curves as ggplot2, so we’d have to manually create densities and draw the curves. Also, note that it is currently not possible to use the after_stat(density) aesthetic in the tooltip.\n\nggiraphplotlyggplotlyhighcharter\n\n\n\nfig_density &lt;- penguins |&gt; \n  ggplot(aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density_interactive(\n    aes(tooltip = paste(\"Species:\", species)),\n    linewidth = 0.75, alpha = 0.5\n  )\ngirafe(ggobj = fig_density)\n\n\n\n\n\n\n\n\nplot_ly(penguins, x = ~body_mass_g,\n        type = \"histogram\", histnorm = \"probability density\",\n        color = ~species, opacity = 0.5) |&gt; \n  layout(barmode = \"overlay\")\n\n\n\n\n\n\n\n\nfig_density &lt;- penguins |&gt; \n  ggplot(aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(linewidth = 0.75, alpha = 0.5)\nggplotly(fig_density)\n\n\n\n\n\n\n\n\nseries &lt;- map(levels(penguins$species), function(x){\n  penguins_subset &lt;- penguins |&gt; \n    filter(species == x)\n  \n  data &lt;- density(penguins_subset$body_mass_g)[1:2] |&gt; \n    as.data.frame() |&gt; \n    list_parse2()\n  \n  list(data = data, name = x)\n})\n\nhighchart() |&gt; \n  hc_add_series_list(series)\n\n\n\n\n\n\n\n\n\n\nTwo categorical columns\nStacked bar plots are a good way to display the relationship between two categorical columns. geom_bar_interactive() with the position argument, plotly::plot_ly(type = \"bar\") and highcharter::hchart(type = \"column\") are your aesthetics of choice for this task. Note that you can easily switch to counts by using position = \"identity\" in ggplotl2 instead of relative frequencies as in the example below, while you have to manually prepare the data to funnel counts or percentages to plotly and highcharter, while ggplot2 handles these things automatically.\n\nggiraphplotlyggplotlyhighcharter\n\n\n\n#\nfig_two_categorical &lt;- penguins |&gt; \n  ggplot(aes(x = species, fill = island)) +\n  geom_bar_interactive(\n    aes(tooltip = paste(fill, \":\", after_stat(count)),\n        data_id = island),\n    position = \"fill\"\n  )\ngirafe(ggobj = fig_two_categorical)\n\n\n\n\n\n\n\n\npenguins |&gt; \n  count(species, island) |&gt; \n  group_by(species) |&gt; \n  mutate(percentage = n / sum(n)) |&gt; \n  plot_ly(x = ~species, y = ~percentage, type = \"bar\", color = ~island) |&gt; \n  layout(barmode = \"stack\")\n\n\n\n\n\n\n\n\nfig_two_categorical &lt;- penguins |&gt; \n  ggplot(aes(x = species, fill = island)) +\n  geom_bar(position = \"fill\")\nggplotly(fig_two_categorical)\n\n\n\n\n\n\n\n\npenguins |&gt; \n  count(species, island) |&gt; \n  group_by(species) |&gt; \n  mutate(percentage = n / sum(n)) |&gt;\n  hchart(type = \"column\", \n         hcaes(x = species, y = percentage, group = island)) |&gt; \n  hc_plotOptions(series = list(stacking = \"percent\"))\n\n\n\n\n\n\n\n\n\n\nTwo numerical columns\nScatter plots and regression lines are definitely the most common approach for visualizing the relationship between two numerical columns and we focus on scatter plots for this example (see the first visualization example if you want to see again how to add a regression line). Here, the size parameter controls the size of the shapes that you use for the data points in ggiraph::geom_point_interactive() relative to the base size (i.e., it is not tied to any unit of measurement like pixels). For plotly.plot_ly(type = \"scatter\") you also have the size to control point sizes manually through the marker options, where size is measured in pixels. For highcharter, you can specify point sizes via radius in the marker options, where it is also measured in pixels (so to get points with diameter 10 pixels, you need a radius of 5).\n\nggiraphplotlyggplotlyhighcharter\n\n\n\nfig_two_columns &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point_interactive(\n    aes(tooltip = paste(\"bill_length_mm:\", bill_length_mm, \"&lt;br&gt;\",\n                        \"flipper_length_mm:\", flipper_length_mm)), \n    size = 2\n  )\ngirafe(ggobj = fig_two_columns)\n\n\n\n\n\n\n\n\nplot_ly(data = penguins, x = ~bill_length_mm, y = ~flipper_length_mm, \n        type = \"scatter\", mode = \"markers\",  marker = list(size = 10))\n\n\n\n\n\n\n\n\nfig_two_columns &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(size = 2)\nggplotly(fig_two_columns)\n\n\n\n\n\n\n\n\npenguins |&gt; \n  hchart(type = \"scatter\", \n         hcaes(x = bill_length_mm, y = flipper_length_mm), \n         marker = list(radius = 5))\n\n\n\n\n\n\n\n\n\n\nThree or more columns\nYou can include more information by mapping columns to additional aesthetics. For instance, we can map colors and shapes to species and create separate plots for each island by using facets. Facets are actually a great way to extend your figures, so I highly recommend playing around with them using your own data.\nIn ggplot2 you add the facet layer at the end, whereas in plotly you have to start with the facet grid at the beginning and map scatter plots across facets. However, I was not able to achieve two things in plotly: how can we have subtitles for each subplot similar to ggplot2? How can I have a shared legend across facets? Both seem to work nicely in Python, at least (see my post here). If you have an idea, please create a comment below!\nFor highcharter, I have no idea how to make shared legends or keep the axis in sync across subplots, while both are easily customizable in ggplot2.\n\nggiraphplotlyggplotlyhighcharter\n\n\n\nfig_many_columns &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point_interactive(\n    aes(color = species, shape = species,\n        tooltip = paste(\"bill_length_mm:\", bill_length_mm, \"&lt;br&gt;\",\n                        \"flipper_length_mm:\", flipper_length_mm, \"&lt;br&gt;\",\n                        \"species:\", species))\n  ) +\n  facet_wrap(~island)\ngirafe(ggobj = fig_many_columns)\n\n\n\n\n\n\n\n\npenguins |&gt;\n  group_by(island) |&gt;\n  group_map(~{\n    plot_ly(data = ., x = ~bill_length_mm, y = ~flipper_length_mm, \n                     color = ~species, type = \"scatter\", mode = \"markers\")\n    }, .keep = TRUE) |&gt;\n  subplot(nrows = 1, shareX = TRUE, shareY = TRUE)\n\n\n\n\n\n\n\n\nfig_many_columns &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\nggplotly(fig_many_columns)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUsing hw_grid() does not work qith Quarto (which this blog is built on) because of CSS conflicts. See the discussion of this issue here. In your IDE, there should be three plots next to each other.\n\n\n\nplots &lt;- map(levels(penguins$island), function(x) {\n  \n  penguins_subset &lt;- penguins  |&gt; \n    filter(island == x)\n\n  hchart(penguins_subset,\n         type = \"scatter\", \n         hcaes(x = bill_length_mm, y = flipper_length_mm, \n               color = species, symbol = species)) |&gt; \n    hc_title(text = x)  |&gt; \n    hc_legend(enabled = FALSE)\n})\n\nhw_grid(plots, ncol = 3, rowheight = 600)"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-r/index.html#time-series",
    "href": "posts/interactive-data-visualization-with-r/index.html#time-series",
    "title": "Interactive Data Visualization with R",
    "section": "Time series",
    "text": "Time series\nAs a last example, we quickly dive into time series plots where you typically want to show multiple lines over some time period. Here, I aggregate the number of penguins by year and island and plot the corresponding lines. All packages behave as expected and show similar output.\nplotly and highcharter do not directly support setting line styles based on a variable like island within their syntax. Instead, you have to iterate over each group and manually set the line style for each island, which I omit here.\n\nggiraphplotlyggplotlyhighcharter\n\n\n\nfig_time &lt;- penguins |&gt; \n  count(year, island) |&gt; \n  ggplot(aes(x = year, y = n, group = island, color = island)) +\n  geom_line_interactive(aes(linetype = island,\n                            tooltip = paste(\"island:\", island, \"&lt;br&gt;\", \n                                            \"count:\", n)))\ngirafe(ggobj = fig_time)\n\n\n\n\n\n\n\n\npenguins |&gt; \n  count(year, island) |&gt; \n  plot_ly(x = ~year, y = ~n, color = ~island, \n          type = \"scatter\", mode = \"lines\")\n\n\n\n\n\n\n\n\nfig_time &lt;- penguins |&gt; \n  count(year, island) |&gt; \n  ggplot(aes(x = year, y = n, color = island)) +\n  geom_line(aes(linetype = island))\nggplotly(fig_time)\n\n\n\n\n\n\n\n\npenguins |&gt; \n  count(year, island)  |&gt; \n  hchart(type = \"line\", \n         hcaes(x = year, y = n, group = island))"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-r/index.html#conclusion",
    "href": "posts/interactive-data-visualization-with-r/index.html#conclusion",
    "title": "Interactive Data Visualization with R",
    "section": "Conclusion",
    "text": "Conclusion\nggiraph is really promising and allows you to create beautiful interactive figures using the grammar of graphics. If you wan to learn more, I recommend Albert Rapp’s blog post, where he’ll also show you how to use a shiny backend to enhance interactivity.\nI really like the visual appearance and user experience of highcharter, but I still prefer the efficiency of ggplot2 in combination with plotly. The ggplotly() function provides a powerful tool for quick, interactive data visualization of complex relationships that you prototype with ggplot2.\nplotly and highcharter require relatively more tinkering as the complexity of plots increases (e.g. if you want to add regression lines). But maybe I just missed some shortcuts. Do you know any that I should include?"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-r/index.html#footnotes",
    "href": "posts/interactive-data-visualization-with-r/index.html#footnotes",
    "title": "Interactive Data Visualization with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the official documentation here.↩︎"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-python/index.html",
    "href": "posts/interactive-data-visualization-with-python/index.html",
    "title": "Interactive Data Visualization with Python",
    "section": "",
    "text": "Interactive figures are an essential tool for communicating data insights, in particular in reports or dashboards. In this blog post, I compare different libraries for dynamic data visualization in Python. Before we dive into the comparison, here is a quick introduction to each contestant.\nplotly is an interactive, open-source plotting library that enable the creation of publication-quality figures. It supports a wide range of chart types including line charts, scatter plots, bar charts, pie charts, bubble charts, heatmaps, and more advanced visualizations like 3D plots and geographical maps. One of the key features of plotly is its ability to produce interactive plots that users can zoom, pan, and hover over, providing tooltips and additional information, which makes it highly effective for data exploration and presentation.\nbokeh is a powerful, flexible library for creating interactive plots and dashboards in the web browser. It is designed to help users create elegant, concise constructions of versatile graphics with high-performance interactivity over very large or streaming datasets. One of the core features of bokeh is its ability to generate dynamic javascript plots directly from Python code, which means you can harness the interactivity of web technologies without needing to write any javascript yourself. The plots can be embedded in HTML pages or served as standalone applications, making it a versatile choice for web development and data analysis tasks.\naltair is a declarative statistical visualization library, designed to create interactive visualizations with a minimal amount of code. It is built on top of the powerful Vega and Vega-Lite visualization grammars, enabling the construction of a wide range of statistical plots with a simple and intuitive syntax. One of the key advantages of altair is its emphasis on data-driven visualization design. By allowing users to think about their data first and foremost, Altair facilitates the exploration and altairunderstanding of complex datasets.\nI compare code to generate plotly, bokeh, and altair output in the post below. The types of plots that I chose for the comparison heavily draw on the examples given in R for Data Science - an amazing resource if you want to get started with data visualization. Spoiler alert: I’m not always able to replicate the same figure with all approaches (yet)."
  },
  {
    "objectID": "posts/interactive-data-visualization-with-python/index.html#loading-libraries-and-data",
    "href": "posts/interactive-data-visualization-with-python/index.html#loading-libraries-and-data",
    "title": "Interactive Data Visualization with Python",
    "section": "Loading libraries and data",
    "text": "Loading libraries and data\nWe start by loading a dew data manipulation libraries, the main libraries and modules of interest, and palmerpenguins data. We then use the penguins data frame as the data to compare all functions and methods below. Note that I drop all rows with missing values because I don’t want to get into related messages in this post.\n\n# Load data manipulation libraries\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.stats import gaussian_kde\n\n# Load plotly visualization modules\nimport plotly.express as px\nimport plotly.graph_objs as go\n\n# Load bokeh visualization modules\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\nfrom bokeh.models import ColumnDataSource, CategoricalColorMapper, HoverTool, FactorRange\nfrom bokeh.layouts import gridplot\nfrom bokeh.transform import factor_cmap\nfrom bokeh.palettes import Spectral6, Category10\noutput_notebook()\n\n# Load altair visualization library\nimport altair as alt\n\n# Load penguins data\nfrom palmerpenguins import load_penguins\npenguins = load_penguins().dropna()\n\n    \n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "posts/interactive-data-visualization-with-python/index.html#a-full-blown-example",
    "href": "posts/interactive-data-visualization-with-python/index.html#a-full-blown-example",
    "title": "Interactive Data Visualization with Python",
    "section": "A full-blown example",
    "text": "A full-blown example\nLet’s start with an advanced example that combines many different aesthetics at the same time: we plot two columns against each other, use color and shape aesthetics do differentiate species, include separate regression lines for each species, manually set nice labels, and use a theme. You can click through the results in the tabs below.\nNote that we have to manually add regression lines for plotly and bokeh, while altair has built-in support for them.\n\nplotlybokehaltair\n\n\n\nfig_full = (px.scatter(\n    penguins, x = \"bill_length_mm\", y = \"flipper_length_mm\", \n    color = \"species\", symbol = \"species\",\n    title = \"Bill length vs. flipper length\",\n    labels = {\"bill_length_mm\": \"Bill length (mm)\", \n              \"flipper_length_mm\": \"Flipper length (mm)\",\n              \"species\": \"Species\"})\n  .update_traces(marker = dict(size = 10))\n  .update_layout(\n    plot_bgcolor = \"white\",\n    xaxis = dict(zeroline = False, ticklen = 5),\n    yaxis = dict(zeroline = False, ticklen = 5))\n)\n\nfor species in penguins[\"species\"].unique():\n  penguins_subset = penguins[penguins[\"species\"] == species]\n  X = penguins_subset[\"bill_length_mm\"]\n  X = sm.add_constant(X)\n  y = penguins_subset[\"flipper_length_mm\"]\n\n  model = sm.OLS(y, X).fit()\n  line = model.params[0] + model.params[1] * penguins_subset[\"bill_length_mm\"]\n\n  fig_full.add_trace(\n    go.Scatter(x = penguins_subset[\"bill_length_mm\"], y = line, \n               mode = \"lines\", showlegend = False)\n  )\n\nfig_full\n\n                                                \n\n\n\n\n\nfig_full = figure(\n  title = \"Bill length vs. flipper length\",\n  x_axis_label = \"Bill length (mm)\", y_axis_label = \"Flipper length (mm)\",\n  tools = \"pan,wheel_zoom,box_zoom,reset,hover\",\n  tooltips = [\n    (\"Bill length (mm)\", \"@bill_length_mm\"),\n    (\"Flipper length (mm)\", \"@flipper_length_mm\"),\n    (\"Species\", \"@species\")\n  ]\n)\n\nspecies = penguins[\"species\"].unique()\ncolor_map = {\n  species: color for species, color in zip(species, [\"red\", \"green\", \"blue\"])\n}  \n\nfor species in species:\n    penguins_subset = penguins[penguins[\"species\"] == species]\n\n    fig_full.scatter(\n      source = ColumnDataSource(penguins_subset),\n      x = \"bill_length_mm\", y = \"flipper_length_mm\", \n      legend_label = species, color = color_map[species], \n      size = 10, fill_alpha = 0.6\n    )\n              \n    X = penguins_subset[\"bill_length_mm\"]\n    X = sm.add_constant(X)\n    y = penguins_subset[\"flipper_length_mm\"]\n\n    model = sm.OLS(y, X).fit()\n    predictions = model.predict(X)\n\n    fig_full.line(\n      penguins_subset[\"bill_length_mm\"], predictions, \n      color =color_map[species], line_width = 2, legend_label = species\n    )\n\nfig_full.legend.title = \"Species\"\nfig_full.legend.location = \"top_left\"\nfig_full.background_fill_color = \"white\"\nfig_full.border_fill_color = \"white\"\nfig_full.outline_line_color = None\n \nshow(fig_full)\n\n\n  \n\n\n\n\n\n\n\n\npoints = (alt.Chart(penguins)\n  .mark_point(size=100, filled=True)\n  .encode(\n    x = alt.X(\"bill_length_mm\", \n              scale = alt.Scale(zero = False), \n              title = \"Bill length (mm)\", \n              axis = alt.Axis(tickCount = 5, grid = False)),\n    y = alt.Y(\"flipper_length_mm\", \n              scale = alt.Scale(zero = False), \n              title = \"Flipper length (mm)\", \n              axis = alt.Axis(tickCount = 5, grid = False)),\n    color = \"species:N\", shape = \"species:N\",\n    tooltip = [alt.Tooltip(\"bill_length_mm\", title = \"Bill length (mm)\"),\n               alt.Tooltip(\"flipper_length_mm\", title = \"Flipper length (mm)\"),\n               alt.Tooltip(\"species\", title = \"Species\")])\n)\n\nregression_lines = (alt.Chart(penguins)\n  .transform_regression(\n    \"bill_length_mm\", \"flipper_length_mm\", groupby = [\"species\"]\n  )\n  .mark_line()\n  .encode(\n    x = \"bill_length_mm:Q\", y = \"flipper_length_mm:Q\", \n    color = \"species:N\"\n  )\n)\n\nfig_full = ((points + regression_lines)\n  .properties(title = \"Bill length vs. flipper length\")\n  .configure_view(stroke = \"transparent\", fill = \"white\")\n  .configure_axis(labelFontSize = 10, titleFontSize = 12)\n)\n\nfig_full"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-python/index.html#visualizing-distributions",
    "href": "posts/interactive-data-visualization-with-python/index.html#visualizing-distributions",
    "title": "Interactive Data Visualization with Python",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\n\nA categorical variable\nLet’s break down the differences in smaller steps by focusing on simpler examples. If you have a categorical variable and want to compare its relevance in your data, then bar charts are your friends. The code chunks below show you how to implement them for each approach.\n\nplotlybokehaltair\n\n\n\nisland_counts = (penguins\n  .groupby(\"island\")\n  .size()\n  .reset_index(name = \"n\")\n)\n\n(px.bar(island_counts, x = \"island\", y = \"n\")\n  .update_layout(barmode = \"stack\")\n) \n\n                                                \n\n\n\n\n\nisland_counts = (penguins\n  .groupby(\"island\")\n  .size()\n  .reset_index(name = \"n\")\n)\n\nislands = island_counts[\"island\"].unique()\n\nfig_bar = figure(x_range = islands,\n                 tools = \"pan,wheel_zoom,box_zoom,reset,hover\")\n\nfig_bar.vbar(source = ColumnDataSource(island_counts),\n             x = \"island\", top = \"n\", width = 0.9, line_color = \"white\")\nshow(fig_bar)\n\n\n  \n\n\n\n\n\n\n\n\nisland_counts = (penguins\n  .groupby(\"island\")\n  .size()\n  .reset_index(name = \"n\")\n)\n\n(alt.Chart(island_counts)\n  .mark_bar()\n  .encode(x = \"island\", y = \"n\",\n          tooltip = [\"island\", \"n\"])\n)\n\n\n\n\n\n\n\n\n\n\n\n\nA numerical variable\nIf you have a numerical variable, usually histograms are a good starting point to get a better feeling for the distribution of your data. You can quickly create histograms in plotly and altair, while you have to manually construct the the histogram from individual bars in bokeh.\n\nplotlybokehaltair\n\n\n\n(px.histogram(penguins, x = \"bill_length_mm\")\n  .update_traces(xbins = dict(size =  2))\n)\n\n                                                \n\n\n\n\n\nbin_size = 2\nbins = np.arange(\n  start = penguins[\"bill_length_mm\"].min(), \n  stop = penguins[\"bill_length_mm\"].max() + bin_size, \n  step = bin_size\n)\nhist, edges = np.histogram(penguins[\"bill_length_mm\"], bins = bins)\n\nsource = ColumnDataSource(\n  data = dict(top = hist, left = edges[:-1], right = edges[1:])\n)\n\nfig_histogram = figure(tools = \"pan,wheel_zoom,box_zoom,reset,hover\")\n\nfig_histogram.quad(\n  source = source, \n  bottom = 0, top = \"top\", left = \"left\", right = \"right\",\n  fill_color = \"skyblue\", line_color = \"white\"\n)\n\nshow(fig_histogram)\n\n\n  \n\n\n\n\n\n\n\n\n(alt.Chart(penguins)\n  .mark_bar()\n  .encode(\n    x = alt.X(\"bill_length_mm:Q\", bin = alt.Bin(step = 2)),\n    y = alt.Y(\"count()\"),\n    tooltip = [alt.Tooltip(\"bill_length_mm:Q\", bin = alt.Bin(step = 2)),\n               alt.Tooltip(\"count()\")]\n  )\n)"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-python/index.html#visualizing-relationships",
    "href": "posts/interactive-data-visualization-with-python/index.html#visualizing-relationships",
    "title": "Interactive Data Visualization with Python",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\n\nA numerical and a categorical variable\nTo visualize relationships, you need to have at least two columns. If you have a numerical and a categorical variable, then histograms or densities with groups are a good starting point. The next example illustrates the use of densities. plotly and altair have built-in support for densities, while you have to manually compute the densities in bokeh.\n\nplotlybokehaltair\n\n\n\n(px.histogram(penguins, x = \"body_mass_g\", color = \"species\",\n              histnorm = \"density\", barmode = \"overlay\", opacity = 0.5)\n  .update_traces(marker_line_width = 0.75)\n)\n\n/Users/krise/Documents/GitHub/tidy-intelligence/blog/renv/python/virtualenvs/renv-python-3.10/lib/python3.10/site-packages/plotly/express/_core.py:2065: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n\n\n                                                \n\n\n\n\n\nspecies = penguins[\"species\"].unique()\n\ncolors = Category10[len(species)]\n\nfig_densities = figure(tools = \"pan,wheel_zoom,box_zoom,reset,hover\")\n\nfor j, species in enumerate(species):\n    penguins_subset = penguins[penguins[\"species\"] == species]\n    body_mass = penguins_subset[\"body_mass_g\"].dropna()\n    \n    kde = gaussian_kde(body_mass)\n    x_range = np.linspace(body_mass.min(), body_mass.max(), 100)\n    density = kde(x_range)\n\n    source = ColumnDataSource(\n      data = dict(body_mass_g = x_range, density = density, species = [species]*len(x_range))\n    )\n    \n    fig_densities.patch(\n      source = source, \n      x = \"body_mass_g\", y = \"density\", \n      alpha = 0.5, color = colors[j], legend_label = species\n    )\n\nshow(fig_densities)\n\n\n  \n\n\n\n\n\n\n\n\n(alt.Chart(penguins)\n  .transform_density(\"body_mass_g\", \n                     as_ = [\"body_mass_g\", \"density\"], groupby = [\"species\"])\n  .mark_area(opacity = 0.5)\n  .encode(\n    x = alt.X(\"body_mass_g:Q\"), y = alt.Y(\"density:Q\"),\n    color = \"species:N\", tooltip = [\"species:N\", \"body_mass_g:Q\"]\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\n\nTwo categorical columns\nStacked bar plots are a good way to display the relationship between two categorical columns. For plotly and altair, we simply compute the percentages by species and island and put them into the bar plotting functions. Note that bokeh is peculiar because it requires the data in wide format for stacked bar charts.\n\nplotlybokehaltair\n\n\n\nspecies_island_counts = (penguins\n  .groupby([\"species\", \"island\"])\n  .size()\n  .reset_index(name = \"n\")\n  .assign(\n    percentage = lambda x: x[\"n\"] / x.groupby(\"species\")[\"n\"].transform(\"sum\")\n  )\n)\n\npx.bar(species_island_counts, x = \"species\", y = \"percentage\", \n       color = \"island\", barmode = \"stack\")\n\n/Users/krise/Documents/GitHub/tidy-intelligence/blog/renv/python/virtualenvs/renv-python-3.10/lib/python3.10/site-packages/plotly/express/_core.py:2065: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n\n\n                                                \n\n\n\n\n\nspecies_island_counts = (penguins\n  .groupby([\"species\", \"island\"])\n  .size()\n  .reset_index(name = \"n\")\n  .assign(\n    percentage = lambda x: x[\"n\"] / x.groupby(\"species\")[\"n\"].transform(\"sum\")\n  )\n)\n\nspecies_island_counts_wide = (species_island_counts\n  .pivot(index = \"species\", columns = \"island\", values = \"percentage\")\n  .fillna(0)\n)\n\nfig_stacked = figure(x_range = penguins[\"species\"].unique())\n\nfig_stacked .vbar_stack(\n  source = ColumnDataSource(data = species_island_counts_wide), \n  stackers = penguins[\"island\"].unique(), x = \"species\", \n  width = 0.9, color = [\"red\", \"blue\", \"green\"]\n)\n\nshow(fig_stacked)\n\n\n  \n\n\n\n\n\n\n\n\nspecies_island_counts = (penguins\n  .groupby([\"species\", \"island\"])\n  .size()\n  .reset_index(name = \"n\")\n  .assign(\n    percentage = lambda x: x[\"n\"] / x.groupby(\"species\")[\"n\"].transform(\"sum\")\n  )\n)\n\n(alt.Chart(species_island_counts)\n  .mark_bar()\n  .encode(\n    x = \"species\", y = \"percentage\", color = \"island\",\n    order = alt.Order(\"island\", sort = \"ascending\"),\n    tooltip = [\"species\", \"island\", \"percentage\"]\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\n\nTwo numerical columns\nScatter plots and regression lines are definitely the most common approach for visualizing the relationship between two numerical columns and we focus on scatter plots for this example (see the first visualization example if you want to see again how to add a regression line). Note that altair axis ranges by default includes 0,so you need to manually tell the scale to ignore it.\n\nplotlybokehaltair\n\n\n\n(px.scatter(penguins, x = \"bill_length_mm\", y = \"flipper_length_mm\")\n  .update_traces(marker = dict(size = 10))\n)\n\n                                                \n\n\n\n\n\nfig_scatter = figure(tools = \"pan,wheel_zoom,box_zoom,reset,hover\")\n\nfig_scatter.circle(\n  source = ColumnDataSource(penguins),\n  x = \"bill_length_mm\", y = \"flipper_length_mm\", \n  size = 10\n)\n\nshow(fig_scatter)\n\n\n  \n\n\n\n\n\n\n\n\n(alt.Chart(penguins)\n  .mark_circle(size = 100)\n  .encode(\n    x = alt.X(\"bill_length_mm\", scale = alt.Scale(zero = False)),\n    y = alt.Y(\"flipper_length_mm\", scale = alt.Scale(zero = False)),\n    tooltip = [\"bill_length_mm\", \"flipper_length_mm\"]\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\n\nThree or more columns\nYou can include more information by mapping columns to additional aesthetics. For instance, we can map colors and shapes to species and create separate plots for each island by using facets. Facets are actually a great way to extend your figures, so I highly recommend playing around with them using your own data.\nFacets in bokeh involve a more manual process because it doesn’t have a direct equivalent of plotly’s facet_col parameter or altair’s facet() method. Instead, you’ll create individual plots for each facet and arrange them in a grid, which also means that you cannot have an automatically shared legend.\n\nplotlybokehaltair\n\n\n\npx.scatter(\n  penguins, \n  x = \"bill_length_mm\", y = \"flipper_length_mm\", \n  color = \"species\", facet_col = \"island\"\n)\n\n                                                \n\n\n\n\n\nislands = penguins[\"island\"].unique()\nspecies = penguins[\"species\"].unique()\n\ncolor_mapper = CategoricalColorMapper(\n  factors = species, palette = [\"red\", \"green\", \"blue\"]\n)\n\nplots = []\nfor island in islands:\n  penguins_subset = penguins[penguins[\"island\"] == island]\n    \n  p = figure(tools=\"pan,wheel_zoom,box_zoom,reset\", \n             width = 250, height = 250)\n    \n  p.circle(x = \"bill_length_mm\", y = \"flipper_length_mm\", \n           source = ColumnDataSource(penguins_subset),\n           color = {\"field\": \"species\", \"transform\": color_mapper},\n           legend_field = \"species\", size = 8)\n    \n  plots.append(p)\n\nfig_grid = gridplot(plots, ncols = 3)\n\nshow(fig_grid)\n\n\n  \n\n\n\n\n\n\n\n\n(alt.Chart(penguins)\n  .mark_circle()\n  .encode(\n    x = alt.X(\"bill_length_mm\", scale = alt.Scale(zero = False)),\n    y = alt.Y(\"flipper_length_mm\", scale = alt.Scale(zero = False)),\n    tooltip = [\"bill_length_mm\", \"flipper_length_mm\"],\n    color = \"species:N\"\n  )\n  .facet(column = \"island:N\")\n)"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-python/index.html#time-series",
    "href": "posts/interactive-data-visualization-with-python/index.html#time-series",
    "title": "Interactive Data Visualization with Python",
    "section": "Time series",
    "text": "Time series\nAs a last example, we quickly dive into time series plots where you typically show multiple lines over some date vector. Here, I aggregate the number of penguins by year and island and plot the corresponding lines. While you can simply define colors and line types in plotly and altair plotting functions, you have to manually loop in bokeh.\n\nplotlybokehaltair\n\n\n\nyear_island_count = (penguins\n  .groupby([\"year\", \"island\"])\n  .size()\n  .reset_index(name = \"n\")\n)\n\npx.line(year_island_count, \n        x = \"year\", y = \"n\", \n        color = \"island\", line_shape = \"linear\", line_dash = \"island\")\n\n                                                \n\n\n\n\n\nislands = year_island_count[\"island\"].unique()\ncolors = [\"blue\", \"green\", \"red\"]\ndashes = [\"solid\", \"dashed\", \"dotdash\"] \n\nfig_time_series = figure(tools = \"pan,wheel_zoom,box_zoom,reset,hover\")\n\nfor j, island in enumerate(islands):\n  year_island_count_subset = year_island_count[\n    year_island_count[\"island\"] == island\n  ]\n  \n  fig_time_series.line(\n    source = ColumnDataSource(year_island_count_subset),\n    x = \"year\", y = \"n\", \n    legend_label = island, \n    color = colors[j % len(colors)], \n    line_dash = dashes[j % len(dashes)], line_width = 2\n  ) \n\nshow(fig_time_series)\n\n\n  \n\n\n\n\n\n\n\n\n(alt.Chart(year_island_count)\n  .mark_line()\n  .encode(\n    x = \"year:T\", y = \"n:Q\",\n    color = \"island:N\", strokeDash = \"island:N\",\n    tooltip = [\"year\", \"n\", \"island\"]\n  )\n)"
  },
  {
    "objectID": "posts/interactive-data-visualization-with-python/index.html#conclusion",
    "href": "posts/interactive-data-visualization-with-python/index.html#conclusion",
    "title": "Interactive Data Visualization with Python",
    "section": "Conclusion",
    "text": "Conclusion\nplotly, bokeh, and altair each cater to distinct visualization needs in Python. plotly shines with its interactive, high-quality visuals and ease of embedding in web applications, making it ideal for creating complex interactive charts and dashboards. bokeh is focused on real-time data visualizations and interactivity, particularly suited for web apps that require dynamic data streaming. Its strength lies in the seamless integration of Python code with web technologies. altair offers a declarative approach, emphasizing simplicity and efficiency in creating elegant statistical visualizations with minimal code, making it ideal for exploratory data analysis in notebooks."
  }
]