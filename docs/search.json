[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy Intelligence",
    "section": "",
    "text": "Tidy Data: Tabular Data Storage Comparison\n\n\nA comparison of popular open-source data storage technologies using R and Python.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Visualization: ggplot2 vs seaborn\n\n\nA comparison of implementations of the grammar of graphics in R and Python.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Visualization: ggplot2 vs plotnine\n\n\nA comparison of implementations of the grammar of graphics in R and Python.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Manipulation: dplyr vs TidierData\n\n\nA comparison of R’s dplyr and Julia’s TidierData data manipulation packages\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Manipulation: dplyr vs pandas\n\n\nA comparison of R’s dplyr and Python’s pandas data manipulation packages\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Manipulation: dplyr vs ibis\n\n\nA comparison of R’s dplyr and Python’s ibis data manipulation packages\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Manipulation: dplyr vs polars\n\n\nA comparison of R’s dplyr and Python’s polars data manipulation packages\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Seasonality in DAX Returns\n\n\nDebunking the Halloween indicator for German market returns using R\n\n\n\n\n\n\n\n\n\n\n\n\n\nScraping ESG Data from Yahoo Finance\n\n\nHow to scrape environmental, social and governance risk scores using R\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering Binary Data\n\n\nAn application of different unsupervised learning approaches to cluster simulated survey responses using R\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data: A Recipe for Efficient Data Analysis\n\n\nOn the importance of tidy data for efficient analysis using the analogy of a well-organized kitchen\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Collaborative Filtering: Building A Stock Recommender\n\n\nA simple implementation for prototyping multiple collaborative filtering algorithms using R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "",
    "text": "In this post, I provide a simple approach to scrape Environmental, Social and Governance (ESG) information from Yahoo Finance (e.g., Apple) using the programming language R. Yahoo Finance provides total ESG scores, environment, social and governance risk scores, as well as controversy levels, all compiled by Sustainalytics (which is by now owned by Morningstar). My code builds on the walk-through by Kyle Ruden, which I adapted to the current page structure of Yahoo Finance and my own coding style. In addition, I added a few steps that I, as web scraping newbie, had to look up while going through his guide.\nTo begin with, I want to urge you to read at least the legal and ethical considerations put forward by Kyle. Most importantly, I want to mention that, when performing web scraping tasks, it is both good practice and often required to set a custom user agent request header to identify yourself, as well as sending requests at a modest rate to ‘smell like a human’. I consider both of these key aspects in my code below.\nThroughout this post, I rely on the following packages:\nlibrary(tidyverse) # overall grammar\nlibrary(tidytext)  # only for reorder_within & scale_y_reordered functions\nlibrary(scales)    # only for percent function\nlibrary(httr2)     # for making http requests\nlibrary(rvest)     # for web scraping function\nlibrary(robotstxt) # only for paths_allowed function"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#get-symbols",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#get-symbols",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Get Symbols",
    "text": "Get Symbols\nFirst, we want to get some companies for which we want to scrap ESG information from Yahoo Finance. Let us get a table of symbols and industry information of the S&P 500 constituents from Wikipedia. The function read_html normalizes the page to a valid XML document. html_nodes then allows us to point exactly to the table we can find on the website using the name of the CSS node. html_table then parses the HTML table into a data frame. Note that, as one of the last steps, we need to replace all dots in the symbols with dashes to get the symbols used by Yahoo Finance.\n\nwikipedia_link &lt;- \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n\nsymbols &lt;- read_html(wikipedia_link) |&gt; \n  html_nodes(css = \"table[id='constituents']\") |&gt; \n  html_table() \n\nsymbols &lt;- symbols[[1]] |&gt; \n  select(symbol = Symbol, \n         company = Security, \n         sector = `GICS Sector`, \n         industry = `GICS Sub-Industry`) |&gt; \n  mutate(symbol = str_replace(symbol, \"[.]\", \"-\")) |&gt; \n  arrange(symbol)\n\nThe following chunk prints what we got from Wikipedia. We will use the sector information in the last section of this post where we take a quick look at the scraped data.\n\nsymbols\n\n# A tibble: 503 × 4\n   symbol company                 sector                 industry               \n   &lt;chr&gt;  &lt;chr&gt;                   &lt;chr&gt;                  &lt;chr&gt;                  \n 1 A      Agilent Technologies    Health Care            Health Care Equipment  \n 2 AAL    American Airlines Group Industrials            Passenger Airlines     \n 3 AAPL   Apple Inc.              Information Technology Technology Hardware, S…\n 4 ABBV   AbbVie                  Health Care            Pharmaceuticals        \n 5 ABNB   Airbnb                  Consumer Discretionary Hotels, Resorts & Crui…\n 6 ABT    Abbott                  Health Care            Health Care Equipment  \n 7 ACGL   Arch Capital Group      Financials             Reinsurance            \n 8 ACN    Accenture               Information Technology IT Consulting & Other …\n 9 ADBE   Adobe Inc.              Information Technology Application Software   \n10 ADI    Analog Devices          Information Technology Semiconductors         \n# ℹ 493 more rows"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#locate-esg-information",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#locate-esg-information",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Locate ESG Information",
    "text": "Locate ESG Information\nThe point where I struggled when I tried to replicate other guides was the search for the exact location of the information that I want to scrape (and the fact that the old locations seemed to have changed). After some trial and error, it turns out that it is really easy. Once you download a web page, you can in principle either use CSS nodes or XML paths to extract information using html_nodes() as above. However, the CSS nodes on Yahoo Finance have a weird structure that is apparently not straight-forward to use in this function. Fortunately, XML paths work perfectly! Google will explain to you what these terms mean, I only demonstrate how you find the relevant paths which we use in the scraping function below.\nLet us stick to Apple as our main example and go to the sustainability tab on Yahoo Finance. If we right-click on the ESG score (e.g., using Google Chrome), we can see the the option to ‘Inspect’.\n\nOnce you click on it, a tab to the right opens where you see the underlying code. What is even more useful is the fact that the browser highlights the corresponding elements on the website as you hover over the code. This way, it is really easy to locate the information we are after. So we click on the relevant element and we copy the XML path.\n\nSo the location of the total ESG score on the page is: '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[1]/div/div[2]/div[1]'\nWe can just point-and-click on all the items we want to scrap and collect the relevant XML paths. Once we downloaded a page, we just tell html_node() where to look for the information we want and afterwards how to parse it."
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#define-functions-for-scraping",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#define-functions-for-scraping",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Define Functions for Scraping",
    "text": "Define Functions for Scraping\nMy function to scrap ESG data takes two main inputs: the stock symbol and your user agent. We got the symbols from Wikipedia, but we need to define our own user agent. For instance, I use an agent that looks like this:\n\nagent &lt;- \"Your Name (your@email.com). Doing personal research.\"\n\nThe main functions then proceeds as follows:\n\nConstruct the link of the page we want to download.\nCheck if scraping is allowed.\nDownload the page.\nExtract individual information using the XML paths we manually extracted following the point-and-click procedure from above.\nCollect all information in a table.\n\nLet us start with a function that scrapes a page for a specific symbol:\n\nscrape_sustainability_page &lt;- function(symbol, agent, max_tries = 10) {\n  link &lt;- paste0(\n    \"https://finance.yahoo.com/quote/\", symbol, \"/sustainability?p=\", symbol\n  )\n  \n  check &lt;- suppressMessages(robotstxt::paths_allowed(link))\n  \n  if (check == TRUE) {\n    resp &lt;- request(link) |&gt; \n      req_user_agent(agent) |&gt; \n      req_retry(max_tries = max_tries) |&gt; \n      req_perform()\n    \n    page &lt;- resp$body |&gt; \n      read_html()\n    \n    return(page)\n  } else {\n    stop(paste0(\"No bots allowed on page '\", link ,\"'!\"))\n  }\n}\n\nThe second function extracts the relevant information from the scraped pages and returns it as a table.\n\nextract_esg_data &lt;- function(symbol, page) {\n  scrape_date &lt;- Sys.time()\n  \n  total_esg_score &lt;- page|&gt; \n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[1]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  total_esg_percentile &lt;- page|&gt; \n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[1]/div/div[2]/div[2]/span')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  environment_risk_score &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[2]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  social_risk_score &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[3]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number() \n  \n  governance_risk_score &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[1]/div/div[4]/div/div[2]/div[1]')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  controversy_level &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[2]/div[2]/div/div/div/div[1]/div')|&gt;\n    html_text()|&gt; \n    parse_number()\n  \n  last_update_date &lt;- page|&gt;\n    html_node(xpath = '//*[@id=\"Col1-0-Sustainability-Proxy\"]/section/div[3]/span[2]/span')|&gt;\n    html_text()\n  \n  last_update_date &lt;- str_remove(last_update_date, \"Last updated on \")\n  \n  tibble(\n    symbol,\n    scrape_date,\n    total_esg_score,\n    environment_risk_score,\n    social_risk_score,\n    governance_risk_score,\n    controversy_level,\n    last_update_date\n  )\n}"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#scrape-esg-data",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#scrape-esg-data",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Scrape ESG Data",
    "text": "Scrape ESG Data\nNow, let us put everything together: we loop over all symbols to download the relevant pages and extract the relevant ESG data. I store each instance of esg_data because the scraping process is very likely to be interrupted by Yahoo Finance as it starts to block requests after some time. By using a loop, I can interrupt the execution any time and continue with the last index.\n\nfor (j in 1:nrow(symbols)) {\n  page &lt;- scrape_sustainability_page(symbols$symbol[j], agent)\n  esg_data &lt;- extract_esg_data(symbols$symbol[j], page)\n  write_rds(esg_data, paste0(\"data/esg_data_\", symbols$symbol[j], \".rds\"))\n}\n\nesg_data &lt;- list.files(\"data/\", full.names = TRUE) |&gt; \n  map_df(read_rds)\n\nThe code chunk from above takes a couple of hours in the current specification because of the increasing waiting times. The whole table then looks like this and also includes our initial example Apple:\n\nesg_data \n\n# A tibble: 503 × 8\n   symbol scrape_date         total_esg_score environment_risk_score\n   &lt;chr&gt;  &lt;dttm&gt;                        &lt;dbl&gt;                  &lt;dbl&gt;\n 1 A      2023-11-27 00:29:33              15                    0.3\n 2 AAL    2023-11-27 00:29:33              29                   11.5\n 3 AAPL   2023-11-27 00:29:35              17                    0.6\n 4 ABBV   2023-11-27 00:29:37              28                    1.1\n 5 ABNB   2023-11-27 00:29:38              NA                   NA  \n 6 ABT    2023-11-27 00:29:39              25                    3  \n 7 ACGL   2023-11-27 00:29:39              21                    1.5\n 8 ACN    2023-11-27 00:29:40              10                    0.3\n 9 ADBE   2023-11-27 00:29:41              12                    1.9\n10 ADI    2023-11-27 00:29:41              24                   10.1\n# ℹ 493 more rows\n# ℹ 4 more variables: social_risk_score &lt;dbl&gt;, governance_risk_score &lt;dbl&gt;,\n#   controversy_level &lt;dbl&gt;, last_update_date &lt;chr&gt;"
  },
  {
    "objectID": "posts/scraping-esg-data-from-yahoo-finance/index.html#quick-evaluation-of-esg-scores",
    "href": "posts/scraping-esg-data-from-yahoo-finance/index.html#quick-evaluation-of-esg-scores",
    "title": "Scraping ESG Data from Yahoo Finance",
    "section": "Quick Evaluation of ESG Scores",
    "text": "Quick Evaluation of ESG Scores\nLet us take a quick look at the data we collected. First, let us check the overall coverage of our sample:\n\nscales::percent(nrow(na.omit(esg_data)) / nrow(esg_data))\n\n[1] \"86%\"\n\n\nThis is not too bad. I believe that for most of the companies without ESG scores in my sample, Yahoo Finance does not provide any data. Admittedly, I should check manually at some point, but for the purpose of this post, this is definitely a success. To analyze sector-level breakdowns, I construct a summary table which I use as the main source for the following figures.\n\nesg_scores_sector &lt;- symbols |&gt;\n  left_join(esg_data, join_by(symbol)) |&gt; \n  group_by(sector)|&gt;\n  summarize(companies = n(),\n            coverage = sum(!is.na(total_esg_score)) / n(),\n            across(c(contains(\"score\"), controversy_level), \n                   ~mean(., na.rm = TRUE)))|&gt;\n  arrange(-coverage)\n\nThe first figure gives us the coverage per sector. All real estate companies have ESG scores, while only a bit more than three quarters of communication services feature this information.\n\nesg_scores_sector|&gt;\n  mutate(labels = paste0(companies * coverage, \" out of \", companies))|&gt;\n  ggplot(aes(y = reorder(sector, coverage), \n             x = coverage, fill = factor(round(coverage, 0)))) +\n  geom_col(show.legend = FALSE) + \n  theme_minimal() + \n  geom_text(aes(label = labels), hjust = 1.1, color = \"white\") +\n  coord_cartesian(xlim = c(0, 1)) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = NULL, y = NULL,\n       title = \"Number of companies with ESG scores per sector\",\n       subtitle = \"Based on Yahoo Finance and S&P 500 data as of November 2023\")\n\n\n\n\n\n\n\n\nNext, I want to look at average ESG scores by sector. For instance, the real estate sector has the lowest total ESG score, indicating the lowest degree to which a sector’s business value is at risk driven by environmental, social and governance risks. Financials exhibit the the lowest environmental risk, while the energy sector (at least the part included in the S&P 500) has the highest exposure to environmental risks.\n\nesg_scores_sector|&gt;\n  pivot_longer(cols = contains(\"score\"))|&gt;\n  mutate(name = str_to_title(str_replace_all(name, \"_\", \" \")),\n         name = factor(name),\n         sector = tidytext::reorder_within(sector, -value, name))|&gt;\n  ggplot(aes(y = sector, x = value, fill = name)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~name, scales = \"free_y\") +\n  theme_minimal() + \n  tidytext::scale_y_reordered() +\n  geom_text(aes(label = round(value, 0)), hjust = 1.1, color = \"white\") +\n  labs(y = NULL, x = NULL,\n       title = \"Average ESG scores per sector\",\n       subtitle = \"Based on Yahoo Finance and S&P 500 data as of November 2023\")\n\n\n\n\n\n\n\n\nFinally, I am also interested in the average controversy level which measures to which degree companies are involved in incidents and events that may negatively impact stakeholders, the environment or their operations. I decided to plot the controversy of each sector relative to the average overall controversy. Real estate and information technology seem to be far less controverse than consumer staples and communication services.\n\nesg_scores_sector|&gt;\n  mutate(controversy_relative = controversy_level - mean(controversy_level)) |&gt; \n  ggplot(aes(y = reorder(sector, -controversy_relative), \n             x = controversy_relative, fill = (controversy_relative &lt; 0))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_minimal() + theme(legend.position = \"none\") + \n  coord_cartesian(xlim = c(-1.5, 1.5)) +\n  labs(y = NULL, x = NULL,\n       title = \"Average sector-level controversy relative to overall controversy\",\n       subtitle = \"Based on Yahoo Finance and S&P 500 data as of November 2023\")\n\n\n\n\n\n\n\n\nI think there is a lot more interesting stuff to uncover using the ESG scores, but for now I’ll leave it at that. I am nonetheless surprised, how easy scraping information from websites is using these amazing packages."
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html",
    "href": "posts/dplyr-vs-ibis/index.html",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "",
    "text": "There are a myriad of options to perform essential data manipulation tasks in R and Python (see, for instance, my other posts on dplyr vs pandas and dplyr vs polars). However, if we want to do tidy data science in R, there is a clear forerunner: dplyr. In the world of Python, ibis has been around since 2015 but recently gained traction due to its appealing flexibility with respect to data backends. In this blog post, I illustrate their syntactic similarities and highlight differences between these two packages that emerge for a few key tasks.\nBefore we dive into the comparison, a short introduction to the packages: the dplyr package in R allows users to refer to columns without quotation marks due to its implementation of non-standard evaluation (NSE). NSE is a programming technique used in R that allows functions to capture the expressions passed to them as arguments, rather than just the values of those arguments. The primary goal of NSE in the context of dplyr is to create a more user-friendly and intuitive syntax. This makes data manipulation tasks more straightforward and aligns with the general philosophy of the tidyverse to make data science faster, easier, and more fun.1\nibis is a Python library that provides a lightweight and universal interface for data wrangling using many different data backends. The core idea behind ibis is to provide Python users with a familiar pandas-like syntax while allowing them to work with larger datasets that don’t fit into memory. As you see in the post below, the ibis syntax can be surprisingly closer to dplyr than to the original idea of resembling pandas. In addition, ibis builds an expression tree as you write code. This tree is then translated into the native query language of the target data source, be it SQL or something else, and executed remotely (similar to the dbplyr package in R). This approach ensures that only the final results are loaded into Python, significantly reducing memory overhead."
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#filter-rows",
    "href": "posts/dplyr-vs-ibis/index.html#filter-rows",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Filter rows",
    "text": "Filter rows\nFiltering rows works very similarly for both packages, they even have the same function names: dplyr::filter() and ibis.filter(). To select columns in ibis, you need the ibis._ selector. Note that you have to provide a dictionary to ibis.filter() in case you want to have multiple conditions.\n\ndplyribis\n\n\n\npenguins |&gt; \n  filter(species == \"Adelie\" & \n           island %in% c(\"Biscoe\", \"Dream\"))\n\n# A tibble: 100 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 90 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .filter([\n    _.species == \"Adelie\", \n    _.island.isin([\"Biscoe\", \"Dream\"])\n  ]) \n)\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ … │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Biscoe │           37.8 │          18.3 │             174.0 │ … │\n│ Adelie  │ Biscoe │           37.7 │          18.7 │             180.0 │ … │\n│ Adelie  │ Biscoe │           35.9 │          19.2 │             189.0 │ … │\n│ Adelie  │ Biscoe │           38.2 │          18.1 │             185.0 │ … │\n│ Adelie  │ Biscoe │           38.8 │          17.2 │             180.0 │ … │\n│ Adelie  │ Biscoe │           35.3 │          18.9 │             187.0 │ … │\n│ Adelie  │ Biscoe │           40.6 │          18.6 │             183.0 │ … │\n│ Adelie  │ Biscoe │           40.5 │          17.9 │             187.0 │ … │\n│ Adelie  │ Biscoe │           37.9 │          18.6 │             172.0 │ … │\n│ Adelie  │ Biscoe │           40.5 │          18.9 │             180.0 │ … │\n│ …       │ …      │              … │             … │                 … │ … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴───┘"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#slice-rows",
    "href": "posts/dplyr-vs-ibis/index.html#slice-rows",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Slice rows",
    "text": "Slice rows\ndplyr::slice() takes integers with row numbers as inputs, so you can use ranges and arbitrary vectors of integers. ibis.limit() only takes the number of rows to slice and the number of rows to skip as inputs. For instance, to the the same result of slicing rows 10 to 20, the code looks as follows (note that indexing starts at 0 in Python, while it starts at 1 in R):\n\ndplyribis\n\n\n\npenguins |&gt; \n  slice(10:20)\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           42            20.2               190        4250\n 2 Adelie  Torgersen           37.8          17.1               186        3300\n 3 Adelie  Torgersen           37.8          17.3               180        3700\n 4 Adelie  Torgersen           41.1          17.6               182        3200\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           36.6          17.8               185        3700\n 8 Adelie  Torgersen           38.7          19                 195        3450\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           34.4          18.4               184        3325\n11 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .limit(11, offset = 9) \n)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           42.0 │          20.2 │             190.0 │ … │\n│ Adelie  │ Torgersen │           37.8 │          17.1 │             186.0 │ … │\n│ Adelie  │ Torgersen │           37.8 │          17.3 │             180.0 │ … │\n│ Adelie  │ Torgersen │           41.1 │          17.6 │             182.0 │ … │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │             191.0 │ … │\n│ Adelie  │ Torgersen │           34.6 │          21.1 │             198.0 │ … │\n│ Adelie  │ Torgersen │           36.6 │          17.8 │             185.0 │ … │\n│ Adelie  │ Torgersen │           38.7 │          19.0 │             195.0 │ … │\n│ Adelie  │ Torgersen │           42.5 │          20.7 │             197.0 │ … │\n│ Adelie  │ Torgersen │           34.4 │          18.4 │             184.0 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#arrange-rows",
    "href": "posts/dplyr-vs-ibis/index.html#arrange-rows",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Arrange rows",
    "text": "Arrange rows\nTo orders the rows of a data frame by the values of selected columns, we have dplyr::arrange() and ibis.order_by(). Both approaches arrange rows in an an ascending order and puts missing values last. Again, you need to provide a dictionary to ibis.order_by().\n\ndplyribis\n\n\n\npenguins |&gt; \n  arrange(island, desc(bill_length_mm))\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .order_by([_.island, _.bill_length_mm.desc()])\n)\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ … │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Gentoo  │ Biscoe │           59.6 │          17.0 │             230.0 │ … │\n│ Gentoo  │ Biscoe │           55.9 │          17.0 │             228.0 │ … │\n│ Gentoo  │ Biscoe │           55.1 │          16.0 │             230.0 │ … │\n│ Gentoo  │ Biscoe │           54.3 │          15.7 │             231.0 │ … │\n│ Gentoo  │ Biscoe │           53.4 │          15.8 │             219.0 │ … │\n│ Gentoo  │ Biscoe │           52.5 │          15.6 │             221.0 │ … │\n│ Gentoo  │ Biscoe │           52.2 │          17.1 │             228.0 │ … │\n│ Gentoo  │ Biscoe │           52.1 │          17.0 │             230.0 │ … │\n│ Gentoo  │ Biscoe │           51.5 │          16.3 │             230.0 │ … │\n│ Gentoo  │ Biscoe │           51.3 │          14.2 │             218.0 │ … │\n│ …       │ …      │              … │             … │                 … │ … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴───┘"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#select-columns",
    "href": "posts/dplyr-vs-ibis/index.html#select-columns",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Select columns",
    "text": "Select columns\nSelecting a subset of columns works essentially the same for both and dplyr::select() and ibis.select() even have the same name. Note that you don’t have to use ibis._ but can also just pass strings in the ibis.select() method.\n\ndplyribis\n\n\n\npenguins |&gt; \n  select(bill_length_mm, sex)\n\n# A tibble: 344 × 2\n   bill_length_mm sex   \n            &lt;dbl&gt; &lt;fct&gt; \n 1           39.1 male  \n 2           39.5 female\n 3           40.3 female\n 4           NA   &lt;NA&gt;  \n 5           36.7 female\n 6           39.3 male  \n 7           38.9 female\n 8           39.2 male  \n 9           34.1 &lt;NA&gt;  \n10           42   &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n\n\n(penguins\n  .select(_.bill_length_mm, _.sex)\n)\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n┃ bill_length_mm ┃ sex    ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n│ float64        │ string │\n├────────────────┼────────┤\n│           39.1 │ male   │\n│           39.5 │ female │\n│           40.3 │ female │\n│           NULL │ NULL   │\n│           36.7 │ female │\n│           39.3 │ male   │\n│           38.9 │ female │\n│           39.2 │ male   │\n│           34.1 │ NULL   │\n│           42.0 │ NULL   │\n│              … │ …      │\n└────────────────┴────────┘\n\n\n\n\n\n\nRename columns\nRenaming columns also works very similarly with the major difference that ibis.rename() does not accept the column selector ibis._ on the right-hand side, while dplyr::rename() takes variable names via the usual NSE.\n\ndplyribis\n\n\n\npenguins |&gt; \n  rename(bill_length = bill_length_mm,\n         bill_depth = bill_depth_mm)\n\n# A tibble: 344 × 8\n   species island    bill_length bill_depth flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen        39.1       18.7               181        3750 male  \n 2 Adelie  Torgersen        39.5       17.4               186        3800 female\n 3 Adelie  Torgersen        40.3       18                 195        3250 female\n 4 Adelie  Torgersen        NA         NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen        36.7       19.3               193        3450 female\n 6 Adelie  Torgersen        39.3       20.6               190        3650 male  \n 7 Adelie  Torgersen        38.9       17.8               181        3625 female\n 8 Adelie  Torgersen        39.2       19.6               195        4675 male  \n 9 Adelie  Torgersen        34.1       18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen        42         20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .rename(bill_length = \"bill_length_mm\", \n          bill_depth = \"bill_depth_mm\")\n)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length ┃ bill_depth ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64     │ float64    │ float64           │ … │\n├─────────┼───────────┼─────────────┼────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │        39.1 │       18.7 │             181.0 │ … │\n│ Adelie  │ Torgersen │        39.5 │       17.4 │             186.0 │ … │\n│ Adelie  │ Torgersen │        40.3 │       18.0 │             195.0 │ … │\n│ Adelie  │ Torgersen │        NULL │       NULL │              NULL │ … │\n│ Adelie  │ Torgersen │        36.7 │       19.3 │             193.0 │ … │\n│ Adelie  │ Torgersen │        39.3 │       20.6 │             190.0 │ … │\n│ Adelie  │ Torgersen │        38.9 │       17.8 │             181.0 │ … │\n│ Adelie  │ Torgersen │        39.2 │       19.6 │             195.0 │ … │\n│ Adelie  │ Torgersen │        34.1 │       18.1 │             193.0 │ … │\n│ Adelie  │ Torgersen │        42.0 │       20.2 │             190.0 │ … │\n│ …       │ …         │           … │          … │                 … │ … │\n└─────────┴───────────┴─────────────┴────────────┴───────────────────┴───┘"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#mutate-columns",
    "href": "posts/dplyr-vs-ibis/index.html#mutate-columns",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Mutate columns",
    "text": "Mutate columns\nTransforming existing columns or creating new ones is an essential part of data analysis. dplyr::mutate() and ibis.mutate() are the work horses for these tasks. A big difference between dplyr::mutate() and ibis.mutate() is that in the latter you have to chain separate mutate calls together when you reference newly-created columns in the same mutate whereas in dplyr, you can put them all in the same call.\n\ndplyribis\n\n\n\npenguins |&gt; \n  mutate(ones = 1,\n         bill_length = bill_length_mm / 10,\n         bill_length_squared = bill_length^2) |&gt; \n  select(ones, bill_length_mm, bill_length, bill_length_squared)\n\n# A tibble: 344 × 4\n    ones bill_length_mm bill_length bill_length_squared\n   &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1     1           39.1        3.91                15.3\n 2     1           39.5        3.95                15.6\n 3     1           40.3        4.03                16.2\n 4     1           NA         NA                   NA  \n 5     1           36.7        3.67                13.5\n 6     1           39.3        3.93                15.4\n 7     1           38.9        3.89                15.1\n 8     1           39.2        3.92                15.4\n 9     1           34.1        3.41                11.6\n10     1           42          4.2                 17.6\n# ℹ 334 more rows\n\n\n\n\n\n(penguins \n  .mutate(ones = 1, \n          bill_length = _.bill_length_mm / 10)\n  .mutate(bill_length_squared = _.bill_length**2)\n  .select(_.ones, _.bill_length_mm, _.bill_length, _.bill_length_squared)\n)\n\n┏━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ ones ┃ bill_length_mm ┃ bill_length ┃ bill_length_squared ┃\n┡━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ int8 │ float64        │ float64     │ float64             │\n├──────┼────────────────┼─────────────┼─────────────────────┤\n│    1 │           39.1 │        3.91 │             15.2881 │\n│    1 │           39.5 │        3.95 │             15.6025 │\n│    1 │           40.3 │        4.03 │             16.2409 │\n│    1 │           NULL │        NULL │                NULL │\n│    1 │           36.7 │        3.67 │             13.4689 │\n│    1 │           39.3 │        3.93 │             15.4449 │\n│    1 │           38.9 │        3.89 │             15.1321 │\n│    1 │           39.2 │        3.92 │             15.3664 │\n│    1 │           34.1 │        3.41 │             11.6281 │\n│    1 │           42.0 │        4.20 │             17.6400 │\n│    … │              … │           … │                   … │\n└──────┴────────────────┴─────────────┴─────────────────────┘\n\n\n\n\n\n\ndplyrelocate columns\ndplyr::relocate() provides options to change the positions of columns in a data frame, using the same syntax as dplyr::select(). In addition, there are the options .after and .before to provide users with additional shortcuts.\nThe recommended way to relocate columns in ibis is to use the ibis.select() method, but there are no options as in dplyr::relocate(). In fact, the safest way to consistently get the correct order of columns is to explicitly specify them.\n\ndplyribis\n\n\n\npenguins |&gt; \n  relocate(c(species, bill_length_mm), .before = sex)\n\n# A tibble: 344 × 8\n   island    bill_depth_mm flipper_length_mm body_mass_g species bill_length_mm\n   &lt;fct&gt;             &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1 Torgersen          18.7               181        3750 Adelie            39.1\n 2 Torgersen          17.4               186        3800 Adelie            39.5\n 3 Torgersen          18                 195        3250 Adelie            40.3\n 4 Torgersen          NA                  NA          NA Adelie            NA  \n 5 Torgersen          19.3               193        3450 Adelie            36.7\n 6 Torgersen          20.6               190        3650 Adelie            39.3\n 7 Torgersen          17.8               181        3625 Adelie            38.9\n 8 Torgersen          19.6               195        4675 Adelie            39.2\n 9 Torgersen          18.1               193        3475 Adelie            34.1\n10 Torgersen          20.2               190        4250 Adelie            42  \n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .select(_.island, _.bill_depth_mm, _.flipper_length_mm, _.body_mass_g, \n          _.species, _.bill_length_mm, _.sex)\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━┳━━━┓\n┃ island    ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ species ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━╇━━━┩\n│ string    │ float64       │ float64           │ float64     │ string  │ … │\n├───────────┼───────────────┼───────────────────┼─────────────┼─────────┼───┤\n│ Torgersen │          18.7 │             181.0 │      3750.0 │ Adelie  │ … │\n│ Torgersen │          17.4 │             186.0 │      3800.0 │ Adelie  │ … │\n│ Torgersen │          18.0 │             195.0 │      3250.0 │ Adelie  │ … │\n│ Torgersen │          NULL │              NULL │        NULL │ Adelie  │ … │\n│ Torgersen │          19.3 │             193.0 │      3450.0 │ Adelie  │ … │\n│ Torgersen │          20.6 │             190.0 │      3650.0 │ Adelie  │ … │\n│ Torgersen │          17.8 │             181.0 │      3625.0 │ Adelie  │ … │\n│ Torgersen │          19.6 │             195.0 │      4675.0 │ Adelie  │ … │\n│ Torgersen │          18.1 │             193.0 │      3475.0 │ Adelie  │ … │\n│ Torgersen │          20.2 │             190.0 │      4250.0 │ Adelie  │ … │\n│ …         │             … │                 … │           … │ …       │ … │\n└───────────┴───────────────┴───────────────────┴─────────────┴─────────┴───┘"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#simple-summaries-by-group",
    "href": "posts/dplyr-vs-ibis/index.html#simple-summaries-by-group",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Simple summaries by group",
    "text": "Simple summaries by group\nLet’s suppose we want to compute summaries by groups such as means or medians. Both packages are very similar again: on the R side you have dplyr::group_by() and dplyr::summarize(), while on the Python side you have ibis.group_by() and ibis.aggregate().\nNote that dplyr::group_by() also automatically arranges the results by the group, so the reproduce the results of dplyr, we need to add ibis.order_by() to the chain.\n\ndplyribis\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  island    bill_depth_mean\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Biscoe               15.9\n2 Dream                18.3\n3 Torgersen            18.4\n\n\n\n\n\n(penguins\n  .group_by(\"island\")\n  .aggregate(bill_depth_mean = _.bill_depth_mm.mean())\n  .order_by(\"island\")\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ island    ┃ bill_depth_mean ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ float64         │\n├───────────┼─────────────────┤\n│ Biscoe    │       15.874850 │\n│ Dream     │       18.344355 │\n│ Torgersen │       18.429412 │\n└───────────┴─────────────────┘"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#more-complicated-summaries-by-group",
    "href": "posts/dplyr-vs-ibis/index.html#more-complicated-summaries-by-group",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "More complicated summaries by group",
    "text": "More complicated summaries by group\nTypically, you want to create multiple different summaries by groups. dplyr provides a lot of flexibility to create new variables on the fly, as does ibis. For instance, we can pass expressions to them mean functions in order to create the share of female penguins per island in the summary statement.\n\ndplyribis\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(\n    count = n(),\n    bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE),\n    flipper_length_median = median(flipper_length_mm, na.rm = TRUE),\n    body_mass_sd = sd(body_mass_g, na.rm = TRUE),\n    share_female = mean(sex == \"female\", na.rm = TRUE)\n  )\n\n# A tibble: 3 × 6\n  island   count bill_depth_mean flipper_length_median body_mass_sd share_female\n  &lt;fct&gt;    &lt;int&gt;           &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Biscoe     168            15.9                   214         783.        0.491\n2 Dream      124            18.3                   193         417.        0.496\n3 Torgers…    52            18.4                   191         445.        0.511\n\n\n\n\n\n(penguins\n  .group_by(\"island\")\n  .aggregate(\n    count = _.count(),\n    bill_depth_mean = _.bill_depth_mm.mean(),\n    flipper_length_median = _.flipper_length_mm.median(),\n    body_mass_sd = _.body_mass_g.std(),\n    share_female = (_.sex == \"female\").mean()\n  )\n  .order_by(\"island\")\n)\n\n┏━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━┓\n┃ island    ┃ count ┃ bill_depth_mean ┃ flipper_length_medi… ┃ body_mass_sd ┃  ┃\n┡━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━┩\n│ string    │ int64 │ float64         │ float64              │ float64      │  │\n├───────────┼───────┼─────────────────┼──────────────────────┼──────────────┼──┤\n│ Biscoe    │   168 │       15.874850 │                214.0 │   782.855743 │  │\n│ Dream     │   124 │       18.344355 │                193.0 │   416.644112 │  │\n│ Torgersen │    52 │       18.429412 │                191.0 │   445.107940 │  │\n└───────────┴───────┴─────────────────┴──────────────────────┴──────────────┴──┘"
  },
  {
    "objectID": "posts/dplyr-vs-ibis/index.html#footnotes",
    "href": "posts/dplyr-vs-ibis/index.html#footnotes",
    "title": "Tidy Data Manipulation: dplyr vs ibis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the unifying principles of the tidyverse: https://design.tidyverse.org/unifying.html.↩︎"
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html",
    "href": "posts/ggplot2-vs-plotnine/index.html",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "",
    "text": "Both ggplot2 and plotnine are based on Leland Wilkinson’s Grammar of Graphics, a set of principles for creating consistent and effective statistical graphics. This means they both use similar syntax and logic for constructing plots, making it relatively easy for users to transition between them. ggplot2, developed by Hadley Wickham, is a cornerstone of the R community and integrates seamlessly with other tidyverse packages. plotnine, on the other hand, is a Python package that attempts to bring ggplot2 functionality and philosophy to Python users, but it is not part of a larger ecosystem (although it works well with pandas, Python’s most popular data manipulation package).\nBoth packages use a layer-based approach, where a plot is built up by adding components like axes, geoms, stats, and scales. However, ggplot2 benefits from R’s native support for data frames and its formula notation, which can make its syntax more concise. plotnine has to adhere to Python’s syntax rules, in particular referring to columns via strings, which can occasionally lead to more verbose code. As you can see in the examples below, the syntactic differences are miniscule.\nThe types of plots that I chose for the comparison heavily draw on the examples given in R for Data Science - an amazing resource if you want to get started with data visualization."
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#a-categorical-variable",
    "href": "posts/ggplot2-vs-plotnine/index.html#a-categorical-variable",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "A categorical variable",
    "text": "A categorical variable\nLet’s break down the similarity in smaller steps by focusing on simpler examples. If you have a categorical variable and want to compare its relevance in your data, then geom_bar() is your friend.\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = island)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n       aes(x = \"island\"))\n  + geom_bar()\n)\n\n&lt;Figure Size: (1280 x 960)&gt;"
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#a-numerical-variable",
    "href": "posts/ggplot2-vs-plotnine/index.html#a-numerical-variable",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "A numerical variable",
    "text": "A numerical variable\nIf you have a numerical variable, usually histograms are a good starting point to get a better feeling for the distribution of your data. geom_histogram() with options to control bin widths or number of bins is the aesthetic for this task.\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n       aes(x = \"bill_length_mm\"))\n  + geom_histogram(binwidth = 2)\n)\n\n&lt;Figure Size: (1280 x 960)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nBoth packages also support the geom_density() geom to plot density curves, but I personally wouldn’t recommend to start with densities because they are estimated curves that might obscure underlying data features. However, we look at densities in the next section."
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#a-numerical-and-a-categorical-variable",
    "href": "posts/ggplot2-vs-plotnine/index.html#a-numerical-and-a-categorical-variable",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "A numerical and a categorical variable",
    "text": "A numerical and a categorical variable\nTo visualize relationships, you need to have at least two columns. If you have a numerical and a categorical variable, then histograms or densities with groups are a good starting point. The next example illustrates the use of geom_density().\nNote that plotnine still uses the historical size option and not the new linewidth wording (see this blog post here). Maybe this will change in the future, so keep an eye on this issue to stay up to date.\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(linewidth = 0.75, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n       aes(x = \"body_mass_g\", color = \"species\", fill = \"species\"))\n  + geom_density(size = 0.75, alpha = 0.5)\n)\n\n&lt;Figure Size: (1280 x 960)&gt;"
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#two-categorical-columns",
    "href": "posts/ggplot2-vs-plotnine/index.html#two-categorical-columns",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "Two categorical columns",
    "text": "Two categorical columns\nStacked bar plots are a good way to display the relationship between two categorical columns. geom_bar() with the position argument is your aesthetic of choice for this task. Note that you can easily switch to counts by using position = \"identity\" instead of relative frequencies as in the example below.\n\nggplot2plotnine\n\n\n\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, aes(x = \"species\", fill = \"island\"))\n  + geom_bar(position = \"fill\")\n)\n\n&lt;Figure Size: (1280 x 960)&gt;"
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#two-numerical-columns",
    "href": "posts/ggplot2-vs-plotnine/index.html#two-numerical-columns",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "Two numerical columns",
    "text": "Two numerical columns\nScatter plots and regression lines are definitely the most common approach for visualizing the relationship between two numerical columns. Here, the size parameter controls the size of the shapes that you use for the data points. See the first visualization example if you want to see again how to add a regression line.\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(size = 2)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n        aes(x = \"bill_length_mm\", y = \"flipper_length_mm\"))\n  + geom_point(size = 2)\n)\n\n&lt;Figure Size: (1280 x 960)&gt;"
  },
  {
    "objectID": "posts/ggplot2-vs-plotnine/index.html#three-or-more-columns",
    "href": "posts/ggplot2-vs-plotnine/index.html#three-or-more-columns",
    "title": "Tidy Data Visualization: ggplot2 vs plotnine",
    "section": "Three or more columns",
    "text": "Three or more columns\nYou can include more information by mapping columns to additional aesthetics. For instance, we can map colors and shapes to species and create separate plots for each island by using facets. Facets are actually a great way to extend your figures, so I highly recommend playing around with them using your own data.\n\nggplot2plotnine\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins, \n       aes(x = \"bill_length_mm\", y = \"flipper_length_mm\"))\n  + geom_point(aes(color = \"island\", shape = \"island\"))\n  + facet_wrap(\"~species\")\n)\n\n&lt;Figure Size: (1280 x 960)&gt;"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html",
    "href": "posts/clustering-binary-data/index.html",
    "title": "Clustering Binary Data",
    "section": "",
    "text": "In this post, I tackle the challenge to extract a small number of typical respondent profiles from a large scale survey with multiple yes-no questions. This type of setting corresponds to a classification problem without knowing the true labels of the observations – also known as unsupervised learning.\nTechnically speaking, we have a set of \\(N\\) observations \\((x_1, x_2, ... , x_N)\\) of a random \\(p\\)-vector \\(X\\) with joint density \\(\\text{Pr}(X)\\). The goal of classification is to directly infer the properties of this probability density without the help of the correct answers (or degree-of-error) for each observation. In this note, we focus on cluster analysis that attempts to find convex regions of the \\(X\\)-space that contain modes of \\(\\text{Pr}(X)\\). This approach aims to tell whether \\(\\text{Pr}(X)\\) can be represented by a mixture of simpler densities representing distinct classes of observations.\nIntuitively, we want to find clusters of the survey responses such that respondents within each cluster are more closely related to one another than respondents assigned to different clusters. There are many possible ways to achieve that, but we focus on the most popular and most approachable ones: \\(K\\)-means, \\(K\\)-modes, as well as agglomerative and divisive hierarchical clustering. As we see below, the 4 models yield quite different results for clustering binary data.\nWe use the following packages throughout this post. In particular, we use klaR and cluster for clustering algorithms that go beyond the stats package that is included with your R installation.1\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(klaR)\nlibrary(cluster)\nNote that there will be an annoying namespace conflict between MASS::select() and dplyr::select()). We use the conflicted package to explicitly resolve these conflicts.\nlibrary(conflicted)\n\nconflicts_prefer(\n  dplyr::filter,\n  dplyr::lag,\n  dplyr::select\n)"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#creating-sample-data",
    "href": "posts/clustering-binary-data/index.html#creating-sample-data",
    "title": "Clustering Binary Data",
    "section": "Creating sample data",
    "text": "Creating sample data\nLet us start by creating some sample data where we basically exactly know which kind of answer profiles are out there. Later, we evaluate the cluster models according to how well they are doing in uncovering the clusters and assigning respondents to clusters. We assume that there are 4 yes/no questions labeled q1, q2, q3 and q4. In addition, there are 3 different answer profiles where cluster 1 answers positively to the first question only, cluster 2 answers positively to question 2 and 3 and cluster 3 answers all questions positively. We also define the the number of respondents for each cluster.\n\ncenters &lt;- tibble(\n  cluster = factor(1:3), \n  respondents = c(250, 500, 200),\n  q1 = c(1, 0, 1),\n  q2 = c(0, 1, 1),             \n  q3 = c(0, 1, 1),\n  q4 = c(0, 0, 1)\n)\n\nAlternatively, we could think of the yes/no questions as medical records that indicate whether the subject has a certain pre-condition or not.\nSince it should be a bit tricky for the clustering models to find the actual response profiles, let us add some noise in the form of respondents that deviate from their assigned cluster profile and shuffle all rows. We find out below how the cluster algorithms are able to deal with this noise.\n\nset.seed(42)\nlabelled_respondents &lt;- centers |&gt; \n  mutate(\n    across(\n      starts_with(\"q\"),\n      ~map2(respondents, .x, function(x, y) {\n        rbinom(x, 1, max((y - 0.1), 0.1))\n      }),\n      .names = \"{col}\"\n    )\n  ) |&gt; \n  select(-respondents) |&gt; \n  unnest(cols = c(q1, q2, q3, q4)) |&gt; \n  sample_n(n())\n\nThe figure below visualizes the distribution of simulated question responses by cluster.\n\nlabelled_respondents |&gt;\n  pivot_longer(cols = -cluster, \n               names_to = \"question\", values_to = \"response\") |&gt;\n  mutate(response = response == 1) |&gt;\n  ggplot(aes(x = response, y = question, color = cluster)) +\n  geom_jitter() +\n  theme_bw() +\n  labs(x = \"Response\", y = \"Question\", color = \"Cluster\",\n       title = \"Visualization of simulated question responses by cluster\")"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#k-means-clustering",
    "href": "posts/clustering-binary-data/index.html#k-means-clustering",
    "title": "Clustering Binary Data",
    "section": "\\(K\\)-means clustering",
    "text": "\\(K\\)-means clustering\nThe \\(K\\)-means algorithm is one of the most popular clustering methods (see also this tidymodels example). It is intended for situations in which all variables are of the quantitative type since it partitions all respondents into \\(k\\) groups such that the sum of squares from respondents to the assigned cluster centers are minimized. For binary data, the Euclidean distance reduces to counting the number of variables on which two cases disagree.\nThis leads to a problem (which is also described here) because of an arbitrary cluster assignment after cluster initialization. The first chosen clusters are still binary data and hence observations have integer distances from each of the centers. The corresponding ties are hard to overcome in any meaningful way. Afterwards, the algorithm computes means in clusters and revisits assignments. Nonetheless, \\(K\\)-means might produce informative results in a fast and easy to interpret way. We hence include it in this post for comparison.\nTo run the \\(K\\)-means algorithm, we first drop the cluster column.\n\nrespondents &lt;- labelled_respondents |&gt;\n  select(-cluster)\n\nIt is very straight-forward to run the built-in stats::kmeans clustering algorithm. We choose the parameter of maximum iterations to be 1000 to increase the likeliness of getting the best fitting clusters. Since the data is fairly small and the algorithm is also quite fast, we see no harm in using a high number of iterations.\n\niter_max &lt;- 1000\nkmeans_example &lt;- stats::kmeans(respondents, centers = 3, iter.max = iter_max)\n\nThe output of the algorithm is a list with different types of information including the assigned clusters for each respondent.\nAs we want to compare cluster assignment across different models and we repeatedly assign different clusters to respondents, we write up a helper function that adds assignments to the respondent data from above. The function shows that \\(K\\)-means and \\(K\\)-modes contain a field with cluster information. The two hierarchical cluster models, however, need to be cut a the desired number of clusters (more on that later).\n\nassign_clusters &lt;- function(model, k = NULL) {\n  if (class(model)[1] %in% c(\"kmeans\", \"kmodes\")) {\n    cluster_assignment &lt;- model$cluster\n  }\n  if (class(model)[1] %in% c(\"agnes\", \"diana\")) {\n    if (is.null(k)) {\n      stop(\"k required for hierarchical models!\")\n    }\n    cluster_assignment &lt;- stats::cutree(model, k = k)\n  }\n  \n  clusters &lt;- respondents |&gt;\n    mutate(cluster = cluster_assignment)\n  \n  return(clusters)\n}\n\nIn addition, we introduce a helper function that summarizes information by cluster. In particular, the function computes average survey responses (which correspond to proportion of yes answers in the current setting) and sorts the clusters according to the total number of positive answers. The latter helps us later to compare clusters across different models.\n\nsummarize_clusters &lt;- function(model, k = NULL) {\n\n  clusters &lt;- assign_clusters(model = model, k = k)\n  \n  summary_statistics &lt;- clusters |&gt;\n    group_by(cluster) |&gt;\n    summarize(across(matches(\"q\"), \\(x) mean(x, na.rm = TRUE)),\n              assigned_respondents = n()) |&gt;\n    select(-cluster) |&gt;\n    mutate(total = rowSums(across(matches(\"q\")))) |&gt;\n    arrange(-total) |&gt;\n    mutate(k = row_number(),\n           model = class(model)[1])\n  \n  return(summary_statistics)\n}\n\nWe could easily introduce other summary statistics into the function, but the current specification is sufficient for the purpose of this note.\n\nkmeans_example &lt;- summarize_clusters(kmeans_example)\n\nSince we do not know the true number of clusters in real-world settings, we want to compare the performance of clustering models for different numbers of clusters. Since we know that the true number of clusters is 3 in the current setting, let us stick to a maximum of 7 clusters. In practice, you might of course choose an arbitrary maximum number of clusters.\n\nk_min &lt;- 1\nk_max &lt;- 7\n\nkmeans_results &lt;- tibble(k = k_min:k_max) |&gt;\n  mutate(\n    kclust = map(k, ~kmeans(respondents, centers = .x, iter.max = iter_max)),\n  )\n\nA common heuristic to determine the optimal number of clusters is the elbow method where we plot the within-cluster sum of squared errors of an algorithm for increasing number of clusters. The optimal number of clusters corresponds to the point where adding another cluster does lead to much of an improvement anymore. In economic terms, we look for the point where the diminishing returns to an additional cluster are not worth the additional cost (assuming that we want the minimum number of clusters with optimal predictive power).\nThe function below computes the within-cluster sum of squares for any cluster assignments.\n\ncompute_withinss &lt;- function(model, k = NULL) {\n  \n  clusters &lt;- assign_clusters(model = model, k = k)\n  \n  centers &lt;- clusters |&gt;\n    group_by(cluster) |&gt;\n    summarize_all(mean) |&gt;\n    pivot_longer(cols = -cluster, names_to = \"question\", values_to = \"cluster_mean\")\n  \n  withinss &lt;- clusters |&gt;\n    pivot_longer(cols = -cluster, names_to = \"question\", values_to = \"response\") |&gt;\n    left_join(centers, by = c(\"cluster\", \"question\")) |&gt;\n    summarize(k = max(cluster),\n              withinss = sum((response - cluster_mean)^2)) |&gt;\n    mutate(model = class(model)[1])\n  \n  return(withinss)\n}\n\nWe can simply map the function across our list of \\(K\\)-means models. For better comparability, we normalize the within-cluster sum of squares for any number of cluster by the benchmark case of only having a single cluster. Moreover, we consider log-differences to because we care more about the percentage decrease in sum of squares rather than the absolute number.\n\nkmeans_logwithindiss &lt;- kmeans_results$kclust |&gt;\n  map(compute_withinss) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#k-modes-clustering",
    "href": "posts/clustering-binary-data/index.html#k-modes-clustering",
    "title": "Clustering Binary Data",
    "section": "\\(K\\)-modes clustering",
    "text": "\\(K\\)-modes clustering\nSince \\(K\\)-means is actually not ideal for binary (or hierarchical data in general), Huang (1997) came up with the \\(K\\)-modes algorithm. This clustering approach aims to partition respondents into \\(K\\) groups such that the distance from respondents to the assigned cluster modes is minimized. A mode is a vector of elements that minimize the dissimilarities between the vector and each object of the data. Rather than using the Euclidean distance, \\(K\\)-modes uses simple matching distance between respondents to quantify dissimilarity which translates into counting the number of mismatches in all question responses in the current setting.\nFortunately, the klaR package provides an implementation of the \\(K\\)-modes algorithm that we can apply just like the \\(K\\)-means above.\n\nkmodes_example &lt;- klaR::kmodes(respondents, iter.max = iter_max, modes = 3) |&gt;\n  summarize_clusters()\n\nSimilarly, we just map the model across different numbers of target cluster modes and compute the within-cluster sum of squares.\n\nkmodes_results &lt;- tibble(k = k_min:k_max) |&gt;\n  mutate(\n    kclust = map(k, ~klaR::kmodes(respondents, modes = ., iter.max = iter_max))\n  )\n\nkmodes_logwithindiss &lt;- kmodes_results$kclust |&gt;\n  map(compute_withinss) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))\n\nNote that we computed the within-cluster sum of squared errors rather than using the within-cluster simple-matching distance provided by the function itself. The latter counts the number of differences from assigned respondents to their cluster modes."
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#hierarchical-clustering",
    "href": "posts/clustering-binary-data/index.html#hierarchical-clustering",
    "title": "Clustering Binary Data",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nAs an alternative to computing optimal assignments for a given number of clusters, we might sometimes prefer to arrange the clusters into a natural hierarchy. This involves successively grouping the clusters themselves such that at each level of the hierarchy, clusters within the same group are more similar to each other than those in different groups. There are two fundamentally different approaches to hierarchical clustering that are fortunately implemented in the great cluster package.\nBoth hierarchical clustering approaches require a dissimilarity or distance matrix. Since we have binary data, we choose the asymmetric binary distance matrix based on the Jaccard distance. Intuitively, the Jaccard distance measures how far the overlap of responses between two groups is from perfect overlap.\n\ndissimilarity_matrix &lt;- stats::dist(respondents, method = \"binary\")\n\nAgglomerative clustering start at the bottom and at each level recursively merge a selected pair of clusters into a single cluster. This produces a clustering at the next higher level with one less cluster. The pair chosen for merging consist of the two clusters with the smallest within-cluster dissimilarity. On an intuitive level, agglomerative clustering is hence better in discovering small clusters.\nThe cluster package provides the agnes algorithm (AGglomerative NESting) that can easily applied to the dissimilarity matrix.\n\nagnes_results &lt;- cluster::agnes(\n  dissimilarity_matrix, diss = TRUE, keep.diss = TRUE, method = \"complete\"\n)\n\nThe function returns a clustering tree that we could plot (which actually is rarely really helpful) or cut into different partitions using the stats::cutree function. This is why the helper functions from above need a number of target clusters as an input for hierarchical clustering models. However, the logic of the summary statistics are just as above.\n\nagnes_example &lt;- summarize_clusters(agnes_results, k = 3)\n\nagnes_logwithindiss &lt;- k_min:k_max |&gt;\n  map(~compute_withinss(agnes_results, .)) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))\n\nDivisive methods start at the top and at each level recursively split one of the existing clusters at that level into two new clusters. The split is chosen such that two new groups with the largest between-group dissimilarity emerge. Intuitively speaking, divisive clustering is thus better in discovering large clusters.\nThe cluster package provides the diana algorithm (DIvise ANAlysis) for this clustering approach where the logic is basically the same as for the agnes model.\n\ndiana_results &lt;- cluster::diana(\n  dissimilarity_matrix, diss = TRUE, keep.diss = TRUE\n) \n\ndiana_example &lt;- diana_results |&gt;\n  summarize_clusters(k = 3)\n\ndiana_logwithindiss &lt;-  k_min:k_max |&gt;\n  map(~compute_withinss(diana_results, .)) |&gt;\n  reduce(bind_rows) |&gt;\n  mutate(logwithindiss = log(withinss) - log(withinss[k == 1]))"
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#model-comparison",
    "href": "posts/clustering-binary-data/index.html#model-comparison",
    "title": "Clustering Binary Data",
    "section": "Model comparison",
    "text": "Model comparison\nLet us start the model comparison by looking at the within cluster sum of squares for different numbers of clusters. The figure shows that the \\(K\\)-modes algorithm improves the fastest towards the true number of 3 clusters. The elbow method would suggest in this case to stick with 3 clusters for this algorithm. Similarly, for the \\(K\\)-means model. The hierarchical clustering models do not seem to support 3 clusters.\n\nbind_rows(kmeans_logwithindiss, kmodes_logwithindiss,\n          agnes_logwithindiss, diana_logwithindiss) |&gt;\n  ggplot(aes(x = k, y = logwithindiss, color = model, linetype = model)) +\n  geom_line() +\n  scale_x_continuous(breaks = k_min:k_max) + \n  theme_minimal() +\n  labs(x = \"Number of Clusters\", y = bquote(log(W[k])-log(W[1])), \n       color = \"Model\", linetype = \"Model\",\n       title = \"Within cluster sum of squares relative to benchmark case of one cluster\")\n\n\n\n\n\n\n\n\nNow, let us compare the proportion of positive responses within assigned clusters across models. Recall that we ranked clusters according to the total share of positive answers to ensure comparability. This approach is only possible in this type of setting where we can easily introduce such a ranking. The figure suggests that \\(K\\)-modes performs best for the current setting as it identifies the correct responses for each cluster.\n\nbind_rows(\n  kmeans_example, kmodes_example,\n  agnes_example, diana_example) |&gt;\n  select(-c(total, assigned_respondents)) |&gt;\n  pivot_longer(cols = -c(k, model), \n               names_to = \"question\", values_to = \"response\") |&gt;\n  mutate(cluster = paste0(\"Cluster \", k)) |&gt;\n  ggplot(aes(x = response, y = question, fill = model)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~cluster) +\n  theme_bw() +\n  scale_x_continuous(labels = scales::percent) + \n  geom_hline(yintercept = seq(1.5, length(unique(colnames(respondents))) - 0.5, 1),\n             colour = 'black') +\n  labs(x = \"Proportion of responses\", y = \"Question\", fill = \"Model\",\n       title = \"Proportion of positive responses within assigned clusters\")\n\n\n\n\n\n\n\n\nFinally, let us check how well each model assigns respondents to the true cluster which is obviously not possible in real unsupervised applications. The figure below shows the true number of respondents by cluster as a dashed box and the assigned respondents as bars. The figure shows that \\(K\\)-modes is the only model that is able to consistently assign respondents to their correct cluster.\n\nbind_rows(\n  kmeans_example, kmodes_example,\n  agnes_example, diana_example) |&gt;\n  mutate(cluster = paste0(\"Cluster \", k)) |&gt;\n  select(model, cluster, assigned_respondents) |&gt;\n  ggplot() +\n  geom_col(position = \"dodge\", \n           aes(y = assigned_respondents, x = cluster, fill = model)) +\n  geom_col(data = labelled_respondents |&gt;\n             group_by(cluster = paste0(\"Cluster \", cluster)) |&gt;\n             summarize(assigned_respondents = n(),\n                       model = \"actual\"),\n           aes(y = assigned_respondents, x = cluster), \n           fill = \"white\", color = \"black\", alpha = 0, linetype = \"dashed\") +\n  theme_bw() +\n  labs(x = NULL, y = \"Number of assigned respondents\", fill = \"Model\",\n       title = \"Number of assigned respondents by cluster\",\n       subtitle = \"Dashed box indicates true number of respondents by cluster\")\n\n\n\n\n\n\n\n\nLet me end this post with a few words of caution: first, the ultimate outcome heavily depends on the seed chosen at the beginning of the post. The results might be quite different for other draws of respondents or initial conditions for clustering algorithms. Second, there are many more models out there that can be applied to the current setting. However, with this post I want to emphasize that it is important to consider different models at the same time and to compare them through a consistent set of measures. Ultimately, choosing the optimal number of clusters in practice requires a judgment call, but at least it can be informed as much as possible."
  },
  {
    "objectID": "posts/clustering-binary-data/index.html#footnotes",
    "href": "posts/clustering-binary-data/index.html#footnotes",
    "title": "Clustering Binary Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs of writing, the tidyclust package only has limited support for hierarchical clustering, so I decided to abstain from using it for this post.↩︎"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html",
    "href": "posts/dplyr-vs-pandas/index.html",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "",
    "text": "There are a myriad of options to perform essential data manipulation tasks in R and Python (see, for instance, my other posts on dplyr vs ibis and dplyr vs polars). However, if we want to do tidy data science in R, there is a clear forerunner: dplyr. In the world of Python, pandas is the most popular data analysis library. In this blog post, I illustrate their syntactic similarities and highlight differences between these two packages that emerge for a few key tasks.\nBefore we dive into the comparison, a short introduction to the packages: the dplyr package in R allows users to refer to columns without quotation marks due to its implementation of non-standard evaluation (NSE). NSE is a programming technique used in R that allows functions to capture the expressions passed to them as arguments, rather than just the values of those arguments. The primary goal of NSE in the context of dplyr is to create a more user-friendly and intuitive syntax. This makes data manipulation tasks more straightforward and aligns with the general philosophy of the tidyverse to make data science faster, easier, and more fun.1\npandas is also designed for data analysis and provides a comprehensive range of functionalities for data manipulation and it is designed to efficiently handle in-memory data. The package has a large community, given Python’s popularity in various fields. The learning curve might be steeper for beginners due to Python’s general-purpose nature and the verbosity of pandas syntax, but it integrates well with web apps, machine learning models, etc."
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#filter-rows",
    "href": "posts/dplyr-vs-pandas/index.html#filter-rows",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Filter rows",
    "text": "Filter rows\nFiltering rows with dplyr is based on NSE and the dplyr::filter() function. To replicate the same results with pandas, you can use pandas.query() method which accepts a string with the filter conditions as input.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  filter(species == \"Adelie\" & \n           island %in% c(\"Biscoe\", \"Dream\"))\n\n# A tibble: 100 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 90 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .query(\"species == 'Adelie' and island in ['Biscoe', 'Dream']\")\n)\n\n    species  island  bill_length_mm  ...  body_mass_g     sex  year\n20   Adelie  Biscoe            37.8  ...       3400.0  female  2007\n21   Adelie  Biscoe            37.7  ...       3600.0    male  2007\n22   Adelie  Biscoe            35.9  ...       3800.0  female  2007\n23   Adelie  Biscoe            38.2  ...       3950.0    male  2007\n24   Adelie  Biscoe            38.8  ...       3800.0    male  2007\n..      ...     ...             ...  ...          ...     ...   ...\n147  Adelie   Dream            36.6  ...       3475.0  female  2009\n148  Adelie   Dream            36.0  ...       3450.0  female  2009\n149  Adelie   Dream            37.8  ...       3750.0    male  2009\n150  Adelie   Dream            36.0  ...       3700.0  female  2009\n151  Adelie   Dream            41.5  ...       4000.0    male  2009\n\n[100 rows x 8 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#slice-rows",
    "href": "posts/dplyr-vs-pandas/index.html#slice-rows",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Slice rows",
    "text": "Slice rows\ndplyr::slice() takes integers with row numbers as inputs, so you can use ranges and arbitrary vectors of integers. pandas.iloc[] also provides a function for integer-location based indexing (note that indexing starts at 0 in Python, while it starts at 1 in R). Note that pandas.iloc[] requires square brackets instead of parentheses.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  slice(10:20)\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           42            20.2               190        4250\n 2 Adelie  Torgersen           37.8          17.1               186        3300\n 3 Adelie  Torgersen           37.8          17.3               180        3700\n 4 Adelie  Torgersen           41.1          17.6               182        3200\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           36.6          17.8               185        3700\n 8 Adelie  Torgersen           38.7          19                 195        3450\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           34.4          18.4               184        3325\n11 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .iloc[9:20]\n)\n\n   species     island  bill_length_mm  ...  body_mass_g     sex  year\n9   Adelie  Torgersen            42.0  ...       4250.0     NaN  2007\n10  Adelie  Torgersen            37.8  ...       3300.0     NaN  2007\n11  Adelie  Torgersen            37.8  ...       3700.0     NaN  2007\n12  Adelie  Torgersen            41.1  ...       3200.0  female  2007\n13  Adelie  Torgersen            38.6  ...       3800.0    male  2007\n14  Adelie  Torgersen            34.6  ...       4400.0    male  2007\n15  Adelie  Torgersen            36.6  ...       3700.0  female  2007\n16  Adelie  Torgersen            38.7  ...       3450.0  female  2007\n17  Adelie  Torgersen            42.5  ...       4500.0    male  2007\n18  Adelie  Torgersen            34.4  ...       3325.0  female  2007\n19  Adelie  Torgersen            46.0  ...       4200.0    male  2007\n\n[11 rows x 8 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#arrange-rows",
    "href": "posts/dplyr-vs-pandas/index.html#arrange-rows",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Arrange rows",
    "text": "Arrange rows\nTo orders the rows of a data frame by the values of selected columns, we have dplyr::arrange() and pandas.sort_values(). Note that both approaches arrange rows in an an ascending order and puts missing values last as defaults.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  arrange(island, desc(bill_length_mm))\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .sort_values(by = [\"island\", \"bill_length_mm\"], \n               ascending = [True, False])\n)\n\n    species     island  bill_length_mm  ...  body_mass_g     sex  year\n185  Gentoo     Biscoe            59.6  ...       6050.0    male  2007\n253  Gentoo     Biscoe            55.9  ...       5600.0    male  2009\n267  Gentoo     Biscoe            55.1  ...       5850.0    male  2009\n215  Gentoo     Biscoe            54.3  ...       5650.0    male  2008\n259  Gentoo     Biscoe            53.4  ...       5500.0    male  2009\n..      ...        ...             ...  ...          ...     ...   ...\n80   Adelie  Torgersen            34.6  ...       3200.0  female  2008\n18   Adelie  Torgersen            34.4  ...       3325.0  female  2007\n8    Adelie  Torgersen            34.1  ...       3475.0     NaN  2007\n70   Adelie  Torgersen            33.5  ...       3600.0  female  2008\n3    Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n\n[344 rows x 8 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#select-columns",
    "href": "posts/dplyr-vs-pandas/index.html#select-columns",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Select columns",
    "text": "Select columns\nSelecting a subset of columns works very similarly withdplyr::select() and pandas.get(). The former accepts column names using NSE (or vectors of charaters), while the latter requires a vector of strings with column names as inputs.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  select(bill_length_mm, sex)\n\n# A tibble: 344 × 2\n   bill_length_mm sex   \n            &lt;dbl&gt; &lt;fct&gt; \n 1           39.1 male  \n 2           39.5 female\n 3           40.3 female\n 4           NA   &lt;NA&gt;  \n 5           36.7 female\n 6           39.3 male  \n 7           38.9 female\n 8           39.2 male  \n 9           34.1 &lt;NA&gt;  \n10           42   &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n\n\n(penguins\n  .get([\"bill_length_mm\", \"sex\"])\n)\n\n     bill_length_mm     sex\n0              39.1    male\n1              39.5  female\n2              40.3  female\n3               NaN     NaN\n4              36.7  female\n..              ...     ...\n339            55.8    male\n340            43.5  female\n341            49.6    male\n342            50.8    male\n343            50.2  female\n\n[344 rows x 2 columns]\n\n\n\n\n\n\nR columns\nRenaming columns also works very similarly with the major difference that pandas.rename() takes a dictionary with mappings of old to new names as input, while dplyr::rename() takes variable names via the usual NSE.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  rename(bill_length = bill_length_mm,\n         bill_depth = bill_depth_mm)\n\n# A tibble: 344 × 8\n   species island    bill_length bill_depth flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen        39.1       18.7               181        3750 male  \n 2 Adelie  Torgersen        39.5       17.4               186        3800 female\n 3 Adelie  Torgersen        40.3       18                 195        3250 female\n 4 Adelie  Torgersen        NA         NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen        36.7       19.3               193        3450 female\n 6 Adelie  Torgersen        39.3       20.6               190        3650 male  \n 7 Adelie  Torgersen        38.9       17.8               181        3625 female\n 8 Adelie  Torgersen        39.2       19.6               195        4675 male  \n 9 Adelie  Torgersen        34.1       18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen        42         20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .rename(columns = {\"bill_length_mm\": \"bill_length\",\n                     \"bill_depth_mm\" : \"bill_depth\"})\n)\n\n       species     island  bill_length  ...  body_mass_g     sex  year\n0       Adelie  Torgersen         39.1  ...       3750.0    male  2007\n1       Adelie  Torgersen         39.5  ...       3800.0  female  2007\n2       Adelie  Torgersen         40.3  ...       3250.0  female  2007\n3       Adelie  Torgersen          NaN  ...          NaN     NaN  2007\n4       Adelie  Torgersen         36.7  ...       3450.0  female  2007\n..         ...        ...          ...  ...          ...     ...   ...\n339  Chinstrap      Dream         55.8  ...       4000.0    male  2009\n340  Chinstrap      Dream         43.5  ...       3400.0  female  2009\n341  Chinstrap      Dream         49.6  ...       3775.0    male  2009\n342  Chinstrap      Dream         50.8  ...       4100.0    male  2009\n343  Chinstrap      Dream         50.2  ...       3775.0  female  2009\n\n[344 rows x 8 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#mutate-columns",
    "href": "posts/dplyr-vs-pandas/index.html#mutate-columns",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Mutate columns",
    "text": "Mutate columns\nTransforming existing columns or creating new ones is an essential part of data analysis. dplyr::mutate() and pandas.assign() are the work horses for these tasks. While dplyr starts with column names before the expressions that transform columns, pandas uses the lambda function to assign expressions to new columns. Note that you have to split up variable assignments if you want to refer to a newly created variable in pandas, while you can refer to the new variables in the same mutate block in dplyr.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  mutate(ones = 1,\n         bill_length = bill_length_mm / 10,\n         bill_length_squared = bill_length^2) |&gt; \n  select(ones, bill_length_mm, bill_length, bill_length_squared)\n\n# A tibble: 344 × 4\n    ones bill_length_mm bill_length bill_length_squared\n   &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1     1           39.1        3.91                15.3\n 2     1           39.5        3.95                15.6\n 3     1           40.3        4.03                16.2\n 4     1           NA         NA                   NA  \n 5     1           36.7        3.67                13.5\n 6     1           39.3        3.93                15.4\n 7     1           38.9        3.89                15.1\n 8     1           39.2        3.92                15.4\n 9     1           34.1        3.41                11.6\n10     1           42          4.2                 17.6\n# ℹ 334 more rows\n\n\n\n\n\n(penguins \n  .assign(ones = 1,\n          bill_length = lambda x: x[\"bill_length_mm\"] / 10)\n  .assign(bill_length_squared = lambda x: x[\"bill_length\"] ** 2)\n  .get([\"ones\", \"bill_length_mm\", \"bill_length\", \"bill_length_squared\"])\n)\n\n     ones  bill_length_mm  bill_length  bill_length_squared\n0       1            39.1         3.91              15.2881\n1       1            39.5         3.95              15.6025\n2       1            40.3         4.03              16.2409\n3       1             NaN          NaN                  NaN\n4       1            36.7         3.67              13.4689\n..    ...             ...          ...                  ...\n339     1            55.8         5.58              31.1364\n340     1            43.5         4.35              18.9225\n341     1            49.6         4.96              24.6016\n342     1            50.8         5.08              25.8064\n343     1            50.2         5.02              25.2004\n\n[344 rows x 4 columns]\n\n\n\n\n\n\ndplyrelocate columns\ndplyr::relocate() provides options to change the positions of columns in a data frame, using the same syntax as dplyr::select(). In addition, there are the options .after and .before to provide users with additional shortcuts.\nThe recommended way to relocate columns in pandas is to use the pandas.get() method, but there are no options as in dplyr::relocate(). In fact, the safest way to consistently get the correct order of columns is to explicitly specify them.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  relocate(c(species, bill_length_mm), .before = sex)\n\n# A tibble: 344 × 8\n   island    bill_depth_mm flipper_length_mm body_mass_g species bill_length_mm\n   &lt;fct&gt;             &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1 Torgersen          18.7               181        3750 Adelie            39.1\n 2 Torgersen          17.4               186        3800 Adelie            39.5\n 3 Torgersen          18                 195        3250 Adelie            40.3\n 4 Torgersen          NA                  NA          NA Adelie            NA  \n 5 Torgersen          19.3               193        3450 Adelie            36.7\n 6 Torgersen          20.6               190        3650 Adelie            39.3\n 7 Torgersen          17.8               181        3625 Adelie            38.9\n 8 Torgersen          19.6               195        4675 Adelie            39.2\n 9 Torgersen          18.1               193        3475 Adelie            34.1\n10 Torgersen          20.2               190        4250 Adelie            42  \n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .get([\"island\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\",\n        \"species\", \"bill_length_mm\", \"sex\"])\n)\n\n        island  bill_depth_mm  ...  bill_length_mm     sex\n0    Torgersen           18.7  ...            39.1    male\n1    Torgersen           17.4  ...            39.5  female\n2    Torgersen           18.0  ...            40.3  female\n3    Torgersen            NaN  ...             NaN     NaN\n4    Torgersen           19.3  ...            36.7  female\n..         ...            ...  ...             ...     ...\n339      Dream           19.8  ...            55.8    male\n340      Dream           18.1  ...            43.5  female\n341      Dream           18.2  ...            49.6    male\n342      Dream           19.0  ...            50.8    male\n343      Dream           18.7  ...            50.2  female\n\n[344 rows x 7 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#simple-summaries-by-group",
    "href": "posts/dplyr-vs-pandas/index.html#simple-summaries-by-group",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Simple summaries by group",
    "text": "Simple summaries by group\nLet’s suppose we want to compute summaries by groups such as means or medians. Both packages are very similar again: on the R side you have dplyr::group_by() and dplyr::summarize(), while on the Python side you have pandas.groupby() and pandas.agg().\nNote that dplyr::groupby() also automatically arranges the results by the group, so the reproduce the results of dplyr, we need to add pandas.sort() to the chain.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  island    bill_depth_mean\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Biscoe               15.9\n2 Dream                18.3\n3 Torgersen            18.4\n\n\n\n\n\n(penguins\n  .groupby(\"island\")\n  .agg(bill_depth_mean = (\"bill_depth_mm\", \"mean\"))\n)\n\n           bill_depth_mean\nisland                    \nBiscoe           15.874850\nDream            18.344355\nTorgersen        18.429412"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#more-complicated-summaries-by-group",
    "href": "posts/dplyr-vs-pandas/index.html#more-complicated-summaries-by-group",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "More complicated summaries by group",
    "text": "More complicated summaries by group\nTypically, you want to create multiple different summaries by groups. dplyr provides a lot of flexibility to create new variables on the fly, as does pandas. For instance, we can pass expressions to them mean functions in order to create the share of female penguins per island in the summary statement. Note that you again have to use lambda functions in pandas.\n\ndplyrpandas\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(count = n(),\n            bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE),\n            flipper_length_median = median(flipper_length_mm, na.rm = TRUE),\n            body_mass_sd = sd(body_mass_g, na.rm = TRUE),\n            share_female = mean(sex == \"female\", na.rm = TRUE))\n\n# A tibble: 3 × 6\n  island   count bill_depth_mean flipper_length_median body_mass_sd share_female\n  &lt;fct&gt;    &lt;int&gt;           &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Biscoe     168            15.9                   214         783.        0.491\n2 Dream      124            18.3                   193         417.        0.496\n3 Torgers…    52            18.4                   191         445.        0.511\n\n\n\n\n\n(penguins\n  .groupby(\"island\")\n  .agg(count = (\"island\", \"size\"),\n       bill_depth_mean = (\"bill_depth_mm\", \"mean\"),\n       flipper_length_median = (\"flipper_length_mm\", \"median\"),\n       body_mass_sd = (\"body_mass_g\", \"std\"),\n       share_female = (\"sex\", lambda x: (x == \"female\").mean))\n)\n\n           count  ...                                       share_female\nisland            ...                                                   \nBiscoe       168  ...  &lt;bound method Series.mean of 20      True\\n21 ...\nDream        124  ...  &lt;bound method Series.mean of 30      True\\n31 ...\nTorgersen     52  ...  &lt;bound method Series.mean of 0      False\\n1  ...\n\n[3 rows x 5 columns]"
  },
  {
    "objectID": "posts/dplyr-vs-pandas/index.html#footnotes",
    "href": "posts/dplyr-vs-pandas/index.html#footnotes",
    "title": "Tidy Data Manipulation: dplyr vs pandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the unifying principles of the tidyverse: https://design.tidyverse.org/unifying.html.↩︎"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html",
    "href": "posts/dplyr-vs-polars/index.html",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "",
    "text": "There are a myriad of options to perform essential data manipulation tasks in R and Python (see, for instance, my other posts on dplyr vs ibis and dplyr vs pandas). However, if we want to do tidy data science in R, there is a clear forerunner: dplyr. In the world of Python, polars is a relatively new kid on the block that shares a lot of semantic with dplyr. In this blog post, I illustrate their syntactic similarities and highlight differences between these two packages that emerge for a few key tasks.\nBefore we dive into the comparison, a short introduction to the packages: the dplyr package in R allows users to refer to columns without quotation marks due to its implementation of non-standard evaluation (NSE). NSE is a programming technique used in R that allows functions to capture the expressions passed to them as arguments, rather than just the values of those arguments. The primary goal of NSE in the context of dplyr is to create a more user-friendly and intuitive syntax. This makes data manipulation tasks more straightforward and aligns with the general philosophy of the tidyverse to make data science faster, easier, and more fun.1\npolars is also designed for data manipulation and heavily optimized for performance, but there are significant differences in their approach, especially in how they handle column referencing and expression evaluation. Python generally relies on standard evaluation, meaning expressions are evaluated to their values before being passed to a function. In polars, column references typically need to be explicitly stated, often using quoted names or through methods attached to data frame objects."
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#filter-rows",
    "href": "posts/dplyr-vs-polars/index.html#filter-rows",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Filter rows",
    "text": "Filter rows\nFiltering rows works very similarly for both packages, they even have the same function names: dplyr::filter() and polars.filter(). To select columns in polars, you need the polars.col() selector.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  filter(species == \"Adelie\" & \n           island %in% c(\"Biscoe\", \"Dream\"))\n\n# A tibble: 100 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 90 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .filter(\n    (pl.col(\"species\") == \"Adelie\") & \n    (pl.col(\"island\").is_in([\"Biscoe\", \"Dream\"]))) \n)\n\n\nshape: (100, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Adelie\"\n\"Biscoe\"\n37.8\n18.3\n174.0\n3400.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Biscoe\"\n37.7\n18.7\n180.0\n3600.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Biscoe\"\n35.9\n19.2\n189.0\n3800.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Biscoe\"\n38.2\n18.1\n185.0\n3950.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Biscoe\"\n38.8\n17.2\n180.0\n3800.0\n\"male\"\n2007\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Adelie\"\n\"Dream\"\n36.6\n18.4\n184.0\n3475.0\n\"female\"\n2009\n\n\n\"Adelie\"\n\"Dream\"\n36.0\n17.8\n195.0\n3450.0\n\"female\"\n2009\n\n\n\"Adelie\"\n\"Dream\"\n37.8\n18.1\n193.0\n3750.0\n\"male\"\n2009\n\n\n\"Adelie\"\n\"Dream\"\n36.0\n17.1\n187.0\n3700.0\n\"female\"\n2009\n\n\n\"Adelie\"\n\"Dream\"\n41.5\n18.5\n201.0\n4000.0\n\"male\"\n2009"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#slice-rows",
    "href": "posts/dplyr-vs-polars/index.html#slice-rows",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Slice rows",
    "text": "Slice rows\ndplyr::slice() takes integers with row numbers as inputs, so you can use ranges and arbitrary vectors of integers. polars.slice() only takes the start index and the length of the slice as inputs. For instance, to the the same result of slicing rows 10 to 20, the code looks as follows (note that indexing starts at 0 in Python, while it starts at 1 in R):\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  slice(10:20)\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           42            20.2               190        4250\n 2 Adelie  Torgersen           37.8          17.1               186        3300\n 3 Adelie  Torgersen           37.8          17.3               180        3700\n 4 Adelie  Torgersen           41.1          17.6               182        3200\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           36.6          17.8               185        3700\n 8 Adelie  Torgersen           38.7          19                 195        3450\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           34.4          18.4               184        3325\n11 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .slice(9, 11)  \n)\n\n\nshape: (11, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Adelie\"\n\"Torgersen\"\n42.0\n20.2\n190.0\n4250.0\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n37.8\n17.1\n186.0\n3300.0\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n37.8\n17.3\n180.0\n3700.0\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n41.1\n17.6\n182.0\n3200.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n38.6\n21.2\n191.0\n3800.0\n\"male\"\n2007\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Adelie\"\n\"Torgersen\"\n36.6\n17.8\n185.0\n3700.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n38.7\n19.0\n195.0\n3450.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n42.5\n20.7\n197.0\n4500.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n34.4\n18.4\n184.0\n3325.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n46.0\n21.5\n194.0\n4200.0\n\"male\"\n2007"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#arrange-rows",
    "href": "posts/dplyr-vs-polars/index.html#arrange-rows",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Arrange rows",
    "text": "Arrange rows\nTo orders the rows of a data frame by the values of selected columns, we have dplyr::arrange() and polars.sort(). Note that dplyr::arrange() arranges rows in an an ascending order and puts NA values last. polars.sort(), on the other hand, arranges rows in an ascending order and starts with null as default. Note that there are options to control these defaults.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  arrange(island, desc(bill_length_mm))\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .sort([\"island\", \"bill_length_mm\"], \n        descending=[False, True], nulls_last=True)\n)\n\n\nshape: (344, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Gentoo\"\n\"Biscoe\"\n59.6\n17.0\n230.0\n6050.0\n\"male\"\n2007\n\n\n\"Gentoo\"\n\"Biscoe\"\n55.9\n17.0\n228.0\n5600.0\n\"male\"\n2009\n\n\n\"Gentoo\"\n\"Biscoe\"\n55.1\n16.0\n230.0\n5850.0\n\"male\"\n2009\n\n\n\"Gentoo\"\n\"Biscoe\"\n54.3\n15.7\n231.0\n5650.0\n\"male\"\n2008\n\n\n\"Gentoo\"\n\"Biscoe\"\n53.4\n15.8\n219.0\n5500.0\n\"male\"\n2009\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Adelie\"\n\"Torgersen\"\n34.6\n17.2\n189.0\n3200.0\n\"female\"\n2008\n\n\n\"Adelie\"\n\"Torgersen\"\n34.4\n18.4\n184.0\n3325.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n34.1\n18.1\n193.0\n3475.0\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n33.5\n19.0\n190.0\n3600.0\n\"female\"\n2008\n\n\n\"Adelie\"\n\"Torgersen\"\nnull\nnull\nnull\nnull\nnull\n2007"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#select-columns",
    "href": "posts/dplyr-vs-polars/index.html#select-columns",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Select columns",
    "text": "Select columns\nSelecting a subset of columns works essentially the same for both and dplyr::select() and polars.select() even have the same name. Note that you don’t have to use polars.col() but can just pass strings in the polars.select() method.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  select(bill_length_mm, sex)\n\n# A tibble: 344 × 2\n   bill_length_mm sex   \n            &lt;dbl&gt; &lt;fct&gt; \n 1           39.1 male  \n 2           39.5 female\n 3           40.3 female\n 4           NA   &lt;NA&gt;  \n 5           36.7 female\n 6           39.3 male  \n 7           38.9 female\n 8           39.2 male  \n 9           34.1 &lt;NA&gt;  \n10           42   &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n\n\n(penguins\n  .select(pl.col(\"bill_length_mm\"), pl.col(\"sex\"))\n)\n\n\nshape: (344, 2)\n\n\n\nbill_length_mm\nsex\n\n\nf64\nstr\n\n\n\n\n39.1\n\"male\"\n\n\n39.5\n\"female\"\n\n\n40.3\n\"female\"\n\n\nnull\nnull\n\n\n36.7\n\"female\"\n\n\n…\n…\n\n\n55.8\n\"male\"\n\n\n43.5\n\"female\"\n\n\n49.6\n\"male\"\n\n\n50.8\n\"male\"\n\n\n50.2\n\"female\"\n\n\n\n\n\n\n\n\n\n\nRename columns\nRenaming columns also works very similarly with the major difference that polars.rename() takes a dictionary with mappings of old to new names as input, while dplyr::rename() takes variable names via the usual NSE.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  rename(bill_length = bill_length_mm,\n         bill_depth = bill_depth_mm)\n\n# A tibble: 344 × 8\n   species island    bill_length bill_depth flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen        39.1       18.7               181        3750 male  \n 2 Adelie  Torgersen        39.5       17.4               186        3800 female\n 3 Adelie  Torgersen        40.3       18                 195        3250 female\n 4 Adelie  Torgersen        NA         NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen        36.7       19.3               193        3450 female\n 6 Adelie  Torgersen        39.3       20.6               190        3650 male  \n 7 Adelie  Torgersen        38.9       17.8               181        3625 female\n 8 Adelie  Torgersen        39.2       19.6               195        4675 male  \n 9 Adelie  Torgersen        34.1       18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen        42         20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .rename({\"bill_length_mm\": \"bill_length\",\n           \"bill_depth_mm\" : \"bill_depth\"})\n)\n\n\nshape: (344, 8)\n\n\n\nspecies\nisland\nbill_length\nbill_depth\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Adelie\"\n\"Torgersen\"\n39.1\n18.7\n181.0\n3750.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n39.5\n17.4\n186.0\n3800.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n40.3\n18.0\n195.0\n3250.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\nnull\nnull\nnull\nnull\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n36.7\n19.3\n193.0\n3450.0\n\"female\"\n2007\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Chinstrap\"\n\"Dream\"\n55.8\n19.8\n207.0\n4000.0\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n43.5\n18.1\n202.0\n3400.0\n\"female\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n49.6\n18.2\n193.0\n3775.0\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.8\n19.0\n210.0\n4100.0\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.2\n18.7\n198.0\n3775.0\n\"female\"\n2009"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#mutate-columns",
    "href": "posts/dplyr-vs-polars/index.html#mutate-columns",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Mutate columns",
    "text": "Mutate columns\nTransforming existing columns or creating new ones is an essential part of data analysis. dplyr::mutate() and polars.with_columns() are the work horses for these tasks. Both approaches have a very similar syntax. Note that you have to split up variable assignments if you want to refer to a newly created variable in polars, while you can refer to the new variables in the same mutate block in dplyr.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  mutate(ones = 1,\n         bill_length = bill_length_mm / 10,\n         bill_length_squared = bill_length^2) |&gt; \n  select(ones, bill_length_mm, bill_length, bill_length_squared)\n\n# A tibble: 344 × 4\n    ones bill_length_mm bill_length bill_length_squared\n   &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1     1           39.1        3.91                15.3\n 2     1           39.5        3.95                15.6\n 3     1           40.3        4.03                16.2\n 4     1           NA         NA                   NA  \n 5     1           36.7        3.67                13.5\n 6     1           39.3        3.93                15.4\n 7     1           38.9        3.89                15.1\n 8     1           39.2        3.92                15.4\n 9     1           34.1        3.41                11.6\n10     1           42          4.2                 17.6\n# ℹ 334 more rows\n\n\n\n\n\n(penguins \n  .with_columns(ones = pl.lit(1),\n                bill_length = pl.col(\"bill_length_mm\") / 10)\n  .with_columns(bill_length_squared = pl.col(\"bill_length\") ** 2)\n  .select(pl.col(\"ones\"), pl.col(\"bill_length_mm\"),  \n          pl.col(\"bill_length\"), pl.col(\"bill_length_squared\"))\n)\n\n\nshape: (344, 4)\n\n\n\nones\nbill_length_mm\nbill_length\nbill_length_squared\n\n\ni32\nf64\nf64\nf64\n\n\n\n\n1\n39.1\n3.91\n15.2881\n\n\n1\n39.5\n3.95\n15.6025\n\n\n1\n40.3\n4.03\n16.2409\n\n\n1\nnull\nnull\nnull\n\n\n1\n36.7\n3.67\n13.4689\n\n\n…\n…\n…\n…\n\n\n1\n55.8\n5.58\n31.1364\n\n\n1\n43.5\n4.35\n18.9225\n\n\n1\n49.6\n4.96\n24.6016\n\n\n1\n50.8\n5.08\n25.8064\n\n\n1\n50.2\n5.02\n25.2004\n\n\n\n\n\n\n\n\n\n\ndplyrelocate columns\ndplyr::relocate() provides options to change the positions of columns in a data frame, using the same syntax as dplyr::select(). In addition, there are the options .after and .before to provide users with additional shortcuts.\nThe recommended way to relocate columns in polars is to use the polars.select() method, but there are no options as in dplyr::relocate(). In fact, the safest way to consistently get the correct order of columns is to explicitly specify them.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  relocate(c(species, bill_length_mm), .before = sex)\n\n# A tibble: 344 × 8\n   island    bill_depth_mm flipper_length_mm body_mass_g species bill_length_mm\n   &lt;fct&gt;             &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1 Torgersen          18.7               181        3750 Adelie            39.1\n 2 Torgersen          17.4               186        3800 Adelie            39.5\n 3 Torgersen          18                 195        3250 Adelie            40.3\n 4 Torgersen          NA                  NA          NA Adelie            NA  \n 5 Torgersen          19.3               193        3450 Adelie            36.7\n 6 Torgersen          20.6               190        3650 Adelie            39.3\n 7 Torgersen          17.8               181        3625 Adelie            38.9\n 8 Torgersen          19.6               195        4675 Adelie            39.2\n 9 Torgersen          18.1               193        3475 Adelie            34.1\n10 Torgersen          20.2               190        4250 Adelie            42  \n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n(penguins\n  .select(pl.col(\"island\"), pl.col(\"bill_depth_mm\"), \n          pl.col(\"flipper_length_mm\"), pl.col(\"body_mass_g\"), \n          pl.col(\"species\"), pl.col(\"bill_length_mm\"), pl.col(\"sex\"))\n)\n\n\nshape: (344, 7)\n\n\n\nisland\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nspecies\nbill_length_mm\nsex\n\n\nstr\nf64\nf64\nf64\nstr\nf64\nstr\n\n\n\n\n\"Torgersen\"\n18.7\n181.0\n3750.0\n\"Adelie\"\n39.1\n\"male\"\n\n\n\"Torgersen\"\n17.4\n186.0\n3800.0\n\"Adelie\"\n39.5\n\"female\"\n\n\n\"Torgersen\"\n18.0\n195.0\n3250.0\n\"Adelie\"\n40.3\n\"female\"\n\n\n\"Torgersen\"\nnull\nnull\nnull\n\"Adelie\"\nnull\nnull\n\n\n\"Torgersen\"\n19.3\n193.0\n3450.0\n\"Adelie\"\n36.7\n\"female\"\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Dream\"\n19.8\n207.0\n4000.0\n\"Chinstrap\"\n55.8\n\"male\"\n\n\n\"Dream\"\n18.1\n202.0\n3400.0\n\"Chinstrap\"\n43.5\n\"female\"\n\n\n\"Dream\"\n18.2\n193.0\n3775.0\n\"Chinstrap\"\n49.6\n\"male\"\n\n\n\"Dream\"\n19.0\n210.0\n4100.0\n\"Chinstrap\"\n50.8\n\"male\"\n\n\n\"Dream\"\n18.7\n198.0\n3775.0\n\"Chinstrap\"\n50.2\n\"female\""
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#simple-summaries-by-group",
    "href": "posts/dplyr-vs-polars/index.html#simple-summaries-by-group",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Simple summaries by group",
    "text": "Simple summaries by group\nLet’s suppose we want to compute summaries by groups such as means or medians. Both packages are very similar again: on the R side you have dplyr::group_by() and dplyr::summarize(), while on the Python side you have polars.group_by() and polars.agg().\nNote that dplyr::group_by() also automatically arranges the results by the group, so the reproduce the results of dplyr, we need to add polars.sort() to the chain.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  island    bill_depth_mean\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Biscoe               15.9\n2 Dream                18.3\n3 Torgersen            18.4\n\n\n\n\n\n(penguins\n  .group_by(\"island\")\n  .agg(bill_depth_mean = pl.mean(\"bill_depth_mm\"))\n  .sort(\"island\")\n)\n\n\nshape: (3, 2)\n\n\n\nisland\nbill_depth_mean\n\n\nstr\nf64\n\n\n\n\n\"Biscoe\"\n15.87485\n\n\n\"Dream\"\n18.344355\n\n\n\"Torgersen\"\n18.429412"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#more-complicated-summaries-by-group",
    "href": "posts/dplyr-vs-polars/index.html#more-complicated-summaries-by-group",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "More complicated summaries by group",
    "text": "More complicated summaries by group\nTypically, you want to create multiple different summaries by groups. dplyr provides a lot of flexibility to create new variables on the fly, while polars seems to be a bit more restrictive. For instance, to compute the share of female penguins by group, it makes more sense to create an ìs_female indicator column using polars because polars.mean() does not accept expressions as inputs.\n\ndplyrpolars\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(count = n(),\n            bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE),\n            flipper_length_median = median(flipper_length_mm, na.rm = TRUE),\n            body_mass_sd = sd(body_mass_g, na.rm = TRUE),\n            share_female = mean(sex == \"female\", na.rm = TRUE))\n\n# A tibble: 3 × 6\n  island   count bill_depth_mean flipper_length_median body_mass_sd share_female\n  &lt;fct&gt;    &lt;int&gt;           &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Biscoe     168            15.9                   214         783.        0.491\n2 Dream      124            18.3                   193         417.        0.496\n3 Torgers…    52            18.4                   191         445.        0.511\n\n\n\n\n\n(penguins\n  .with_columns(is_female = pl.when(pl.col(\"sex\") == \"female\").then(1)\n                  .when(pl.col(\"sex\").is_null()).then(None)\n                  .otherwise(0))\n  .group_by(\"island\")\n  .agg(\n    count = pl.count(),\n    bill_depth_mean = pl.mean(\"bill_depth_mm\"),\n    flipper_length_median = pl.median(\"flipper_length_mm\"),\n    body_mass_sd = pl.std(\"body_mass_g\"),\n    share_female = pl.mean(\"is_female\")\n  )\n  .sort(\"island\")\n)\n\n\nshape: (3, 6)\n\n\n\nisland\ncount\nbill_depth_mean\nflipper_length_median\nbody_mass_sd\nshare_female\n\n\nstr\nu32\nf64\nf64\nf64\nf64\n\n\n\n\n\"Biscoe\"\n168\n15.87485\n214.0\n782.855743\n0.490798\n\n\n\"Dream\"\n124\n18.344355\n193.0\n416.644112\n0.495935\n\n\n\"Torgersen\"\n52\n18.429412\n191.0\n445.10794\n0.510638"
  },
  {
    "objectID": "posts/dplyr-vs-polars/index.html#footnotes",
    "href": "posts/dplyr-vs-polars/index.html#footnotes",
    "title": "Tidy Data Manipulation: dplyr vs polars",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the unifying principles of the tidyverse: https://design.tidyverse.org/unifying.html.↩︎"
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html",
    "href": "posts/ggplot2-vs-seaborn/index.html",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "",
    "text": "ggplot2 is based on Leland Wilkinson”s Grammar of Graphics, a set of principles for creating consistent and effective statistical graphics, and was developed by Hadley Wickham. The package is a cornerstone of the R community and integrates seamlessly with other tidyverse packages. One of the key strengths of ggplot2 is its use of a consistent syntax, making it relatively easy to learn and enabling users to create a wide range of graphics with a common set of functions. The package is also highly customizable, allowing detailed adjustments to almost every element of a plot.\nseaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Key features of seaborn include its ability to work well with pandas data frames, built-in themes for styling matplotlib graphics, and functions for visualizing distributions.\nThe types of plots that I chose for the comparison heavily draw on the examples given in R for Data Science - an amazing resource if you want to get started with data visualization."
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#a-categorical-variable",
    "href": "posts/ggplot2-vs-seaborn/index.html#a-categorical-variable",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "A categorical variable",
    "text": "A categorical variable\nLet’s break down the differences in smaller steps by focusing on simpler examples. If you have a categorical variable and want to compare its relevance in your data, then ggplot2::geom_bar() and seaborn.countplot() are your friends. I manually specify the order in the seaborn figure to mimic the automatic behavior of ggplot2.\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = island)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\nsns.countplot(\n  data = penguins, \n  x = \"island\",\n  order = sorted(penguins[\"island\"].unique())\n)"
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#a-numerical-variable",
    "href": "posts/ggplot2-vs-seaborn/index.html#a-numerical-variable",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "A numerical variable",
    "text": "A numerical variable\nIf you have a numerical variable, usually histograms are a good starting point to get a better feeling for the distribution of your data. ggplot2::geom_histogram() and seaborn.histplot with options to control bin widths or number of bins are the functions for this task.\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\n\n\nsns.histplot(\n  data = penguins, \n  x = \"bill_length_mm\", \n  binwidth = 2\n)\n\n\n\n\n\n\n\n\n\n\n\nBoth packages also support density curves, but I personally wouldn”t recommend to start with densities because they are estimated curves that might obscure underlying data features. However, we look at densities in the next section."
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#a-numerical-and-a-categorical-variable",
    "href": "posts/ggplot2-vs-seaborn/index.html#a-numerical-and-a-categorical-variable",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "A numerical and a categorical variable",
    "text": "A numerical and a categorical variable\nTo visualize relationships, you need to have at least two columns. If you have a numerical and a categorical variable, then histograms or densities with groups are a good starting point. The next example illustrates the use of density curves via ggplot2::geom_density() and seaborn.kdeplot() with similar options to control the appearance.\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(linewidth = 0.75, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(\n  data = penguins, \n  x = \"body_mass_g\", \n  hue = \"species\", \n  fill = True, common_norm = False, alpha = 0.5, linewidth = 0.75\n)"
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#two-categorical-columns",
    "href": "posts/ggplot2-vs-seaborn/index.html#two-categorical-columns",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "Two categorical columns",
    "text": "Two categorical columns\nStacked bar plots are a good way to display the relationship between two categorical columns. geom_bar() with the position argument and seaborn.histplot() with stat are your aesthetics of choice for this task. Note that you can easily switch to counts by using position = \"identity\" and stat = \"count\", respectively, instead of relative frequencies as in the example below. Note that I use shrink = 0.8 to get some spacing between columns in the seaborn plot.\n\nggplot2seaborn\n\n\n\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\n\nsns.histplot(\n  data = penguins, \n  x = \"species\", \n  hue = \"island\", multiple = \"fill\", stat = \"percent\", shrink = 0.8\n)"
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#two-numerical-columns",
    "href": "posts/ggplot2-vs-seaborn/index.html#two-numerical-columns",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "Two numerical columns",
    "text": "Two numerical columns\nScatter plots and regression lines are definitely the most common approach for visualizing the relationship between two numerical columns and we focus on scatter plots for this example (see the first visualization example if you want to see again how to add a regression line). Here, the size parameter controls the size of the shapes that you use for the data points in ggplot2::geom_point() relative to the base size (i.e., it is not tied to any unit of measurement like pixels). For seaborn.scatterplot() you have the s parameter to control point sizes manually, where size is typically given in squared points (where a point is a unit of measure in typography, equal to 1/72 of an inch).\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(size = 2)\n\n\n\n\n\n\n\n\n\n\n\nsns.scatterplot(\n  data = penguins, \n  x = \"bill_length_mm\", y = \"flipper_length_mm\", \n  s = 50\n)"
  },
  {
    "objectID": "posts/ggplot2-vs-seaborn/index.html#three-or-more-columns",
    "href": "posts/ggplot2-vs-seaborn/index.html#three-or-more-columns",
    "title": "Tidy Data Visualization: ggplot2 vs seaborn",
    "section": "Three or more columns",
    "text": "Three or more columns\nYou can include more information by mapping columns to additional aesthetics. For instance, we can map colors and shapes to species and create separate plots for each island by using facets. Facets are actually a great way to extend your figures, so I highly recommend playing around with them using your own data.\nIn ggplot2 you add the facet layer at the end, whereas in seaborn you have to start with the facet grid at the beginning and map scatter plots across facets. Note that I use variable assignment to penguins_facet in order to prevent seaborn from printing the figure twice while rendering this post (no idea why though).\n\nggplot2seaborn\n\n\n\nggplot(penguins, \n       aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\n\n\n\n\n\n\n\n\n\npenguins_facet = (sns.FacetGrid(\n    data = penguins, col=\"island\", col_order = sorted(penguins[\"island\"].unique()),\n    hue=\"species\", margin_titles = True\n  )\n  .map(sns.scatterplot, \"bill_length_mm\", \"flipper_length_mm\", alpha = 0.7)\n  .add_legend()\n)"
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "",
    "text": "Imagine trying to cook a meal in a disorganized kitchen where ingredients are mixed up and nothing is labeled. It would be chaotic and time-consuming to look for the right ingredients and there might be some trial error involved, possibly ruining your planned meal.\nTidy data are like well-organized shelves in your kitchen. Each shelf provides a collection of containers that semantically belong together, e.g., spices or dairies. Each container on the shelf holds one type of ingredient, and the labels on the containers clearly describe what is inside, e.g., pepper or milk. In the same way, tidy data organizes information into a clear and consistent format, where each type of observational unit forms a table, each variable is in a column, and each observation is in a row (Wickham 2014).\nTidying data is about structuring datasets to facilitate analysis, visualization, report generation, or modelling. By following the principle that each variable forms a column, each observation forms a row, and each type of observational unit forms a table, data analysis becomes more intuitive, akin to cooking in a well-organized kitchen where everything has its place and you spend less time on searching for ingredients."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#example-for-tidy-data",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#example-for-tidy-data",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "Example for tidy data",
    "text": "Example for tidy data\nTo illustrate the concept of tidy data in our tidy kitchen, suppose we have a table called ingredient that contains information about all the ingredients that we currently have in our kitchen. It might look as follows:\n\n\n\nname\nquantity\nunit\ncategory\n\n\n\n\nflour\n500\ngrams\nbaking\n\n\nsugar\n200\ngrams\nbaking\n\n\nbutter\n100\ngrams\ndairy\n\n\neggs\n4\nunits\ndairy\n\n\nmilk\n1\nliters\ndairy\n\n\nsalt\n10\ngrams\nseasoning\n\n\nolive oil\n0.2\nliters\noil\n\n\ntomatoes\n300\ngrams\nvegetable\n\n\nchicken\n400\ngrams\nmeat\n\n\nrice\n250\ngrams\ngrain\n\n\n\nEach row refers to a specific ingredient and each column has a dedicated type and meaning. For instance, the column quantity contains information about how much of the ingredient called name we currently have and which unit we use to measure it.\nSimilarly, we could have a table just for dairy that might look as follows:\n\n\n\nname\nquantity\nunit\n\n\n\n\nmilk\n1\nliters\n\n\nbutter\n200\ngrams\n\n\nyogurt\n150\ngrams\n\n\ncheese\n100\ngrams\n\n\ncream\n0.5\nliters\n\n\ncottage cheese\n250\ngrams\n\n\nsour cream\n150\ngrams\n\n\nghee\n100\ngrams\n\n\nwhipping cream\n0.3\nliters\n\n\nice cream\n500\ngrams\n\n\n\nNotice that there is no category column in this table? It would actually be redundant to have this column because all rows in the `dairy`` table have the same category."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-colum-headers-are-values-not-variable-names",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-colum-headers-are-values-not-variable-names",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When colum headers are values, not variable names",
    "text": "When colum headers are values, not variable names\nNow let us move to data structures that are untidy. Consider the following variant of our dairy table:\n\n\n\ntype\nliters\ngrams\n\n\n\n\nmilk\n1\n\n\n\nbutter\n\n200\n\n\nyogurt\n\n150\n\n\ncheese\n\n100\n\n\ncream\n0.5\n\n\n\ncottage cheese\n\n250\n\n\nsour cream\n\n150\n\n\nghee\n\n100\n\n\nwhipping cream\n0.3\n\n\n\nice cream\n\n500\n\n\n\nWhat is the issue here? Each row still refers to a specific dairy product. However, instead of dedicated quantity and unit columns, we have a liters and grams column. Since the units differ across dairy products, the table even contains missing values in the form of emtpy cells. So if you want to find out how much of ice cream you still have, you need to also check out the column name. In practice, we would create dedicated quantity and unit columns. we might even decide to have the same unit for all ingredients (e.g., measure everything in grams) and just keep a quantity column."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-multiple-variables-are-stored-in-one-column",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-multiple-variables-are-stored-in-one-column",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When multiple variables are stored in one column",
    "text": "When multiple variables are stored in one column\nLet us consider the following untidy version of our ingredient table.\n\n\n\ntype\nquantity_and_unit\n\n\n\n\nflour\n500 grams\n\n\nsugar\n200 grams\n\n\nbutter\n100 grams\n\n\neggs\n4 units\n\n\nmilk\n1 liter\n\n\nsalt\n10 grams\n\n\nolive oil\n0.2 liters\n\n\ntomatoes\n300 grams\n\n\nchicken\n400 grams\n\n\nrice\n250 grams\n\n\n\nThis one is really annoying, since the quantity_and_unit column combines both the quantity and the unit of measurement into one string for each ingredient. Why is this an issue? This format actually makes it harder to perform numerical operations on the quantities or to filter or aggregate the data based on the unit of measurement. So in practice, we would actually start our data analysis by splitting out the quantity_and_unit column into quantity and unit."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-variables-are-stored-in-both-rows-and-columns",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-variables-are-stored-in-both-rows-and-columns",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When variables are stored in both rows and columns",
    "text": "When variables are stored in both rows and columns\nLet us extend our kitchen analogy by additionally considering recipes. For simplicity, a recipe just denotes how much of each ingredient is required. The following table contains two variants of a recipe for pancakes:\n\n\n\ningredient\nrecipe1_quantity\nrecipe2_quantity\n\n\n\n\nflour\n500 grams\n300 grams\n\n\nsugar\n200 grams\n150 grams\n\n\nbutter\n100 grams\n50 grams\n\n\neggs\n4 units\n3 units\n\n\nmilk\n1 liters\n0.5 liters\n\n\n\nThe quantity for each ingredient for two different recipes is stored in separate columns. This structure makes it harder to perform operations like filtering or summarizing the data by recipe or ingredient.\nTo convert this data to a tidy format, you would typically want to gather the quantities into a single column, and include additional columns to specify the recipe and unit of measurement for each quantity. We can then filer"
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-there-are-multiple-types-of-data-in-the-same-column",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-there-are-multiple-types-of-data-in-the-same-column",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When there are multiple types of data in the same column",
    "text": "When there are multiple types of data in the same column\nA recipe typically contains information on the required utensils and how much time a step requires. Consider the following table with different types of data:\n\n\n\ntype\nquantity\ncategory\n\n\n\n\nflour\n500 grams\ningredient\n\n\nbutter\n100 grams\ningredient\n\n\nwhisk\n1 unit\nutensil\n\n\nsugar\n200 grams\ningredient\n\n\nbaking time\n30 minutes\ntime\n\n\n\nThe table is trying to describe a recipe but combines different types of data within the same columns. There are ingredients with their quantities, a utensil, and cooking time, all mixed together.\nA tidy approach would typically separate these different types of data into separate tables or at least into distinct sets of columns, making it clear what each part of the data represents and facilitating further analysis and visualization."
  },
  {
    "objectID": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-some-data-is-missing",
    "href": "posts/tidy-data-a-recipe-for-efficient-data-analysis/index.html#when-some-data-is-missing",
    "title": "Tidy Data: A Recipe for Efficient Data Analysis",
    "section": "When some data is missing",
    "text": "When some data is missing\nAs a last example for untidy data, let us consider the original ingredient table again, but with a few empty cells.\n\n\n\nname\nquantity\nunit\n\n\n\n\nflour\n\ngrams\n\n\nsugar\n200\ngrams\n\n\nbutter\n100\ngrams\n\n\neggs\n4\nunits\n\n\nmilk\n10\n\n\n\nsalt\n10\ngrams\n\n\nolive oil\n0.2\nliters\n\n\ntomatoes\n300\ngrams\n\n\nchicken\n400\ngrams\n\n\n\n250\ngrams\n\n\n\nWhat is the issue here? There are actually a couple of them:\n\nThe flour row does have any information about quantity, so we just don’t know how much we have.\nThe milk row does not contain a unit, so we might have 10 liters, 10 milliliters, or 10 cups of milk.\nThe last row does not have any name, so we have 250 grams of something that we just can’t identify.\n\nWhy is this important? It makes a huge difference how me treat the missing information. For instance, we might make an educated guess for milk if we always record that information in litres, then the missing unit is very likely litres. For flour, we could play it safe and just say that the available quantity is zero. For the ingredient without a name, we might have to throw it away or ask somebody else to tell us what it is.\nOverall, these examples highlight the most important issues that you might have to consider when preparing data for your analysis."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html",
    "href": "posts/tidy-collaborative-filtering/index.html",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "",
    "text": "Recommender systems are a key component of our digital lifes, ranging from e-commerce, online advertisements, movie recommendations, or more generally all kinds of product recommendations. A recommender system aims to efficiently deliver personalized content to users based on a large pool of potentially relevant information. In this blog post, I illustrate the concept of recommender systems by building a simple stock recommendation tool that relies on publicly available portfolios from the social trading platform wikifolio.com. The resulting recommender proposes stocks to investors who already have their own portfolios and look for new investment opportunities. The underlying assumption is that the wikifolio traders hold stock portfolios that can provide meaningful inspiration for other investors. The resulting stock recommendations of course do not constitute any investment advice and rather serve an illustrative purpose.\nwikifolio.com is the leading social trading platform in Europe, where anyone can publish and monetize their trading strategies through virtual portfolios, which are called wikifolios. The community of wikifolio traders includes full time investors and successful entrepreneurs, as well as experts from different sectors, portfolio managers, and editors of financial magazines. All traders share their trading ideas through fully transparent wikifolios. The wikifolios are easy to track and replicate, by investing in the corresponding, collateralized index certificate. As of writing, there are more than 30k published wikifolios of more than 9k traders, indicating a large diversity of available portfolios for training our recommender.\nThere are essentially three types of recommender models: recommenders via collaborative filtering, recommenders via content-based filtering, and hybrid recommenders (that mix the first two). In this blog post, I focus on the collaborative filtering approach as it requires no information other than portfolios and can provide fairly high precision with little complexity. Nonetheless, I first briefly describe the recommender approaches and refer to Ricci et al. (2011)1 for a comprehensive exposition."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#a-primer-on-recommender-systems",
    "href": "posts/tidy-collaborative-filtering/index.html#a-primer-on-recommender-systems",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "A Primer on Recommender Systems",
    "text": "A Primer on Recommender Systems\n\nCollaborative Filtering\nIn collaborative filtering, recommendations are based on past user interactions and items to produce new recommendations. The central notion is that past user-item interactions are sufficient to detect similar users or items. Broadly speaking, there are two sub-classes of collaborative filtering: the memory-based approach, which essentially searches nearest neighbors based on recorded transactions and is hence model-free, and the model-based approach, where new representations of users and items are built based on some generative pre-estimated model. Theoretically, the memory-based approach has a low bias (since no latent model is assumed) but a high variance (since the recommendations change a lot in the nearest neighbor search). The model-based approach relies on a trained interaction model. It has a relatively higher bias but a lower variance, i.e., recommendations are more stable since they come from a model.\nAdvantages of collaborative filtering include: (i) no information about users or items is required; (ii) a high precision can be achieved with little data; (iii) the more interaction between users and items is available, the more recommendations become accurate. However, the disadvantages of collaborative filtering are: (i) it is impossible to make recommendations to a new user or recommend a new item (cold start problem); (ii) calculating recommendations for millions of users or items consumes a lot of computational power (scalability problem); (iii) if the number of items is large relative to the users and most users only have interacted with a small subset of all items, then the resulting representation has many zero interactions and might hence lead to computational difficulties (sparsity problem).\n\n\nContent-Based Filering\nContent-based filtering methods exploit information about users or items to create recommendations by building a model that relates available characteristics of users or items to each other. The recommendation problem is hence cast into a classification problem (the user will like the item or not) or more generally a regression problem (which rating will the user give an item). The classification problem can be item-centered by focusing on available user information and estimating a model for each item. If there are a lot of user-item interactions available, the resulting model is fairly robust, but it is less personalized (as it ignores user characteristics apart from interactions). The classification problem can also be user-centered by working with item features and estimating a model for each user. However, if a user only has a few interactions then the resulting model becomes easily unstable. Content-based filtering can also be neither user nor item-centered by stacking the two feature vectors, hence considering both input simultaneously, and putting them into a neural network.\nThe main advantage of content-based filtering is that it can make recommendations for new users without any interaction history or recommend new items to users. The disadvantages include: (i) training needs a lot of users and item examples for reliable results; (ii) tuning might be much harder in practice than collaborative filtering; (iii) missing information might be a problem since there is no clear solution how to treat missingness in user or item characteristics.\n\n\nHybrid Recommenders\nHybrid recommender systems combine both collaborative and content-based filtering to overcome the challenges of each approach. There are different hybridization techniques available, e.g., combining the scores of different components (weighted), chosing among different component (switching), following strict priority rules (cascading), presenting outputs from different components at the same time (mixed), etc."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#train-collaborative-filtering-recommenders-in-r",
    "href": "posts/tidy-collaborative-filtering/index.html#train-collaborative-filtering-recommenders-in-r",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Train Collaborative Filtering Recommenders in R",
    "text": "Train Collaborative Filtering Recommenders in R\nFor this post, we rely on the tidyverse family of packages, scales for scale functions for visualization, and recommenderlab2 - a package that provides an infrastructure to develop and test collaborative filtering recommender algorithms.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(recommenderlab)\n\nI load a data set with stock holdings of investable wikifolios at the beginning of 2023 that I host in one of my repositories. The data contains the portfolios of 6,544 wikifolios that held in total 5,069 stocks on January 1st 2023.\n\nwikifolio_portfolios &lt;- read_csv(\"https://raw.githubusercontent.com/christophscheuch/christophscheuch.github.io/main/data/wikifolio_portfolios.csv\") \nglimpse(wikifolio_portfolios)\n\nRows: 149,916\nColumns: 2\n$ wikifolio &lt;chr&gt; \"DE000LS9AAB3\", \"DE000LS9AAB3\", \"DE000LS9AAB3\", \"DE000LS9AAB…\n$ stock     &lt;chr&gt; \"AU000000CSL8\", \"AU0000193666\", \"CA0679011084\", \"CA136635109…\n\n\nFirst, I convert the long data to a binary rating matrix.\n\nbinary_rating_matrix &lt;- wikifolio_portfolios |&gt;\n  mutate(in_portfolio = 1) |&gt; \n  pivot_wider(id_cols = wikifolio,\n              names_from = stock,\n              values_from = in_portfolio,\n              values_fill = list(in_portfolio = 0)) |&gt;\n  select(-wikifolio) |&gt;\n  as.matrix() |&gt;\n  as(\"binaryRatingMatrix\")\nbinary_rating_matrix\n\n6544 x 5069 rating matrix of class 'binaryRatingMatrix' with 149916 ratings.\n\n\nI perform cross-validation and split the data into training and test data. The training sample constitute 80% of the data and I perform 5-fold cross validation. Testing is performed by withholding items from the test portfolios (parameter given) and checking how well the algorithm predicts the withheld items. The value given=-1 means that an algorithm sees all but 1 withheld stock for the test portfolios and needs to predict the missing stock. I refer to Breese et al. (1998)3 for a discussion of other withholding strategies.\n\nscheme &lt;- binary_rating_matrix |&gt;\n  evaluationScheme(\n    method = \"cross-validation\",\n    k      = 5,\n    train  = 0.8,\n    given  = -1\n)\nscheme\n\nEvaluation scheme using all-but-1 items\nMethod: 'cross-validation' with 5 run(s).\nGood ratings: NA\nData set: 6544 x 5069 rating matrix of class 'binaryRatingMatrix' with 149916 ratings.\n\n\nHere is the list of recommenders that I consider for the backtest with some intuition:\n\nRandom Items: the benchmark case because it just stupidly chooses random stocks from all possible choices.\nPopular Items: just recommends the most popular stocks to measured by the number of wikifolios that hold the stock.\nAssociation Rules: each wikifolio and its portfolio is considered as a transaction. Association rule mining finds similar portfolios across all traders (if traders have x, y and z in their portfolio, then they are X% likely of also including w).\nItem-Based Filtering: the algorithm calculates a similarity matrix across stocks. Recommendations are then based on the list of most similar stocks to the ones the wikifolio already has in its portfolio.\nUser-Based Filtering: the algorithm finds a neighborhood of similar wikifolios for each wikifolio (for this exercise it is set to 100 most similar wikifolios). Recommendations are then based on what the most similar wikifolios have in their portfolio.\n\nFor each algorithm, I base the evaluation on 1, 3, 5, and 10 recommendations. This specification means that each algorithm proposes 1 to 10 recommendations to the test portfolios and the evaluation scheme then checks whether the proposals contain the one withheld stock. Note that the evaluation takes a couple of hours, in particular because the Item-Based and User-Based Filtering approaches are quite time-consuming.\n\nalgorithms &lt;- list(\n  \"Random Items\"         = list(name  = \"RANDOM\",  param = NULL),\n  \"Popular Items\"        = list(name  = \"POPULAR\", param = NULL),\n  \"Association Rules\"    = list(name  = \"AR\", param = list(supp = 0.01, conf = 0.1)),\n  \"Item-Based Filtering\" = list(name  = \"IBCF\", param = list(k = 10)),\n  \"User-Based Filtering\" = list(name  = \"UBCF\", param = list(method = \"Cosine\", nn = 100))\n)\n\nnumber_of_recommendations &lt;- c(1, 3, 5, 10)\nresults &lt;- evaluate(\n  scheme,\n  algorithms,\n  type = \"topNList\",\n  progress = TRUE,\n  n = number_of_recommendations\n)"
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#evaluate-recommenders",
    "href": "posts/tidy-collaborative-filtering/index.html#evaluate-recommenders",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Evaluate Recommenders",
    "text": "Evaluate Recommenders\nThe output of evaluate() already provides the evaluation metrics in a structured way. I can simply average the metrics over the cross-validation folds.\n\nresults_tbl &lt;- results |&gt;\n  avg() |&gt;\n  map(as_tibble) |&gt;\n  bind_rows(.id = \"model\")\n\nNow, for each recommender, we are interested the following numbers:\n\nTrue Negative (TN) = number of not predicted items that do not correspond to withheld items\nFalse Positive (FP) = number of incorrect predictions that do not correspond to withheld items\nFalse negative (FN) = number of not predicted items that correspond to withheld items\nTrue Positive (TP) = number of correct predictions that correspond to withheld items\n\nThe two figures below present the most common evaluation techniques for the performance of recommender algorithms in backtest settings like mine.\n\nROC curves\nThe first visualization approach comes from signal-detection and is called “Receiver Operating Characteristic” (ROC). The ROC-curve plots the algorithm’s probability of detection (TPR) against the probability of false alarm (FPR).\n\nTPR = TP / (TP + FN) (i.e., share of true positive recommendations relative to all known portfolios)\nFPR = FP / (FP + TN) (i.e., share of incorrect recommendations relative to )\n\nIntuitively, the bigger the area under the ROC curve, the better is the corresponding algorithm.\n\nresults_tbl |&gt;\n  ggplot(aes(FPR, TPR, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"False Positive Rate (FPR)\",\n    y = \"True Positive Rate (TPR)\",\n    title = \"ROC curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\n\nThe figure shows that recommending random items exhibits the lowest TPR for any FPR, so it is the worst among all algorithms (which is not surprising). Association rules, on the other hand, constitute the best algorithm among the current selection. This result is neat because association rule mining is a computationally cheap algorithm, so we could potentially fine-tune or reestimate the model easily.\n\n\nPrecision-Recall Curves\nThe second popular approach is to plot Precision-Recall curves. The two measures are often used in information retrieval problems:\n\nPrecision = TP / (TP + FP) (i.e., correctly recommended items relative to total recommended items)\nRecall = TP / (TP + FN) (i.e., correctly recommended items relative to total number of known useful recommendations)\n\nThe goal is to have a higher precision for any level of recall. In fact, there is trade-off between the two measures since high precision means low recall and vice-versa.\n\nresults_tbl |&gt;\n  ggplot(aes(x = recall, y = precision, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"Recall\", y = \"Precision\",\n    title = \"Precision-Recall curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\n\nAgain, proposing random items exhibits the worst performance, as for any given level of recall, this approach has the lowest precision. Association rules are also the best algorithm with this visualization approach."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#create-predictions",
    "href": "posts/tidy-collaborative-filtering/index.html#create-predictions",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Create Predictions",
    "text": "Create Predictions\nThe final step is to create stock recommendations for investors who already have portfolios. I pick the AR algorithm to create such recommendations because it excelled in the analyses above. Note that in the case of association rules, I also need to provide the support and confidence parameters.\n\nrecommender &lt;- Recommender(binary_rating_matrix, method = \"AR\", param = list(supp = 0.01, conf = 0.1))\n\nAs an example, suppose you currently have a portfolio that consists of Nvidia (US67066G1040) and Apple (US0378331005). I have to transform this sample portfolio into a rating matrix with the same dimensions as the data we used as input for our training. The predict() function then delivers a prediction for the example portfolio.\n\nsample_portfolio &lt;- c(\"US67066G1040\", \"US0378331005\")\nsample_rating_matrix &lt;- tibble(distinct(wikifolio_portfolios, stock)) |&gt;\n  mutate(in_portfolio = if_else(stock %in% sample_portfolio, 1, 0)) |&gt;\n  pivot_wider(names_from = stock,\n              values_from = in_portfolio,\n              values_fill = list(in_portfolio = 0)) |&gt;\n  as.matrix() |&gt;\n  as(\"binaryRatingMatrix\")\n\nprediction &lt;- predict(recommender, sample_rating_matrix, n = 1)\nas(prediction, \"list\")[[1]]\n\n[1] \"US5949181045\"\n\n\nSo the AR algorithm recommends Microsoft (US5949181045) as a stock if you are already invested in Nvidia and Apple, which makes a lot of sense given the similarity in business model. Of course, this recommendation is not serious investment advice, but rather serves an illustrative purpose of how recommenderlab can be used to quickly prototype different collaborative filtering recommender algorithms."
  },
  {
    "objectID": "posts/tidy-collaborative-filtering/index.html#footnotes",
    "href": "posts/tidy-collaborative-filtering/index.html#footnotes",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRicci, F, Rokach, L., Shapira, B. and Kantor, P. (2011). “Recommender Systems Handbook”. https://link.springer.com/book/10.1007/978-0-387-85820-3.↩︎\nHahsler, M. (2022). “recommenderlab: An R Framework for Developing and Testing Recommendation Algorithms”, R package version 1.0.3. https://CRAN.R-project.org/package=recommenderlab.↩︎\nBreese, J.S., Heckerman, D. and Kadie, C. (1998). “Empirical Analysis of Predictive Algorithms for Collaborative Filtering”, Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, Madison, 43-52. https://arxiv.org/pdf/1301.7363.pdf.↩︎"
  },
  {
    "objectID": "posts/dax-seasonality/index.html",
    "href": "posts/dax-seasonality/index.html",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "",
    "text": "Seasonalcharts.de claims that stock indices exhibit persistent seasonality that may be exploited through an appropriate trading strategy. As part of a job application, I had to replicate the seasonal pattern for the DAX and then test whether this pattern entails a profitable trading strategy. To sum up, I indeed find that a trading strategy that holds the index only over a specific season outperforms the market significantly, but these results might be driven by a few outliers. Note that the post below references an opinion and is for information purposes only. I do not intend to provide any investment advice.\nThe code is structured in a way that allows for a straight-forward replication of the methodology for other indices. The post uses the following packages:\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(frenchdata)\nlibrary(scales)\nlibrary(fixest)"
  },
  {
    "objectID": "posts/dax-seasonality/index.html#data-preparation",
    "href": "posts/dax-seasonality/index.html#data-preparation",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, download data from yahoo finance using the tidyquant package. Note that the DAX was officially launched in July 1988, so this is where our sample starts.\n\ndax_raw &lt;- tq_get(\n  \"^GDAXI\", get = \"stock.prices\", \n  from = \"1988-07-01\", to = \"2023-10-30\"\n) \n\nThen, select only date and the adjusted price (i.e., closing price after adjustments for all applicable splits and dividend distributions) as the relevant variables and compute summary statistics to check for missing or weird values. The results are virtually the same if we use unadjusted closing prices.\n\ndax &lt;- dax_raw |&gt;\n  select(date, price = adjusted)\n\nWe replace the missing values by the last available index value.\n\ndax &lt;- dax |&gt;\n  arrange(date) |&gt;\n  fill(price, .direction = \"down\")\n\nAs a immediate plausibility check, we plot the DAX over the whole sample period.\n\ndax |&gt;\n  ggplot(aes(x = date, y = price)) +\n  geom_line() + \n  labs(x = NULL, y = \"Adjusted Price\",\n       title = \"Adjusted DAX index price between 1988 and 2023\") +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\nThe main idea of Seasonalcharts is to implement the strategy proposed by Jacobsen and Bouman (2002)1 and Jacobsen and Zhan (2018)2 which they label ‘The Halloween Indicator’ (or ‘Sell in May Effect’). The main finding of these papers is that stock indices returns seem significantly lower during the May-October period than during the remainder of the year. The corresponding trading strategy holds an index during the months November-April, but holds the risk-free asset in the May-October period.\nTo replicate their approach (and avoid noise in the daily data), we focus on monthly returns from now on.\n\ndax_monthly &lt;- dax |&gt;\n  mutate(year = year(date),\n         month = factor(month(date))) |&gt;\n  group_by(year, month) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  arrange(date) |&gt;\n  mutate(ret = price / lag(price) - 1) |&gt;\n  drop_na()\n\nAnd as usual in empirical asset pricing, we do not care about raw returns, but returns in excess of the risk-free asset. We simply add the European risk free rate from the Fama-French data library as the corresponding reference point. Of course, one could use other measures for the risk-free rate, but the impact on the results won’t be substantial.\n\nfactors_ff3_monthly_raw &lt;- download_french_data(\"Fama/French 3 Factors\")\n\nrisk_free_monthly &lt;- factors_ff3_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    year = year(ymd(str_c(date, \"01\"))),\n    month = factor(month(ymd(str_c(date, \"01\")))),\n    rf = as.numeric(RF) / 100,\n    .keep = \"none\"\n  )\n\ndax_monthly &lt;- dax_monthly |&gt; \n  left_join(risk_free_monthly, join_by(year, month)) |&gt; \n  mutate(ret_excess = ret - rf) |&gt; \n  drop_na()"
  },
  {
    "objectID": "posts/dax-seasonality/index.html#graphical-evidence-for-seasonality",
    "href": "posts/dax-seasonality/index.html#graphical-evidence-for-seasonality",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Graphical Evidence for Seasonality",
    "text": "Graphical Evidence for Seasonality\nWe start by first plotting the average returns for each month.\n\ndax_monthly |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret)) |&gt; \n  ggplot(aes(x = month, y = ret, fill = ret &gt; 0)) +\n  geom_col() +\n  scale_y_continuous(labels = percent) + \n  labs(\n    x = \"Month\", y = \"Average DAX Return\", \n    title = \"Average monthly DAX returns between 1988 and 2023\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe figure shows negative returns for June, August, and September, while all other months exhibit positive returns. However, it makes more sense to look at distributions instead of simple means, which might be heavily influenced by outliers. To illustrate distributions, I follow Cedric Scherer and use raincloud plots. which combine halved violin plot, a box plot, and the raw data as some kind of scatter. These plots hence provide detailed visualizations of the distributions.\n\ndax_monthly |&gt; \n  ggplot(aes(x = month, y = ret, group = month)) + \n  ggdist::stat_halfeye(\n    adjust = .5, width = .6, .width = 0, justification = -.3, point_colour = NA\n    ) + \n  geom_boxplot(\n    width = .25, outlier.shape = NA\n    ) +\n  stat_summary(\n    fun = mean, geom=\"point\", color = \"red\", fill = \"red\"\n    ) +\n  geom_point(\n    size = 1.5, alpha = .2, position = position_jitter(seed = 42, width = .1)\n    ) +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n  labs(\n    x = \"Month\", y = \"DAX Return\", \n    title = \"Distributions of monthly DAX returns between 1988 and 2023\",\n    subtitle = \"Red dots indicate means, solid lines indicate medians\"\n  ) \n\n\n\n\n\n\n\n\nThe figure suggests that August and September exhibit considerable negative outliers."
  },
  {
    "objectID": "posts/dax-seasonality/index.html#evaluating-trading-strategies",
    "href": "posts/dax-seasonality/index.html#evaluating-trading-strategies",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Evaluating Trading Strategies",
    "text": "Evaluating Trading Strategies\nLet us now take a look at the average excess returns per month. We also add the standard deviation, 5% and 95% quantiles, and t-statistic of a t-test of the null hypothesis that average returns are zero in a given month.\n\ndax_monthly |&gt;\n  drop_na(ret_excess) |&gt; \n  group_by(Month = month) |&gt;\n  summarize(\n    Mean = mean(ret_excess),\n    SD = sd(ret_excess),\n    Q05 = quantile(ret_excess, 0.05),\n    Q95 = quantile(ret_excess, 0.95),\n    `t-Statistic` = sqrt(n()) * mean(ret_excess) / sd(ret_excess)\n  )\n\n# A tibble: 12 × 6\n   Month     Mean     SD     Q05    Q95 `t-Statistic`\n   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n 1 1      0.00621 0.0552 -0.0911 0.0868         0.666\n 2 2      0.00200 0.0541 -0.0868 0.0775         0.219\n 3 3      0.00486 0.0539 -0.0732 0.0842         0.533\n 4 4      0.0260  0.0599 -0.0502 0.119          2.57 \n 5 5      0.00470 0.0401 -0.0573 0.0610         0.693\n 6 6     -0.00353 0.0476 -0.0934 0.0587        -0.439\n 7 7      0.0131  0.0598 -0.0658 0.0902         1.29 \n 8 8     -0.0235  0.0622 -0.168  0.0347        -2.27 \n 9 9     -0.0244  0.0730 -0.176  0.0613        -2.01 \n10 10     0.0186  0.0660 -0.0960 0.122          1.70 \n11 11     0.0228  0.0483 -0.0455 0.0862         2.79 \n12 12     0.0195  0.0554 -0.0586 0.108          2.08 \n\n\nAugust and September seem to usually exhibit negative excess returns with an average of about -2.4% (statistically significant) over all years, while April is the only months that tend to exhibit statistically significant positive excess returns.\nLet us proceed to test for the presence of statistically significant excess returns due to seasonal patterns. In the above table, I only test for significance for each month separately. To test for positive returns in a joint model, I regress the monthly excess returns on month indicators. Note that I always adjust the standard errors to be heteroskedasticity robust.\n\nmodel_monthly &lt;- feols(\n  ret_excess ~ month, \n  data = dax_monthly,\n  vcov = \"hetero\"\n)\n\nsummary(model_monthly)\n\nOLS estimation, Dep. Var.: ret_excess\nObservations: 423 \nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)  0.006214   0.009331  0.665936 0.505825    \nmonth2      -0.004210   0.013064 -0.322298 0.747391    \nmonth3      -0.001355   0.013045 -0.103901 0.917298    \nmonth4       0.019820   0.013762  1.440170 0.150581    \nmonth5      -0.001516   0.011533 -0.131456 0.895479    \nmonth6      -0.009744   0.012318 -0.791049 0.429372    \nmonth7       0.006857   0.013752  0.498605 0.618325    \nmonth8      -0.029730   0.013951 -2.131118 0.033672 *  \nmonth9      -0.030625   0.015334 -1.997223 0.046460 *  \nmonth10      0.012424   0.014422  0.861410 0.389514    \nmonth11      0.016551   0.012399  1.334888 0.182652    \nmonth12      0.013261   0.013224  1.002802 0.316547    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.056177   Adj. R2: 0.048833\n\n\nSeems like August and September have on average indeed lower returns than January (which is the omitted reference point in this regression). Note that the size of the coefficients from the regression are the same as in the table above (i.e., constant plus coefficient), but the t-statistics are different because we are estimating a joint model now.\nAs the raincloud plots indicated that outliers might drive any statistical significant differences, we estimate the model again after trimming the data. In particular, we drop the top and bottom 1% of observations. This trimming step only drops 10 observations.\n\nret_excess_q01 &lt;- quantile(dax_monthly$ret_excess, 0.01)\nret_excess_q99 &lt;- quantile(dax_monthly$ret_excess, 0.99)\n\nmodel_monthly_trimmed &lt;- feols(\n  ret_excess ~ month, \n  data = dax_monthly |&gt; \n     filter(ret_excess &gt;= ret_excess_q01 & ret_excess &lt;= ret_excess_q99),\n  vcov = \"hetero\"\n)\n   \netable(model_monthly, model_monthly_trimmed, coefstat = \"tstat\")\n\n                    model_monthly model_monthly_t..\nDependent Var.:        ret_excess        ret_excess\n                                                   \nConstant          0.0062 (0.6659)   0.0062 (0.6657)\nmonth2          -0.0042 (-0.3223) -0.0042 (-0.3222)\nmonth3          -0.0014 (-0.1039) -0.0014 (-0.1039)\nmonth4             0.0198 (1.440)   0.0099 (0.8135)\nmonth5          -0.0015 (-0.1315) -0.0015 (-0.1314)\nmonth6          -0.0097 (-0.7910) -0.0097 (-0.7908)\nmonth7            0.0069 (0.4986)   0.0024 (0.1805)\nmonth8          -0.0297* (-2.131)  -0.0201 (-1.602)\nmonth9          -0.0306* (-1.997)  -0.0142 (-1.127)\nmonth10           0.0124 (0.8614)   0.0124 (0.8611)\nmonth11            0.0165 (1.335)    0.0128 (1.071)\nmonth12            0.0133 (1.003)   0.0087 (0.6897)\n_______________ _________________ _________________\nVCOV type       Heteroskeda.-rob. Heteroskeda.-rob.\nObservations                  423               413\nR2                        0.07363           0.03824\nAdj. R2                   0.04883           0.01186\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIndeed, now no month exhibits a statistically significant outperformance compared to January.\nNext, I follow Jacobsen and Bouman (2002) and simply regress excess returns on dummies that indicate specific seasons, i.e., I estimate the model\n\\[ y_t=\\alpha + \\beta D_t + \\epsilon_t,\\]\nwhere \\(D_t\\) is a dummy variable equal to one for the months in a specific season and zero otherwise. We consider both the ‘Halloween’ season (where the dummy is one for November-April). If \\(D_t\\) is statistically significant and positive for the corresponding season, then we take this as evidence for the presence of seasonality effects.\n\nhalloween_months &lt;- c(11, 12, 1, 2, 3, 4)\n\ndax_monthly &lt;- dax_monthly |&gt;\n  mutate(halloween = if_else(month %in% halloween_months, 1L, 0L))\n\nWe again estimate two models to analyze the ‘Halloween’ effect:\n\nmodel_halloween &lt;- feols(\n  ret_excess ~ halloween, \n  data = dax_monthly,\n  vcov = \"hetero\"\n)\n\nmodel_halloween_trimmed &lt;- feols(\n  ret_excess ~ halloween, \n  data = dax_monthly |&gt; \n     filter(ret_excess &gt;= ret_excess_q01 & ret_excess &lt;= ret_excess_q99),\n  vcov = \"hetero\"\n)\n\netable(model_halloween, model_halloween_trimmed, coefstat = \"tstat\")\n\n                  model_halloween model_hallowe..\nDependent Var.:        ret_excess      ret_excess\n                                                 \nConstant        -0.0026 (-0.6254) 0.0013 (0.3592)\nhalloween        0.0162** (2.872) 0.0091. (1.806)\n_______________ _________________ _______________\nVCOV type       Heteroskeda.-rob. Heteroske.-rob.\nObservations                  423             413\nR2                        0.01919         0.00787\nAdj. R2                   0.01685         0.00546\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe indeed find evidence that excess returns are higher during the months November-April relative to the remaining months in the full sample. However, if we remove the top and bottom 1% of observations, then the statistical significant outperformance disappears again.\nAs a last step, let us compare three different strategies: (i) buy and hold the index over the full year, (ii) go long in the index outside of the Halloween season and otherwise hold the risk-free asset, and (iii) go long in the index outside of the Halloween season and otherwise short the index. Below I compare the returns of the three different strategies on an annual basis:\n\ndax_monthly &lt;- dax_monthly |&gt;\n  mutate(\n    ret_excess_halloween = if_else(halloween == 1, ret_excess, 0),\n    ret_excess_halloween_short = if_else(halloween == 1, ret_excess, -ret_excess)\n  )\n\nWhich of these strategies might constitute a better investment opportunity? For a very simple assessment, let us compute the corresponding Sharpe ratios. Note that I annualize Sharpe ratios by multiplying them with \\(\\sqrt{12}\\) which strictly speaking only works under IID distributed returns (which is typically unlikely to be the case), but which suffices for the purpose of this post.\n\nsharpe_ratio &lt;- function(x) {\n  sqrt(12) *  mean(x) / sd(x)\n}\n\nbind_rows(\n  dax_monthly |&gt;\n    summarize(\n      `Buy and Hold` = sharpe_ratio(ret),\n      `Halloween` = sharpe_ratio(ret_excess_halloween),\n      `Halloween-Short` = sharpe_ratio(ret_excess_halloween_short)\n    ) |&gt; \n    mutate(Data = \"Full\"),\n  dax_monthly |&gt; \n    filter(ret_excess &gt;= ret_excess_q01 & ret_excess &lt;= ret_excess_q99) |&gt;\n    summarize(\n      `Buy and Hold` = sharpe_ratio(ret_excess),\n      `Halloween` = sharpe_ratio(ret_excess_halloween),\n      `Halloween-Short` = sharpe_ratio(ret_excess_halloween_short)\n    ) |&gt; \n    mutate(Data = \"Trimmed\")\n) |&gt; \n  select(Data, everything())\n\n# A tibble: 2 × 4\n  Data    `Buy and Hold` Halloween `Halloween-Short`\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n1 Full             0.458     0.596             0.479\n2 Trimmed          0.394     0.502             0.305\n\n\nThe Sharpe ratio suggests that the Halloween strategy is a better investment opportunity than the other strategies for both the full and the trimmed sample. Shorting the market in the Halloween season even leads to worse performance than just staying invested in the market the whole time once we drop the outliers.\nTo sum up, this post showed some simple data-related issues that we should consider when we analyze return data. Overall, we could find strong support for this seasonality effect from a statistical perspective once we get rid of a few extreme observations."
  },
  {
    "objectID": "posts/dax-seasonality/index.html#footnotes",
    "href": "posts/dax-seasonality/index.html#footnotes",
    "title": "Analyzing Seasonality in DAX Returns",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBouman, Sven and Jacobsen, Ben (2002). “The Halloween Indicator, ‘Sell in May and Go Away’: Another Puzzle”, The American Economic Review, Vol. 92, No. 5, pp. 1618-1635, https://www.jstor.org/stable/3083268↩︎\nJacobsen, Ben and Zhang, Cherry Yi (2018), “The Halloween Indicator, ‘Sell in May and Go Away’: Everywhere and All the Time”, https://ssrn.com/abstract=2154873↩︎"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html",
    "href": "posts/dplyr-vs-tidierdata/index.html",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "",
    "text": "There are a myriad of options to perform essential data manipulation tasks in R and Julia. However, if we want to do tidy data science in R, there is a clear forerunner: dplyr. In the world of Julia, TidierData is a relatively new kid on the block that allows R users to dabble in Julia without learning a lot of new syntax. In this blog post, I illustrate their syntactic similarities and highlight differences between these two packages that emerge for a few key tasks.\nBefore we dive into the comparison, a short introduction to the packages: the dplyr package in R allows users to refer to columns without quotation marks due to its implementation of non-standard evaluation (NSE). NSE is a programming technique used in R that allows functions to capture the expressions passed to them as arguments, rather than just the values of those arguments. The primary goal of NSE in the context of dplyr is to create a more user-friendly and intuitive syntax. This makes data manipulation tasks more straightforward and aligns with the general philosophy of the tidyverse to make data science faster, easier, and more fun.1\nTidierData is a 100% Julia implementation of the dplyr and tidyr R packages with three goals: (i) stick as closely to the tidyverse syntax as possible, so that R users find it easier to switch; (ii) make broadcasting2 mostly invisible, so that many functions are automatically vectorized for users; (iii) make scalars and tuples mostly interchangeable, so that users can provide a scalar or a tuple as arguments as they see fit. Check out the package website for more information, in particular with respect to the features of Julia."
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#filter-rows",
    "href": "posts/dplyr-vs-tidierdata/index.html#filter-rows",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Filter rows",
    "text": "Filter rows\nFiltering rows with dplyr is based on NSE and the dplyr::filter() function. To replicate the same results with TidierData, you can use TidierData.@filter() method which accepts a remarkably similar notation to dplyr with the only exceptions that you need && or || for boolean operators and that you can omit the percentage signs around in.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  filter(species == \"Adelie\" & \n           island %in% c(\"Biscoe\", \"Dream\"))\n\n# A tibble: 100 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 90 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n@chain penguins begin\n  @filter(species == \"Adelie\" &&\n            island in (\"Biscoe\", \"Dream\"))\nend\n\n100×7 DataFrame\n Row │ species   island    bill_length_mm  bill_depth_mm  flipper_length_mm  b ⋯\n     │ String15  String15  Float64?        Float64?       Int64?             I ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Adelie    Biscoe              37.8           18.3                174    ⋯\n   2 │ Adelie    Biscoe              37.7           18.7                180\n   3 │ Adelie    Biscoe              35.9           19.2                189\n   4 │ Adelie    Biscoe              38.2           18.1                185\n   5 │ Adelie    Biscoe              38.8           17.2                180    ⋯\n  ⋮  │    ⋮         ⋮            ⋮               ⋮                ⋮            ⋱\n  96 │ Adelie    Dream               36.6           18.4                184\n  97 │ Adelie    Dream               36.0           17.8                195\n  98 │ Adelie    Dream               37.8           18.1                193\n  99 │ Adelie    Dream               36.0           17.1                187    ⋯\n 100 │ Adelie    Dream               41.5           18.5                201\n                                                   2 columns and 90 rows omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#slice-rows",
    "href": "posts/dplyr-vs-tidierdata/index.html#slice-rows",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Slice rows",
    "text": "Slice rows\ndplyr::slice() takes integers with row numbers as inputs, so you can use ranges and arbitrary vectors of integers. TidierData.@slice() does exactly the same.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  slice(10:20)\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           42            20.2               190        4250\n 2 Adelie  Torgersen           37.8          17.1               186        3300\n 3 Adelie  Torgersen           37.8          17.3               180        3700\n 4 Adelie  Torgersen           41.1          17.6               182        3200\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           36.6          17.8               185        3700\n 8 Adelie  Torgersen           38.7          19                 195        3450\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           34.4          18.4               184        3325\n11 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n@chain penguins begin\n  @slice(10:20)\nend\n\n11×7 DataFrame\n Row │ species   island     bill_length_mm  bill_depth_mm  flipper_length_mm   ⋯\n     │ String15  String15   Float64?        Float64?       Int64?              ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Adelie    Torgersen            42.0           20.2                190   ⋯\n   2 │ Adelie    Torgersen            37.8           17.1                186\n   3 │ Adelie    Torgersen            37.8           17.3                180\n   4 │ Adelie    Torgersen            41.1           17.6                182\n   5 │ Adelie    Torgersen            38.6           21.2                191   ⋯\n   6 │ Adelie    Torgersen            34.6           21.1                198\n   7 │ Adelie    Torgersen            36.6           17.8                185\n   8 │ Adelie    Torgersen            38.7           19.0                195\n   9 │ Adelie    Torgersen            42.5           20.7                197   ⋯\n  10 │ Adelie    Torgersen            34.4           18.4                184\n  11 │ Adelie    Torgersen            46.0           21.5                194\n                                                               2 columns omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#arrange-rows",
    "href": "posts/dplyr-vs-tidierdata/index.html#arrange-rows",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Arrange rows",
    "text": "Arrange rows\nTo orders the rows of a data frame by the values of selected columns, we have dplyr::arrange() and TidierData.@arrange(). Note that both approaches arrange rows in an an ascending order and puts missing values last as defaults.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  arrange(island, desc(bill_length_mm))\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n@chain penguins begin\n  @arrange(island, desc(bill_length_mm))\nend\n\n344×7 DataFrame\n Row │ species   island     bill_length_mm  bill_depth_mm  flipper_length_mm   ⋯\n     │ String15  String15   Float64?        Float64?       Int64?              ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Gentoo    Biscoe          missing        missing              missing   ⋯\n   2 │ Gentoo    Biscoe               59.6           17.0                230\n   3 │ Gentoo    Biscoe               55.9           17.0                228\n   4 │ Gentoo    Biscoe               55.1           16.0                230\n   5 │ Gentoo    Biscoe               54.3           15.7                231   ⋯\n  ⋮  │    ⋮          ⋮            ⋮               ⋮                ⋮           ⋱\n 340 │ Adelie    Torgersen            34.6           21.1                198\n 341 │ Adelie    Torgersen            34.6           17.2                189\n 342 │ Adelie    Torgersen            34.4           18.4                184\n 343 │ Adelie    Torgersen            34.1           18.1                193   ⋯\n 344 │ Adelie    Torgersen            33.5           19.0                190\n                                                  2 columns and 334 rows omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#select-columns",
    "href": "posts/dplyr-vs-tidierdata/index.html#select-columns",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Select columns",
    "text": "Select columns\nSelecting a subset of columns works exactly the same withdplyr::select() and TidierData.@select().\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  select(bill_length_mm, sex)\n\n# A tibble: 344 × 2\n   bill_length_mm sex   \n            &lt;dbl&gt; &lt;fct&gt; \n 1           39.1 male  \n 2           39.5 female\n 3           40.3 female\n 4           NA   &lt;NA&gt;  \n 5           36.7 female\n 6           39.3 male  \n 7           38.9 female\n 8           39.2 male  \n 9           34.1 &lt;NA&gt;  \n10           42   &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n\n\n@chain penguins begin\n  @select(bill_length_mm, sex)\nend\n\n344×2 DataFrame\n Row │ bill_length_mm  sex\n     │ Float64?        String7\n─────┼─────────────────────────\n   1 │           39.1  male\n   2 │           39.5  female\n   3 │           40.3  female\n   4 │      missing    missing\n   5 │           36.7  female\n  ⋮  │       ⋮            ⋮\n 340 │           55.8  male\n 341 │           43.5  female\n 342 │           49.6  male\n 343 │           50.8  male\n 344 │           50.2  female\n               334 rows omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#rename-columns",
    "href": "posts/dplyr-vs-tidierdata/index.html#rename-columns",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Rename columns",
    "text": "Rename columns\nRenaming columns also works exactly the same with dplyr::rename() and TidierData.rename().\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  rename(bill_length = bill_length_mm,\n         bill_depth = bill_depth_mm)\n\n# A tibble: 344 × 8\n   species island    bill_length bill_depth flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen        39.1       18.7               181        3750 male  \n 2 Adelie  Torgersen        39.5       17.4               186        3800 female\n 3 Adelie  Torgersen        40.3       18                 195        3250 female\n 4 Adelie  Torgersen        NA         NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen        36.7       19.3               193        3450 female\n 6 Adelie  Torgersen        39.3       20.6               190        3650 male  \n 7 Adelie  Torgersen        38.9       17.8               181        3625 female\n 8 Adelie  Torgersen        39.2       19.6               195        4675 male  \n 9 Adelie  Torgersen        34.1       18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen        42         20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\n\n\n@chain penguins begin\n  @rename(bill_length = bill_length_mm,\n          bill_depth = bill_depth_mm)\nend\n\n344×7 DataFrame\n Row │ species    island     bill_length  bill_depth  flipper_length_mm  body_ ⋯\n     │ String15   String15   Float64?     Float64?    Int64?             Int64 ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Adelie     Torgersen         39.1        18.7                181        ⋯\n   2 │ Adelie     Torgersen         39.5        17.4                186\n   3 │ Adelie     Torgersen         40.3        18.0                195\n   4 │ Adelie     Torgersen    missing     missing              missing      m\n   5 │ Adelie     Torgersen         36.7        19.3                193        ⋯\n  ⋮  │     ⋮          ⋮           ⋮           ⋮               ⋮                ⋱\n 340 │ Chinstrap  Dream             55.8        19.8                207\n 341 │ Chinstrap  Dream             43.5        18.1                202\n 342 │ Chinstrap  Dream             49.6        18.2                193\n 343 │ Chinstrap  Dream             50.8        19.0                210        ⋯\n 344 │ Chinstrap  Dream             50.2        18.7                198\n                                                  2 columns and 334 rows omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#mutate-columns",
    "href": "posts/dplyr-vs-tidierdata/index.html#mutate-columns",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Mutate columns",
    "text": "Mutate columns\nTransforming existing columns or creating new ones is an essential part of data analysis. dplyr::mutate() and TidierData.@mutate() are the work horses for these tasks. Note that you have to split up variable assignments if you want to refer to a newly created variable in TidierData, while you can refer to the new variables in the same mutate block in dplyr.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  mutate(ones = 1,\n         bill_length = bill_length_mm / 10,\n         bill_length_squared = bill_length^2) |&gt; \n  select(ones, bill_length_mm, bill_length, bill_length_squared)\n\n# A tibble: 344 × 4\n    ones bill_length_mm bill_length bill_length_squared\n   &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1     1           39.1        3.91                15.3\n 2     1           39.5        3.95                15.6\n 3     1           40.3        4.03                16.2\n 4     1           NA         NA                   NA  \n 5     1           36.7        3.67                13.5\n 6     1           39.3        3.93                15.4\n 7     1           38.9        3.89                15.1\n 8     1           39.2        3.92                15.4\n 9     1           34.1        3.41                11.6\n10     1           42          4.2                 17.6\n# ℹ 334 more rows\n\n\n\n\n\n@chain penguins begin\n  @mutate(ones = 1,\n          bill_length = bill_length_mm / 10)\n  @mutate(bill_length_squared = bill_length^2)\n  @select(ones, bill_length_mm, bill_length, bill_length_squared)\nend\n\n344×4 DataFrame\n Row │ ones   bill_length_mm  bill_length  bill_length_squared\n     │ Int64  Float64?        Float64?     Float64?\n─────┼─────────────────────────────────────────────────────────\n   1 │     1            39.1         3.91              15.2881\n   2 │     1            39.5         3.95              15.6025\n   3 │     1            40.3         4.03              16.2409\n   4 │     1       missing     missing            missing\n   5 │     1            36.7         3.67              13.4689\n  ⋮  │   ⋮          ⋮              ⋮                ⋮\n 340 │     1            55.8         5.58              31.1364\n 341 │     1            43.5         4.35              18.9225\n 342 │     1            49.6         4.96              24.6016\n 343 │     1            50.8         5.08              25.8064\n 344 │     1            50.2         5.02              25.2004\n                                               334 rows omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#relocate-columns",
    "href": "posts/dplyr-vs-tidierdata/index.html#relocate-columns",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Relocate columns",
    "text": "Relocate columns\ndplyr::relocate() provides options to change the positions of columns in a data frame, using the same syntax as dplyr::select(). In addition, there are the options .after and .before to provide users with additional shortcuts.\nThe recommended way to relocate columns in TidierData is to use the TidierData.@select() method, but there are no options as in dplyr::relocate(). In fact, the safest way to consistently get the correct order of columns is to explicitly specify them.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  relocate(c(species, bill_length_mm), .before = sex)\n\n# A tibble: 344 × 8\n   island    bill_depth_mm flipper_length_mm body_mass_g species bill_length_mm\n   &lt;fct&gt;             &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1 Torgersen          18.7               181        3750 Adelie            39.1\n 2 Torgersen          17.4               186        3800 Adelie            39.5\n 3 Torgersen          18                 195        3250 Adelie            40.3\n 4 Torgersen          NA                  NA          NA Adelie            NA  \n 5 Torgersen          19.3               193        3450 Adelie            36.7\n 6 Torgersen          20.6               190        3650 Adelie            39.3\n 7 Torgersen          17.8               181        3625 Adelie            38.9\n 8 Torgersen          19.6               195        4675 Adelie            39.2\n 9 Torgersen          18.1               193        3475 Adelie            34.1\n10 Torgersen          20.2               190        4250 Adelie            42  \n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n@chain penguins begin\n  @select(island, bill_depth_mm, flipper_length_mm, body_mass_g, \n          species, bill_length_mm, sex)\nend\n\n344×7 DataFrame\n Row │ island     bill_depth_mm  flipper_length_mm  body_mass_g  species    bi ⋯\n     │ String15   Float64?       Int64?             Int64?       String15   Fl ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Torgersen           18.7                181         3750  Adelie        ⋯\n   2 │ Torgersen           17.4                186         3800  Adelie\n   3 │ Torgersen           18.0                195         3250  Adelie\n   4 │ Torgersen      missing              missing      missing  Adelie\n   5 │ Torgersen           19.3                193         3450  Adelie        ⋯\n  ⋮  │     ⋮            ⋮                ⋮               ⋮           ⋮         ⋱\n 340 │ Dream               19.8                207         4000  Chinstrap\n 341 │ Dream               18.1                202         3400  Chinstrap\n 342 │ Dream               18.2                193         3775  Chinstrap\n 343 │ Dream               19.0                210         4100  Chinstrap     ⋯\n 344 │ Dream               18.7                198         3775  Chinstrap\n                                                  2 columns and 334 rows omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#simple-summaries-by-group",
    "href": "posts/dplyr-vs-tidierdata/index.html#simple-summaries-by-group",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Simple summaries by group",
    "text": "Simple summaries by group\nLet’s suppose we want to compute summaries by groups such as means or medians. Both packages are virtually the same again: on the R side you have dplyr::group_by() and dplyr::summarize(), while on the Julia side you have TidierData.@group_by() and TidierData.@summarize(). Note that you have to include the skipmissing() wrapper in order to drop missing values in the mean() function.\nMoreover, dplyr also automatically arranges the results by the group, so the reproduce the results of dplyr, we need to add TidierData.@arrange() to the chain.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  island    bill_depth_mean\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Biscoe               15.9\n2 Dream                18.3\n3 Torgersen            18.4\n\n\n\n\n\n@chain penguins begin\n  @group_by(island) \n  @summarize(bill_depth_mean = mean(skipmissing(bill_depth_mm)))\n  @arrange(island)\nend\n\n3×2 DataFrame\n Row │ island     bill_depth_mean\n     │ String15   Float64\n─────┼────────────────────────────\n   1 │ Biscoe             15.8749\n   2 │ Dream              18.3444\n   3 │ Torgersen          18.4294"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#more-complicated-summaries-by-group",
    "href": "posts/dplyr-vs-tidierdata/index.html#more-complicated-summaries-by-group",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "More complicated summaries by group",
    "text": "More complicated summaries by group\nTypically, you want to create multiple different summaries by groups. dplyr provides a lot of flexibility to create new variables on the fly, as does TidierData. For instance, we can pass expressions to them mean functions in order to create the share of female penguins per island in the summary statement.\n\ndplyrTidierData\n\n\n\npenguins |&gt; \n  group_by(island) |&gt; \n  summarize(count = n(),\n            bill_depth_mean = mean(bill_depth_mm, na.rm = TRUE),\n            flipper_length_median = median(flipper_length_mm, na.rm = TRUE),\n            body_mass_sd = sd(body_mass_g, na.rm = TRUE),\n            share_female = mean(sex == \"female\", na.rm = TRUE))\n\n# A tibble: 3 × 6\n  island   count bill_depth_mean flipper_length_median body_mass_sd share_female\n  &lt;fct&gt;    &lt;int&gt;           &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Biscoe     168            15.9                   214         783.        0.491\n2 Dream      124            18.3                   193         417.        0.496\n3 Torgers…    52            18.4                   191         445.        0.511\n\n\n\n\n\n@chain penguins begin\n  @group_by(island) \n  @summarize(count = n(),\n             bill_depth_mean = mean(skipmissing(bill_depth_mm)),\n             flipper_length_median = median(skipmissing(flipper_length_mm)),\n             body_mass_sd = std(skipmissing(body_mass_g)),\n             share_female = mean(skipmissing(sex == \"female\")))\n  @arrange(island)\nend\n\n3×6 DataFrame\n Row │ island     count  bill_depth_mean  flipper_length_median  body_mass_sd  ⋯\n     │ String15   Int64  Float64          Float64                Float64       ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ Biscoe       168          15.8749                  214.0       782.856  ⋯\n   2 │ Dream        124          18.3444                  193.0       416.644\n   3 │ Torgersen     52          18.4294                  191.0       445.108\n                                                                1 column omitted"
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#conclusion",
    "href": "posts/dplyr-vs-tidierdata/index.html#conclusion",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Conclusion",
    "text": "Conclusion\nThis post highlights syntactic similarities and differences across R’s dplyr and Julia’s TidierData packages. The key difference is between pipes and chains: dplyr uses the pipe operator |&gt; to chain functions, while TidierData uses the @chain df begin ... end syntax for piping a value through a series of transformation expressions. Nonetheless, the similarities are remarkable and demonstrate the flexibility of Julia to seemingly replicate the NSE capabilities of R. If you want to play around with Julia or some of its packages, I can highly recommend to take a shortcut using TidierData."
  },
  {
    "objectID": "posts/dplyr-vs-tidierdata/index.html#footnotes",
    "href": "posts/dplyr-vs-tidierdata/index.html#footnotes",
    "title": "Tidy Data Manipulation: dplyr vs TidierData",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the unifying principles of the tidyverse: https://design.tidyverse.org/unifying.html.↩︎\nBroadcasting expands singleton dimensions in array arguments to match the corresponding dimension in the other array without using extra memory.↩︎\nIn Julia, setting the number of rows to display for all DataFrames globally isn’t straightforward as there isn’t a direct global setting for this in the DataFrame package. ENV[\"COLUMNS\"] or ENV[\"LINES\"] control the display based on the size of your terminal.↩︎"
  },
  {
    "objectID": "posts/data-storage-comparison/index.html",
    "href": "posts/data-storage-comparison/index.html",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "",
    "text": "Sharing data between different collaborators, machines, or programming languages can be cumbersome for many reasons. In this post, I look into the issue of column types and how different storage technologies handle them. I focus on self-contained technologies that are easy to install and run on your machine without setting up a separate backend server. This requirement typically arises in academic contexts, educational settings, or when you quickly want to prototype something without spending time on setting up a data backend.\nI start with simple CSV, then move on to the popular SQLite database before I look at the rising star DuckDB. We close the comparison with a look at the Parquet and Feather file formats. I always check how the column type depends on the language that is used to store the data in the corresponding storage technology."
  },
  {
    "objectID": "posts/data-storage-comparison/index.html#footnotes",
    "href": "posts/data-storage-comparison/index.html#footnotes",
    "title": "Tidy Data: Tabular Data Storage Comparison",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is also a dedicated StringDtype for string data, introduced in more recent versions of pandas (version 1.0.0 and later), but it seems that it is not used often.↩︎\nIn R, dates are typically represented as the number of days since January 1, 1970, known as the Unix epoch. This is a standard date reference used in many programming languages. The integer is positive for dates after January 1, 1970, and negative for dates before it. For example, a Date object representing January 2, 1970, would have a value of 1, as it is one day after the epoch start. Similarly, POSIXct is a count of the number of seconds since the Unix epoch (January 1, 1970, 00:00:00 GMT).↩︎"
  }
]