(customer_churn_clean
.dropna()
.drop(["gender", "customer_id"])
.mutate(
senior_citizen = _.senior_citizen * 2
#       female=customer_churn_clean.gender.case().when('Female', 1).else_(0).end()
)
)
import ibis.selectors as s
from ibis import _
(customer_churn_clean
.dropna()
.drop(["gender", "customer_id"])
.mutate(
senior_citizen = _.senior_citizen * 2
#       female=customer_churn_clean.gender.case().when('Female', 1).else_(0).end()
)
)
from ibis import _
customer_churn_clean = clean_names(customer_churn_raw)
customer_churn_clean
(customer_churn_clean.dropna().drop(["gender", "customer_id"])
.mutate(
churn=customer_churn_clean.churn.case().when('Yes', 1).when('No', 0).else_(0).end()
#       female=customer_churn_clean.gender.case().when('Female', 1).else_(0).end()
)
)
customer_churn_clean = clean_names(customer_churn_raw)
(customer_churn_clean
.dropna()
.drop(["gender", "customer_id"])
.mutate(
churn=_.churn.case().when('Yes', 1).when('No', 0).else_(0).end()
#       female=customer_churn_clean.gender.case().when('Female', 1).else_(0).end()
)
)
(customer_churn_clean
.dropna()
.drop(["gender", "customer_id"])
.mutate(
churn=_.churn.case().when('Yes', 1).when('No', 0).else_(0).end()
female=_.gender.case().when('Female', 1).else_(0).end()
)
)
(customer_churn_clean
.dropna()
.drop(["gender", "customer_id"])
.mutate(
churn=_.churn.case().when('Yes', 1).when('No', 0).else_(0).end(),
female=_.gender.case().when('Female', 1).else_(0).end()
)
)
(customer_churn_clean
.mutate(
churn=_.churn.case().when('Yes', 1).when('No', 0).else_(0).end(),
female=_.gender.case().when('Female', 1).else_(0).end()
)
.dropna()
.drop(["gender", "customer_id"])
)
(customer_churn_clean
.mutate(
churn = _.churn.ifelse('Yes', 1, 0),
female = _.gender.case().when('Female', 1).else_(0).end()
)
.dropna()
.drop(["gender", "customer_id"])
)
customer_churn_raw = ibis.read_csv(data_url)
def clean_name(name):
name_with_underscores = re.sub('((?<=[a-z])[A-Z]|(?<=[A-Z])[A-Z](?=[a-z]))', r'_\1', name)
name_with_underscores = name_with_underscores.lower()
name_with_underscores = name_with_underscores.replace(' ', '_')
clean = re.sub(r'[^\w\s]', '', name_with_underscores)
return clean
def clean_names(table):
rename_map = {clean_name(col): col for col in table.columns}
table_clean = table.rename(rename_map)
return table_clean
customer_churn_raw = clean_names(customer_churn_raw)
customer_churn = (customer_churn_raw
.mutate(
churn = _.churn.case().when('Yes', 1).when('No', 0).else_(0).end(),
female = _.gender.case().when('Female', 1).else_(0).end()
)
.dropna()
.drop(["gender", "customer_id"])
)
customer_churn
binary_cols = ['partner', 'dependents', 'phone_service', 'paperless_billing']
binary_cols = ['partner', 'dependents', 'phone_service', 'paperless_billing']
for col in binary_cols:
customer = (customer_churn
.mutate(
**{col: customer[col].ifelse(1, 0).cast('int32')}
)
)
binary_cols = ['partner', 'dependents', 'phone_service', 'paperless_billing']
for col in binary_cols:
customer = (customer_churn
.mutate(
**{col: customer_churn[col].ifelse(1, 0).cast('int32')}
)
)
customer_churn
binary_cols = ['partner', 'dependents', 'phone_service', 'paperless_billing']
for col in binary_cols:
customer = (customer_churn
.mutate(
**{col: customer_churn[col].when('Yes', 1).when('No', 0).else_(0).end()}
)
)
# Further mutations for binary transformations
binary_cols = ['partner', 'dependents', 'phone_service', 'paperless_billing']
for col in binary_cols:
customer = (customer_churn
.mutate(
**{col: customer_churn[col].case().when('Yes', 1).when('No', 0).else_(0).end()}
)
)
customer
customer_churn = (customer_churn_raw
.mutate(
churn = _.churn.case().when('Yes', 1).when('No', 0).else_(0).end(),
female = _.gender.case().when('Female', 1).else_(0).end()
)
.dropna()
.drop(["gender", "customer_id"])
)
customer_churn
# Further mutations for binary transformations
binary_cols = ['partner', 'dependents', 'phone_service', 'paperless_billing']
for col in binary_cols:
customer_churn = (customer_churn
.mutate(
**{col: customer_churn[col].case().when('Yes', 1).when('No', 0).else_(0).end()}
)
)
customer_churn
# Lowercase and replace spaces or dashes with underscores in specified columns
replace_cols = ['multiple_lines', 'internet_service', 'online_security',
'online_backup', 'device_protection', 'tech_support',
'streaming_tv', 'streaming_movies', 'contract',
'paperless_billing', 'payment_method']
for col in replace_cols:
customer_churn = (customer_churn
.mutate(
**{col: customer_churn[col].lower().replace(' ', '_').replace('-', '_')}
)
)
customer_churn
customer_churn.columns
customer_churn.multiple_lines
customer_churn.internet_service
customer_churn.online_security
customer_churn = (customer_churn_raw
.mutate(
churn = _.churn.case().when('Yes', 1).when('No', 0).else_(0).end(),
female = _.gender.case().when('Female', 1).else_(0).end()
)
.dropna()
.drop(["gender", "customer_id"])
)
customer_churn
# Further mutations for binary transformations
binary_cols = ['partner', 'dependents', 'phone_service', 'paperless_billing']
for col in binary_cols:
customer_churn = (customer_churn
.mutate(
**{col: _[col].case().when('Yes', 1).when('No', 0).else_(0).end()}
)
)
customer_churn
customer_churn.describe()
replace_cols = ['multiple_lines', 'internet_service', 'online_security',
'online_backup', 'device_protection', 'tech_support',
'streaming_tv', 'streaming_movies', 'contract',
'paperless_billing', 'payment_method']
for col in replace_cols:
customer_churn = (customer_churn
.mutate(
**{col: customer_churn[col].lower().replace(' ', '_').replace('-', '_')}
)
)
customer_churn
customer_churn.to_pandas().describe()
customer_churn.to_pandas().info()
customer_churn_raw
customer_churn_raw.paperless_billing
customer_churn_raw.paperless_billing.nunique
customer_churn_raw.paperless_billing.to_pandas().unique()
customer_churn_raw = clean_names(customer_churn_raw)
customer_churn = (customer_churn_raw
.mutate(
churn = _.churn.case().when('Yes', 1).when('No', 0).else_(0).end(),
female = _.gender.case().when('Female', 1).else_(0).end()
)
.dropna()
.drop(["gender", "customer_id"])
)
# Further mutations for binary transformations
binary_cols = ['partner', 'dependents', 'phone_service', 'paperless_billing']
for col in binary_cols:
customer_churn = (customer_churn
.mutate(
**{col: _[col].case().when('Yes', 1).when('No', 0).else_(0).end()}
)
)
# Lowercase and replace spaces or dashes with underscores in specified columns
replace_cols = ['multiple_lines', 'internet_service', 'online_security',
'online_backup', 'device_protection', 'tech_support',
'streaming_tv', 'streaming_movies', 'contract', 'payment_method']
for col in replace_cols:
customer_churn = (customer_churn
.mutate(
**{col: customer_churn[col].lower().replace(' ', '_').replace('-', '_')}
)
)
customer_churn
customer_churn.info()
customer_churn.to_pandas().info()
reticulate::repl_python()
import re
import ibis
from ibis import _
ibis.options.interactive = True
data_url = "https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"
customer_churn_raw = ibis.read_csv(data_url)
def clean_name(name):
name_with_underscores = re.sub('((?<=[a-z])[A-Z]|(?<=[A-Z])[A-Z](?=[a-z]))', r'_\1', name)
name_with_underscores = name_with_underscores.lower()
name_with_underscores = name_with_underscores.replace(' ', '_')
clean = re.sub(r'[^\w\s]', '', name_with_underscores)
return clean
def clean_names(table):
rename_map = {clean_name(col): col for col in table.columns}
table_clean = table.rename(rename_map)
return table_clean
customer_churn_raw = clean_names(customer_churn_raw)
customer_churn = (customer_churn_raw
.mutate(
female = _.gender.case().when('Female', 1).else_(0).end()
)
.dropna()
.drop(["gender", "customer_id"])
)
binary_cols = ["churn", "partner", "dependents", "phone_service", "paperless_billing"]
for col in binary_cols:
customer_churn = (customer_churn
.mutate(
**{col: _[col].case().when("Yes", 1).when("No", 0).else_(0).end()}
)
)
replace_cols = ["multiple_lines", "internet_service", "online_security",
"online_backup", "device_protection", "tech_support",
"streaming_tv", "streaming_movies", "contract", "payment_method"]
for col in replace_cols:
customer_churn = (customer_churn
.mutate(
**{col: _[col].lower().replace(" ", "_").replace("-", "_")}
)
)
customer_churn.to_pandas().info()
customer_churn.to_polars()
customer_churn.to_pandas()
import re
import ibis
from ibis import _
ibis.options.interactive = True
data_url = "https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"
customer_raw = ibis.read_csv(data_url)
customer_raw = clean_names(customer_raw)
customer = (customer_raw
.mutate(
female = _.gender.case().when('Female', 1).else_(0).end()
)
.dropna()
.drop(["gender", "customer_id"])
)
binary_cols = ["churn", "partner", "dependents", "phone_service", "paperless_billing"]
for col in binary_cols:
customer = (customer
.mutate(
**{col: _[col].case().when("Yes", 1).when("No", 0).else_(0).end()}
)
)
replace_cols = ["multiple_lines", "internet_service", "online_security",
"online_backup", "device_protection", "tech_support",
"streaming_tv", "streaming_movies", "contract", "payment_method"]
for col in replace_cols:
customer = (customer
.mutate(
**{col: _[col].lower().replace(" ", "_").replace("-", "_")}
)
)
customer.to_pandas().info()
import numpy as np
np.random.seed(1234)
from sklearn.model_selection import train_test_split
np.random.seed(1234)
train_test_split(customer.drop('churn', axis=1), customer['churn'], test_size=0.2, stratify=y)
train_test_split(customer.drop('churn'), customer['churn'], test_size=0.2, stratify=y)
customer.drop('churn')
customer['churn']
train_test_split(customer.drop('churn'), customer['churn'], test_size=0.2, stratify=customer['churn'])
customer = customer.to_pandas()
train_test_split(customer.drop('churn'), customer['churn'], test_size=0.2, stratify=customer['churn'])
customer
customer['churn']
X = customer.drop('churn', axis=1)  # Features
y = customer['churn']  # Target variable
# Splitting the data, with 80% for training and 20% for testing, stratified by 'churn'
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
X_train
cv = StratifiedKFold(n_splits=5)
from sklearn.model_selection import train_test_split, StratifiedKFold
cv = StratifiedKFold(n_splits=5)
cv
cv = StratifiedKFold(n_splits = 5, random_state = 1234)
cv = StratifiedKFold(n_splits = 5)
np.random.seed(1234)
X = customer.drop('churn', axis=1)  # Features
y = customer['churn']  # Target variable
# Splitting the data, with 80% for training and 20% for testing, stratified by 'churn'
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
# Setting up stratified k-fold cross-validation, with k=5
cv = StratifiedKFold(n_splits = 5)
pre_processor = ColumnTransformer(
transformers=[
('log', FunctionTransformer(np.log1p), ['total_charges']),  # Apply log transformation
('norm', StandardScaler(), ['tenure', 'monthly_charges']),  # Normalize
('ohe', OneHotEncoder(drop='if_binary'), X_train.select_dtypes(include=['object', 'category']).columns.drop('churn'))  # One-hot encode
])
from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder
pre_processor = ColumnTransformer(
transformers=[
('log', FunctionTransformer(np.log1p), ['total_charges']),  # Apply log transformation
('norm', StandardScaler(), ['tenure', 'monthly_charges']),  # Normalize
('ohe', OneHotEncoder(drop='if_binary'), X_train.select_dtypes(include=['object', 'category']).columns.drop('churn'))  # One-hot encode
])
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder
pre_processor = ColumnTransformer(
transformers=[
('log', FunctionTransformer(np.log1p), ['total_charges']),  # Apply log transformation
('norm', StandardScaler(), ['tenure', 'monthly_charges']),  # Normalize
('ohe', OneHotEncoder(drop='if_binary'), X_train.select_dtypes(include=['object', 'category']).columns.drop('churn'))  # One-hot encode
])
X_train.select_dtypes(include=['object', 'category'])
pre_processor = ColumnTransformer(
transformers=[
('log', FunctionTransformer(np.log1p), ['total_charges']),  # Apply log transformation
('norm', StandardScaler(), ['tenure', 'monthly_charges']),  # Normalize
('ohe', OneHotEncoder(drop='if_binary'), X_train.select_dtypes(include=['object', 'category']))  # One-hot encode
])
pre_processor
from sklearn.linear_model import LogisticRegression
logistic = LogisticRegressionCV(
Cs=[10000],  # Inverse of 0.0001 for demonstration; adjust based on actual penalty needed
cv=5, penalty='l1', solver='saga', max_iter=1000, random_state=1234)
from sklearn.linear_model import LogisticRegressionCV
logistic = LogisticRegressionCV(
Cs=[10000],  # Inverse of 0.0001 for demonstration; adjust based on actual penalty needed
cv=5, penalty='l1', solver='saga', max_iter=1000, random_state=1234)
pipeline = Pipeline(steps = [
('preprocessor', preprocessor),
('model', model_logistic)
])
from sklearn.pipeline import Pipeline
pipeline = Pipeline(steps = [
('preprocessor', preprocessor),
('model', model_logistic)
])
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder
preprocessor = ColumnTransformer(
transformers=[
("log", FunctionTransformer(np.log1p), ["total_charges"]),
("norm", StandardScaler(), ["tenure", "monthly_charges"]),
("ohe", OneHotEncoder(drop="if_binary"), X_train.select_dtypes(include=["object", "category"]))
])
from sklearn.linear_model import LogisticRegressionCV
model_logistic = LogisticRegressionCV(
Cs=[10000],  # Inverse of 0.0001 for demonstration; adjust based on actual penalty needed
cv=5, penalty='l1', solver='saga', max_iter=1000, random_state=1234)
from sklearn.pipeline import Pipeline
pipeline = Pipeline(steps = [
('preprocessor', preprocessor),
('model', model_logistic)
])
from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, recall_score, precision_score, accuracy_score
cv_results = cross_validate(
pipeline, X_train, y_train, cv=cv,
scoring={'recall': make_scorer(recall_score),
'precision': make_scorer(precision_score),
'accuracy': make_scorer(accuracy_score)},
return_train_score=False)
X_train
y_train
cv
X_train
cv_results = cross_validate(
pipeline, X_train, y_train, cv=cv,
scoring={'recall': make_scorer(recall_score),
'precision': make_scorer(precision_score),
'accuracy': make_scorer(accuracy_score)},
return_train_score=False)
X_train
X_train.info()
X_train.columns = X_train.columns.astype(str)
cv_results = cross_validate(
pipeline, X_train, y_train, cv=cv,
scoring={'recall': make_scorer(recall_score),
'precision': make_scorer(precision_score),
'accuracy': make_scorer(accuracy_score)},
return_train_score=False)
X_test.columns = X_test.columns.astype(str)
cv_results = cross_validate(
pipeline, X_train, y_train, cv=cv,
scoring={'recall': make_scorer(recall_score),
'precision': make_scorer(precision_score),
'accuracy': make_scorer(accuracy_score)},
return_train_score=False)
pipeline.fit(X_train, y_train)
from sklearn.linear_model import ElasticNet
model_elastic_net = ElasticNet(
alpha=0.007,
l1_ratio=1,
max_iter=5000,
fit_intercept=False
)
from sklearn.linear_model import ElasticNet
model_elastic_net = ElasticNet(
alpha=0.007,
l1_ratio=1,
max_iter=5000
)
pipeline = Pipeline(steps = [
('preprocessor', preprocessor),
('model', model_elastic_net)
])
pipeline.fit(
X_train,
y_train
)
X_train.columns
X_train.columns.to_list
X_train.columns.to_list()
X_train.columns = X_train.columns.astype(str)
X_test.columns = X_test.columns.astype(str)
pipeline.fit(
X_train,
y_train
)
preprocessor
for col in X_train.columns:
print(repr(col))
preprocessor
X_train.select_dtypes(include=["object", "category"])
preprocessor.fit(X_train)
preprocessor.get_feature_names_out()
preprocessor.transform(X_train)
quit
customer_recipe <- recipe(churn ~ ., data = training(customer_split)) |>
step_log(c(total_charges)) |>
step_normalize(c(tenure, monthly_charges)) |>
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
reticulate::repl_python()
preprocessor = ColumnTransformer(
transformers=[
("log", FunctionTransformer(np.log1p), ["total_charges"]),
("norm", StandardScaler(), ["tenure", "monthly_charges"]),
("ohe", OneHotEncoder(drop="if_binary"), X_train.select_dtypes(include=["object", "category"])),
remainder='passthrough'
])
preprocessor = ColumnTransformer(
transformers=[
("log", FunctionTransformer(np.log1p), ["total_charges"]),
("norm", StandardScaler(), ["tenure", "monthly_charges"]),
("ohe", OneHotEncoder(drop="if_binary"), X_train.select_dtypes(include=["object", "category"]))],
remainder='passthrough'
)
pipeline = Pipeline(steps = [
('preprocessor', preprocessor),
('model', model_elastic_net)
])
pipeline.fit(
X_train,
y_train
)
X_train.select_dtypes(include=["object", "category"])
X_train.select_dtypes(include=["object", "category"]).columns
X_train.select_dtypes(include=["object", "category"]).columns.to_list()
nominal_columns = X_train.select_dtypes(include=["object", "category"]).columns.to_list()
nominal_columns = X_train.select_dtypes(include=["object", "category"]).columns.to_list()
preprocessor = ColumnTransformer(
transformers=[
("log", FunctionTransformer(np.log1p), ["total_charges"]),
("norm", StandardScaler(), ["tenure", "monthly_charges"]),
("ohe", OneHotEncoder(drop="if_binary"), nominal_columns)],
remainder='passthrough'
)
pipeline = Pipeline(steps = [
('preprocessor', preprocessor),
('model', model_elastic_net)
])
pipeline.fit(
X_train,
y_train
)
reticulate::repl_python()
